{"repo_name": "base4sistemas/satcfe", "file_path": "tests/conftest.py", "context": "[{\"identifier\":\"BibliotecaSAT\", \"path\":\"satcfe/base.py\", \"snippet\":\"class BibliotecaSAT(object):\\n    \\\"\\\"\\\"Configura a localização da biblioteca que efetivamente acessará o\\n    equipamento SAT. A biblioteca deverá ser uma DLL (*dynamic linked library*,\\n    em sistemas Microsoft Windows) ou uma *shared library* em sistemas baseados\\n    no UNIX ou GNU/Linux.\\n\\n    :param string caminho: Caminho completo para a biblioteca SAT.\\n\\n    :param integer convencao: Opcional. Indica a convenção de chamada da\\n        biblioteca, devendo ser uma das constantes definidas em\\n        :attr:`~satcomum.constantes.CONVENCOES_CHAMADA`. Se não for informado,\\n        a convenção de chamada será decidida conforme a extensão do nome do\\n        arquivo, assumindo :attr:`~satcomum.constantes.WINDOWS_STDCALL` para as\\n        extensões ``.DLL`` ou ``.dll``. Quaisquer outras extensões, assume a\\n        convenção de chamada :attr:`~satcomum.constantes.STANDARD_C`.\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, caminho, convencao=None):\\n        self._libsat = None\\n        self._caminho = caminho\\n        self._convencao = convencao\\n        self._carregar()\\n\\n    def _carregar(self):\\n        \\\"\\\"\\\"Carrega (ou recarrega) a biblioteca SAT. Se a convenção de chamada\\n        ainda não tiver sido definida, será determinada pela extensão do\\n        arquivo da biblioteca.\\n\\n        :raises ValueError: Se a convenção de chamada não puder ser determinada\\n            ou se não for um valor válido.\\n        \\\"\\\"\\\"\\n        if self._convencao is None:\\n            if self._caminho.endswith(('.DLL', '.dll')):\\n                self._convencao = constantes.WINDOWS_STDCALL\\n            else:\\n                self._convencao = constantes.STANDARD_C\\n\\n        if self._convencao == constantes.STANDARD_C:\\n            loader = ctypes.CDLL\\n\\n        elif self._convencao == constantes.WINDOWS_STDCALL:\\n            loader = ctypes.WinDLL\\n\\n        else:\\n            raise ValueError((\\n                    'Convencao de chamada desconhecida: {!r}'\\n                ).format(self._convencao))\\n\\n        self._libsat = loader(self._caminho)\\n\\n    @property\\n    def ref(self):\\n        \\\"\\\"\\\"Uma referência para a biblioteca SAT carregada.\\\"\\\"\\\"\\n        return self._libsat\\n\\n    @property\\n    def caminho(self):\\n        \\\"\\\"\\\"Caminho completo para a biblioteca SAT.\\\"\\\"\\\"\\n        return self._caminho\\n\\n    @property\\n    def convencao(self):\\n        \\\"\\\"\\\"Convenção de chamada para a biblioteca SAT.\\n        Deverá ser um dos valores disponíveis na contante\\n        :attr:`~satcomum.constantes.CONVENCOES_CHAMADA`.\\n        \\\"\\\"\\\"\\n        return self._convencao\"}, {\"identifier\":\"ClienteSATLocal\", \"path\":\"satcfe/clientelocal.py\", \"snippet\":\"class ClienteSATLocal(FuncoesSAT):\\n    \\\"\\\"\\\"Fornece acesso ao equipamento SAT conectado na máquina local.\\n\\n    As respostas às funções SAT serão trabalhadas resultando em objetos Python\\n    regulares cujos atributos representam as peças de informação conforme\\n    descrito, função por função, na ER SAT.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, *args, **kwargs):\\n        super(ClienteSATLocal, self).__init__(*args, **kwargs)\\n\\n    def ativar_sat(self, tipo_certificado, cnpj, codigo_uf):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.ativar_sat`.\\n\\n        :return: Uma resposta SAT especilizada em ``AtivarSAT``.\\n        :rtype: satcfe.resposta.ativarsat.RespostaAtivarSAT\\n        \\\"\\\"\\\"\\n        retorno = super(ClienteSATLocal, self).ativar_sat(\\n                tipo_certificado,\\n                cnpj,\\n                codigo_uf)\\n        return RespostaAtivarSAT.analisar(retorno)\\n\\n    def comunicar_certificado_icpbrasil(self, certificado):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.comunicar_certificado_icpbrasil`.\\n\\n        :return: Uma resposta SAT padrão.\\n        :rtype: satcfe.resposta.padrao.RespostaSAT\\n        \\\"\\\"\\\"\\n        retorno = super(ClienteSATLocal, self).comunicar_certificado_icpbrasil(\\n                certificado)\\n        return RespostaSAT.comunicar_certificado_icpbrasil(retorno)\\n\\n    def enviar_dados_venda(self, dados_venda, *args, **kwargs):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.enviar_dados_venda`.\\n\\n        :return: Uma resposta SAT especializada em ``EnviarDadosVenda``.\\n        :rtype: satcfe.resposta.enviardadosvenda.RespostaEnviarDadosVenda\\n        \\\"\\\"\\\"\\n        retorno = super(ClienteSATLocal, self).enviar_dados_venda(\\n                dados_venda,\\n                *args,\\n                **kwargs)\\n        return RespostaEnviarDadosVenda.analisar(retorno)\\n\\n    def cancelar_ultima_venda(\\n            self,\\n            chave_cfe,\\n            dados_cancelamento,\\n            *args,\\n            **kwargs):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.cancelar_ultima_venda`.\\n\\n        :return: Uma resposta SAT especializada em ``CancelarUltimaVenda``.\\n        :rtype: satcfe.resposta.cancelarultimavenda.RespostaCancelarUltimaVenda\\n        \\\"\\\"\\\"\\n        retorno = super(ClienteSATLocal, self).cancelar_ultima_venda(\\n                chave_cfe,\\n                dados_cancelamento,\\n                *args,\\n                **kwargs)\\n        return RespostaCancelarUltimaVenda.analisar(retorno)\\n\\n    def consultar_sat(self):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.consultar_sat`.\\n\\n        :return: Uma resposta SAT padrão.\\n        :rtype: satcfe.resposta.padrao.RespostaSAT\\n        \\\"\\\"\\\"\\n        retorno = super(ClienteSATLocal, self).consultar_sat()\\n        return RespostaSAT.consultar_sat(retorno)\\n\\n    def teste_fim_a_fim(self, dados_venda, *args, **kwargs):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.teste_fim_a_fim`.\\n\\n        :return: Uma resposta SAT especializada em ``TesteFimAFim``.\\n        :rtype: satcfe.resposta.testefimafim.RespostaTesteFimAFim\\n        \\\"\\\"\\\"\\n        retorno = super(ClienteSATLocal, self).teste_fim_a_fim(\\n                dados_venda,\\n                *args,\\n                **kwargs)\\n        return RespostaTesteFimAFim.analisar(retorno)\\n\\n    def consultar_status_operacional(self):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.consultar_status_operacional`.\\n\\n        :return: Uma resposta SAT especializada em\\n            ``ConsultarStatusOperacional``.\\n        :rtype: satcfe.resposta.consultarstatusoperacional.RespostaConsultarStatusOperacional\\n        \\\"\\\"\\\"  # noqa: E501\\n        retorno = super(ClienteSATLocal, self).consultar_status_operacional()\\n        return RespostaConsultarStatusOperacional.analisar(retorno)\\n\\n    def consultar_numero_sessao(self, numero_sessao):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.consultar_numero_sessao`.\\n\\n        :return: Uma resposta SAT que irá depender da sessão consultada.\\n        :rtype: satcfe.resposta.padrao.RespostaSAT\\n        \\\"\\\"\\\"\\n        retorno = super(ClienteSATLocal, self).consultar_numero_sessao(\\n                numero_sessao)\\n        return RespostaConsultarNumeroSessao.analisar(retorno)\\n\\n    def configurar_interface_de_rede(self, configuracao, *args, **kwargs):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.configurar_interface_de_rede`.\\n\\n        :return: Uma resposta SAT padrão.\\n        :rtype: satcfe.resposta.padrao.RespostaSAT\\n        \\\"\\\"\\\"\\n        retorno = super(ClienteSATLocal, self).configurar_interface_de_rede(\\n                configuracao,\\n                *args,\\n                **kwargs)\\n        return RespostaSAT.configurar_interface_de_rede(retorno)\\n\\n    def associar_assinatura(self, sequencia_cnpj, assinatura_ac):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.associar_assinatura`.\\n\\n        :return: Uma resposta SAT padrão.\\n        :rtype: satcfe.resposta.padrao.RespostaSAT\\n        \\\"\\\"\\\"\\n        retorno = super(ClienteSATLocal, self).associar_assinatura(\\n                sequencia_cnpj,\\n                assinatura_ac)\\n        return RespostaAssociarAssinatura.analisar(retorno)\\n\\n    def atualizar_software_sat(self):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.atualizar_software_sat`.\\n\\n        :return: Uma resposta SAT padrão.\\n        :rtype: satcfe.resposta.padrao.RespostaSAT\\n        \\\"\\\"\\\"\\n        retorno = super(ClienteSATLocal, self).atualizar_software_sat()\\n        return RespostaSAT.atualizar_software_sat(retorno)\\n\\n    def extrair_logs(self):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.extrair_logs`.\\n\\n        :return: Uma resposta SAT especializada em ``ExtrairLogs``.\\n        :rtype: satcfe.resposta.extrairlogs.RespostaExtrairLogs\\n        \\\"\\\"\\\"\\n        retorno = super(ClienteSATLocal, self).extrair_logs()\\n        return RespostaExtrairLogs.analisar(retorno)\\n\\n    def bloquear_sat(self):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.bloquear_sat`.\\n\\n        :return: Uma resposta SAT padrão.\\n        :rtype: satcfe.resposta.padrao.RespostaSAT\\n        \\\"\\\"\\\"\\n        retorno = super(ClienteSATLocal, self).bloquear_sat()\\n        return RespostaSAT.bloquear_sat(retorno)\\n\\n    def desbloquear_sat(self):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.desbloquear_sat`.\\n\\n        :return: Uma resposta SAT padrão.\\n        :rtype: satcfe.resposta.padrao.RespostaSAT\\n        \\\"\\\"\\\"\\n        retorno = super(ClienteSATLocal, self).desbloquear_sat()\\n        return RespostaSAT.desbloquear_sat(retorno)\\n\\n    def trocar_codigo_de_ativacao(\\n            self, novo_codigo_ativacao,\\n            opcao=constantes.CODIGO_ATIVACAO_REGULAR,\\n            codigo_emergencia=None):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.trocar_codigo_de_ativacao`.\\n\\n        :return: Uma resposta SAT padrão.\\n        :rtype: satcfe.resposta.padrao.RespostaSAT\\n        \\\"\\\"\\\"\\n        retorno = super(ClienteSATLocal, self).trocar_codigo_de_ativacao(\\n                novo_codigo_ativacao,\\n                opcao=opcao,\\n                codigo_emergencia=codigo_emergencia)\\n\\n        resposta = RespostaSAT.trocar_codigo_de_ativacao(retorno)\\n\\n        # NOTA: Se não foi lançada nenhuma exceção, assume que a troca do\\n        # código de ativação foi bem sucedida. Alterna o código de ativação\\n        # atual para o novo código de ativação para que as chamadas\\n        # subsequentes tenham sucesso.\\n        self._codigo_ativacao = novo_codigo_ativacao\\n\\n        return resposta\\n\\n    def consultar_ultima_sessao_fiscal(self):\\n        \\\"\\\"\\\"Sobrepõe :meth:`~satcfe.base.FuncoesSAT.consultar_ultima_sessao_fiscal`.\\n\\n        :return: Uma resposta SAT que irá depender do último comando \\\"fiscal\\\"\\n            executado pelo equipmamento SAT, que poderá ser uma venda ou um\\n            cancelamento de venda.\\n\\n        :rtype: satcfe.resposta.consultarultimasessaofiscal.RespostaConsultarUltimaSessaoFiscal |\\n            satcfe.resposta.enviardadosvenda.RespostaEnviarDadosVenda |\\n            satcfe.resposta.cancelarultimavenda.RespostaCancelarUltimaVenda\\n\\n        \\\"\\\"\\\"  # noqa: E501\\n        retorno = super(ClienteSATLocal, self).consultar_ultima_sessao_fiscal()\\n        return RespostaConsultarUltimaSessaoFiscal.analisar(retorno)\"}, {\"identifier\":\"CFeCancelamento\", \"path\":\"satcfe/entidades.py\", \"snippet\":\"class CFeCancelamento(Entidade):\\n    \\\"\\\"\\\"\\n    Representa um CF-e de cancelamento.\\n\\n    :param Destinatario destinatario: Opcional. Uma instância de\\n        :class:`Destinatario` contendo apenas os dados exigidos para a operação\\n        de cancelamento (ie. ``CPF`` ou ``CNPJ`` do destinatário).\\n\\n    :param str chCanc: Chave de acesso do CF-e a ser cancelado. Deve ser\\n        precedido do literal ``CFe`` seguido dos quarenta e quatro dígitos\\n        que compõem a chave de acesso.\\n\\n    :param str CNPJ: CNPJ da software house, desenvolvedora do aplicativo\\n        comercial, contendo apenas os dígitos do número e incluindo zeros não\\n        significativos, se for o caso (14 dígitos).\\n\\n    :param str signAC: Assinatura do aplicativo comercial (344 dígitos).\\n\\n    :param int numeroCaixa: Número do caixa ao qual o SAT está conectado.\\n        Normalmente este será o número do caixa de onde parte a solicitação de\\n        cancelamento. Deverá ser um número inteiro entre ``0`` e ``999``.\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, destinatario=None, **kwargs):\\n        self._destinatario = destinatario\\n        super(CFeCancelamento, self).__init__(schema={\\n                'chCanc': {\\n                        'type': 'string',\\n                        'required': True,\\n                        'regex': r'^CFe\\\\d{44}$',\\n                    },\\n                'CNPJ': {\\n                        'type': 'string',\\n                        'check_with': 'cnpj',\\n                        'required': True,\\n                    },\\n                'signAC': {\\n                        'type': 'string',\\n                        'check_with': 'assinatura_ac',\\n                        'required': True,\\n                    },\\n                'numeroCaixa': {\\n                        'type': 'integer',\\n                        'required': True,\\n                        'min': 0,\\n                        'max': 999,\\n                    },\\n            }, **kwargs)\\n\\n    @property\\n    def destinatario(self):\\n        \\\"\\\"\\\"O :class:`Destinatario` ou ``None``.\\\"\\\"\\\"\\n        return self._destinatario\\n\\n    def _xml(self, *args, **kwargs):\\n        self.erros.clear()\\n        return super(CFeCancelamento, self)._xml(*args, **kwargs)\\n\\n    def _construir_elemento_xml(self, *args, **kwargs):\\n        cfecanc = ET.Element('CFeCanc')\\n\\n        infCFe = ET.SubElement(cfecanc, 'infCFe')\\n        infCFe.attrib['chCanc'] = self.chCanc\\n\\n        ide = ET.SubElement(infCFe, 'ide')\\n        ET.SubElement(ide, 'CNPJ').text = self.CNPJ\\n        ET.SubElement(ide, 'signAC').text = self.signAC\\n        ET.SubElement(ide, 'numeroCaixa').text = '{:03d}'.format(\\n                self.numeroCaixa)\\n\\n        ET.SubElement(infCFe, 'emit')\\n\\n        dest = self.destinatario or Destinatario()\\n        infCFe.append(dest._xml(cancelamento=True))\\n\\n        ET.SubElement(infCFe, 'total')\\n\\n        return cfecanc\"}, {\"identifier\":\"CFeVenda\", \"path\":\"satcfe/entidades.py\", \"snippet\":\"class CFeVenda(Entidade):\\n    \\\"\\\"\\\"\\n    Representa um CF-e de venda.\\n\\n    :param Emitente emitente: Identificação do emitente do CF-e.\\n\\n    :param Destinatario destinatario: Opcional. Identificação do\\n        destinatário.\\n\\n    :param LocalEntrega entrega: Opcional. Informações do local de entrega.\\n\\n    :param list detalhamentos: Uma lista de objetos :class:`ProdutoServico` que\\n        representam os produtos/serviços participantes do CF-e de venda.\\n\\n    :param DescAcrEntr descontos_acrescimos_subtotal: Opcional. Se informado,\\n        deverá ser um objeto :class:`DescAcrEntr` que contenha o valor de\\n        desconto ou acréscimo sobre o subtotal.\\n\\n    :param list pagamentos: Uma lista de objetos :class:`MeioPagamento` que\\n        descrevem cada um dos meios de pagamentos usados no CF-e de venda.\\n\\n    :param InformacoesAdicionais informacoes_adicionais: Opcional.\\n\\n    :param str versaoDadosEnt: Opcional. String contendo a versão do layout\\n        do arquivo de dados do aplicativo comercial. Se não informado será\\n        utilizado o valor da constante ``VERSAO_LAYOUT_ARQUIVO_DADOS_AC`` do\\n        módulo ``constantes`` do\\n        `projeto ``satcomum`` <https://github.com/base4sistemas/satcomum/>`_\\n\\n    :param str CNPJ:  CNPJ da software house, desenvolvedora do aplicativo\\n        comercial, contendo apenas os dígitos do número e incluindo zeros não\\n        significativos, se for o caso (14 dígitos).\\n\\n    :param str signAC:  Assinatura do aplicativo comercial (344 dígitos).\\n\\n    :param int numeroCaixa: Número do caixa ao qual o SAT está conectado.\\n        Normalmente este será o número do caixa de onde parte a solicitação de\\n        cancelamento. Deverá ser um número inteiro entre ``0`` e ``999``.\\n\\n    :param Decimal vCFeLei12741: Opcional. Se informado deve representar a\\n        soma total dos valores aproximados dos tributos, em cumprimento à Lei\\n        nº 12.741/2012.\\n\\n    ..note::\\n\\n        Não há uma classe específica para representar o elemento ``ide``\\n        do grupo ``B01``, já que todos os seus atributos são esperados nesta\\n        classe.\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n            self,\\n            emitente=None,\\n            destinatario=None,\\n            entrega=None,\\n            detalhamentos=None,\\n            descontos_acrescimos_subtotal=None,\\n            pagamentos=None,\\n            informacoes_adicionais=None,\\n            **kwargs):\\n\\n        self._emitente = emitente\\n        self._destinatario = destinatario\\n        self._entrega = entrega\\n        self._detalhamentos = detalhamentos\\n        self._descontos_acrescimos_subtotal = descontos_acrescimos_subtotal\\n        self._pagamentos = pagamentos\\n        self._informacoes_adicionais = informacoes_adicionais\\n\\n        super(CFeVenda, self).__init__(\\n                versaoDadosEnt=constantes.VERSAO_LAYOUT_ARQUIVO_DADOS_AC,\\n                schema={\\n                        'versaoDadosEnt': {\\n                                'type': 'string',\\n                                'required': True,\\n                                'regex': r'^\\\\d{1}\\\\.\\\\d{2}$',\\n                            },\\n                        'CNPJ': {\\n                                'type': 'string',\\n                                'check_with': 'cnpj',\\n                                'required': True,\\n                            },\\n                        'signAC': {\\n                                'type': 'string',\\n                                'check_with': 'assinatura_ac',\\n                                'required': True,\\n                            },\\n                        'numeroCaixa': {\\n                                'type': 'integer',\\n                                'required': True,\\n                                'min': 0,\\n                                'max': 999,\\n                            },\\n                        'vCFeLei12741': {\\n                                'type': 'decimal',\\n                                'required': False,\\n                            },\\n                    }, **kwargs)\\n\\n    @property\\n    def emitente(self):\\n        \\\"\\\"\\\"O :class:`Emitente` do CF-e.\\\"\\\"\\\"\\n        return self._emitente\\n\\n    @property\\n    def destinatario(self):\\n        \\\"\\\"\\\"O :class:`Destinatario` do CF-e ou ``None``.\\\"\\\"\\\"\\n        return self._destinatario\\n\\n    @property\\n    def entrega(self):\\n        \\\"\\\"\\\"O Local de entrega (:class:`LocalEntrega`) ou ``None``.\\\"\\\"\\\"\\n        return self._entrega\\n\\n    @property\\n    def detalhamentos(self):\\n        \\\"\\\"\\\"\\n        Lista de objetos :class:`Detalhamento`, descrevendo os produtos e\\n        serviços do CF-e.\\n        \\\"\\\"\\\"\\n        return tuple(self._detalhamentos or ())\\n\\n    @property\\n    def descontos_acrescimos_subtotal(self):\\n        \\\"\\\"\\\"\\n        Os descontos e acréscimos no subtotal do CF-e (:class:`DescAcrEntr`)\\n        ou ``None``.\\n        \\\"\\\"\\\"\\n        return self._descontos_acrescimos_subtotal\\n\\n    @property\\n    def pagamentos(self):\\n        \\\"\\\"\\\"\\n        Lista de objetos :class`MeioPagamento`, descrevendo os meios de\\n        pagamento empregados na quitação do CF-e.\\n        \\\"\\\"\\\"\\n        return tuple(self._pagamentos or ())\\n\\n    @property\\n    def informacoes_adicionais(self):\\n        \\\"\\\"\\\"\\n        Informações adicionais do CF-e (:class:`InformacoesAdicionais`)\\n        ou ``None``.\\n        \\\"\\\"\\\"\\n        return self._informacoes_adicionais\\n\\n    def _xml(self, *args, **kwargs):\\n        self.erros.clear()\\n        return super(CFeVenda, self)._xml(*args, **kwargs)\\n\\n    def _construir_elemento_xml(self, *args, **kwargs):\\n        cfe = ET.Element('CFe')\\n        infCFe = ET.SubElement(cfe, 'infCFe')\\n        infCFe.attrib['versaoDadosEnt'] = self.versaoDadosEnt\\n\\n        ide = ET.SubElement(infCFe, 'ide')\\n        ET.SubElement(ide, 'CNPJ').text = self.CNPJ\\n        ET.SubElement(ide, 'signAC').text = self.signAC\\n        ET.SubElement(ide, 'numeroCaixa').text = '{:03d}'.format(\\n                self.numeroCaixa)\\n\\n        infCFe.append(self.emitente._xml())\\n\\n        dest = self.destinatario or Destinatario()\\n        infCFe.append(dest._xml())\\n\\n        if self.entrega is not None:\\n            infCFe.append(self.entrega._xml())\\n\\n        if self.detalhamentos:\\n            for n, det in enumerate(self.detalhamentos):\\n                infCFe.append(det._xml(nItem=n+1))\\n\\n        total = ET.SubElement(infCFe, 'total')\\n\\n        if hasattr(self, 'vCFeLei12741'):\\n            ET.SubElement(total, 'vCFeLei12741').text = str(self.vCFeLei12741)\\n\\n        if self.descontos_acrescimos_subtotal is not None:\\n            total.append(self.descontos_acrescimos_subtotal._xml())\\n\\n        pgto = ET.SubElement(infCFe, 'pgto')\\n        if self.pagamentos:\\n            for pg in self.pagamentos:\\n                pgto.append(pg._xml())\\n\\n        if self.informacoes_adicionais is not None:\\n            infCFe.append(self.informacoes_adicionais._xml())\\n\\n        return cfe\"}, {\"identifier\":\"Destinatario\", \"path\":\"satcfe/entidades.py\", \"snippet\":\"class Destinatario(Entidade):\\n    \\\"\\\"\\\"\\n    Identificação do destinatário do CF-e (``dest``, grupo ``E01``).\\n\\n    :param str CNPJ: Número do CNPJ do destinatário, contendo apenas os\\n        digitos e incluindo os zeros não significativos. **Não deve ser\\n        informado se o ``CPF`` for informado.**\\n\\n    :param str CPF: Número do CPF do destinatário, contendo apenas os digitos e\\n        incluindo os zeros não significativos. **Não deve ser informado se o\\n        ``CNPJ`` for informado.**\\n\\n    :param str xNome: Opcional. Nome ou razão social do destinatário.\\n        O nome do destinatário será ignorado no XML do CF-e de cancelamento.\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, **kwargs):\\n        super(Destinatario, self).__init__(schema={\\n                'CNPJ': {  # E02\\n                        'type': 'string',\\n                        'check_with': 'cnpj',\\n                        'excludes': 'CPF',\\n                        'required': False,\\n                    },\\n                'CPF': {  # E03\\n                        'type': 'string',\\n                        'check_with': 'cpf',\\n                        'excludes': 'CNPJ',\\n                        'required': False,\\n                    },\\n                'xNome': {  # E04\\n                        'type': 'string',\\n                        'required': False,\\n                        'minlength': 2,\\n                        'maxlength': 60,\\n                    }\\n            }, **kwargs)\\n\\n    def _construir_elemento_xml(self, *args, **kwargs):\\n        is_cancelamento = kwargs.pop('cancelamento', False)\\n\\n        dest = ET.Element('dest')\\n\\n        if not is_cancelamento:\\n            if hasattr(self, 'CNPJ'):\\n                ET.SubElement(dest, 'CNPJ').text = self.CNPJ\\n\\n            if hasattr(self, 'CPF'):\\n                ET.SubElement(dest, 'CPF').text = self.CPF\\n\\n            if hasattr(self, 'xNome'):\\n                ET.SubElement(dest, 'xNome').text = self.xNome\\n\\n        return dest\"}, {\"identifier\":\"Emitente\", \"path\":\"satcfe/entidades.py\", \"snippet\":\"class Emitente(Entidade):\\n    \\\"\\\"\\\"\\n    Identificação do emitente do CF-e (``emit``, grupo ``C01``).\\n\\n    :param str CNPJ: Número do CNPJ do emitente do CF-e, contendo apenas os\\n        digitos e incluindo os zeros não significativos.\\n\\n    :param str IE: Número de Inscrição Estadual do emitente do CF-e, contendo\\n        apenas digitos.\\n\\n    :param str IM: Opcional. Deve ser informado o número da Inscrição\\n        Municipal quando o CF-e possuir itens com prestação de serviços\\n        sujeitos ao ISSQN, por exemplo.\\n\\n    :param str cRegTribISSQN: Opcional. Indica o regime especial de\\n        tributação do ISSQN. Veja as constantes em\\n        :attr:`~satcomum.constantes.C15_CREGTRIBISSQN_EMIT`.\\n\\n    :param str indRatISSQN: Opcional. Indicador de rateio do desconto sobre o\\n        subtotal entre itens sujeitos à tributação pelo ISSQN. Veja as\\n        constantes em :attr:`~satcomum.constantes.C16_INDRATISSQN_EMIT`.\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, **kwargs):\\n        super(Emitente, self).__init__(schema={\\n                'CNPJ': {\\n                        'type': 'string',\\n                        'check_with': 'cnpj',\\n                        'required': True,\\n                    },\\n                'IE': {\\n                        'type': 'string',\\n                        'required': True,\\n                        'regex': r'^\\\\d{2,12}$'\\n                    },\\n                'IM': {\\n                        'type': 'string',\\n                        'required': False,\\n                        'regex': r'^\\\\d{1,15}$'\\n                    },\\n                'cRegTribISSQN': {\\n                        'type': 'string',\\n                        'required': False,\\n                        'allowed': [\\n                                v for v, s in constantes.C15_CREGTRIBISSQN_EMIT\\n                            ],\\n                    },\\n                'indRatISSQN': {\\n                        'type': 'string',\\n                        'required': True,\\n                        'allowed': [\\n                                v for v, s in constantes.C16_INDRATISSQN_EMIT\\n                            ],\\n                    },\\n            }, **kwargs)\\n\\n    def _construir_elemento_xml(self, *args, **kwargs):\\n        emit = ET.Element('emit')\\n        ET.SubElement(emit, 'CNPJ').text = self.CNPJ\\n        ET.SubElement(emit, 'IE').text = self.IE\\n\\n        if hasattr(self, 'IM'):\\n            ET.SubElement(emit, 'IM').text = self.IM\\n\\n        if hasattr(self, 'cRegTribISSQN'):\\n            ET.SubElement(emit, 'cRegTribISSQN').text = self.cRegTribISSQN\\n\\n        ET.SubElement(emit, 'indRatISSQN').text = self.indRatISSQN\\n\\n        return emit\"}, {\"identifier\":\"LocalEntrega\", \"path\":\"satcfe/entidades.py\", \"snippet\":\"class LocalEntrega(Entidade):\\n    \\\"\\\"\\\"\\n    Identificação do Local de Entrega (``entrega``, grupo ``G01``).\\n\\n    :param str xLgr:\\n    :param str nro:\\n    :param str xCpl: Opcional.\\n    :param str xBairro:\\n    :param str xMun:\\n    :param str UF:\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, **kwargs):\\n        super(LocalEntrega, self).__init__(schema={\\n                'xLgr': {  # G02\\n                        'type': 'string',\\n                        'required': True,\\n                        'minlength': 2,\\n                        'maxlength': 60,\\n                    },\\n                'nro': {  # G03\\n                        'type': 'string',\\n                        'required': True,\\n                        'minlength': 1,\\n                        'maxlength': 60,\\n                    },\\n                'xCpl': {  # G04\\n                        'type': 'string',\\n                        'required': False,\\n                        'minlength': 1,\\n                        'maxlength': 60,\\n                    },\\n                'xBairro': {  # G05\\n                        'type': 'string',\\n                        'required': True,\\n                        'minlength': 1,\\n                        'maxlength': 60,\\n                    },\\n                'xMun': {  # G06\\n                        'type': 'string',\\n                        'required': True,\\n                        'minlength': 2,\\n                        'maxlength': 60,\\n                    },\\n                'UF': {  # G07\\n                        'type': 'string',\\n                        'check_with': 'uf',\\n                        'required': True,\\n                    },\\n            }, **kwargs)\\n\\n    def _construir_elemento_xml(self, *args, **kwargs):\\n        entrega = ET.Element('entrega')\\n        ET.SubElement(entrega, 'xLgr').text = self.xLgr\\n        ET.SubElement(entrega, 'nro').text = self.nro\\n\\n        if hasattr(self, 'xCpl'):\\n            ET.SubElement(entrega, 'xCpl').text = self.xCpl\\n\\n        ET.SubElement(entrega, 'xBairro').text = self.xBairro\\n        ET.SubElement(entrega, 'xMun').text = self.xMun\\n        ET.SubElement(entrega, 'UF').text = self.UF\\n\\n        return entrega\"}, {\"identifier\":\"Detalhamento\", \"path\":\"satcfe/entidades.py\", \"snippet\":\"class Detalhamento(Entidade):\\n    \\\"\\\"\\\"\\n    Detalhamento do produto ou serviço do CF-e (``det``, grupo ``H01``).\\n\\n    :param ProdutoServico produto: Produto ou serviço, como uma instância\\n        de :class:`ProdutoServico` ao qual o detalhamento se refere.\\n\\n    :param Imposto imposto: O grupo de tributos incidentes no produto ou\\n        serviço ao qual o detalhamento se refere, como uma instância\\n        de :class:`Imposto`.\\n\\n    :param str infAdProd: Opcional. Informações adicionais do produto ou\\n        serviço (norma referenciada, informações complementares, etc).\\n\\n    .. note::\\n\\n        O atributo XML ``nItem`` (``H02``) não é determinado aqui, mas\\n        atribuído automaticamente, conforme a sua posição na lista de\\n        :attr:`~CFeVenda.detalhamentos`.\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, produto=None, imposto=None, **kwargs):\\n        self._produto = produto\\n        self._imposto = imposto\\n        super(Detalhamento, self).__init__(schema={\\n               'infAdProd': {  # V01\\n                        'type': 'string',\\n                        'required': False,\\n                        'minlength': 1,\\n                        'maxlength': 500,\\n                    },\\n            }, **kwargs)\\n\\n    @property\\n    def produto(self):\\n        return self._produto\\n\\n    @property\\n    def imposto(self):\\n        return self._imposto\\n\\n    def _construir_elemento_xml(self, *args, **kwargs):\\n        det = ET.Element('det')\\n        det.attrib['nItem'] = str(kwargs.pop('nItem'))\\n\\n        det.append(self.produto._xml())\\n        det.append(self.imposto._xml())\\n\\n        if hasattr(self, 'infAdProd'):\\n            ET.SubElement(det, 'infAdProd').text = self.infAdProd\\n\\n        return det\"}, {\"identifier\":\"ProdutoServico\", \"path\":\"satcfe/entidades.py\", \"snippet\":\"class ProdutoServico(Entidade):\\n    \\\"\\\"\\\"\\n    Produto ou serviço do CF-e (``prod``, grupo ``I01``).\\n\\n    :param str cProd:\\n    :param str cEAN: Opcional.\\n    :param str xProd:\\n    :param str NCM: Opcional.\\n    :param str CFOP:\\n    :param str uCom:\\n    :param Decimal qCom:\\n    :param Decimal vUnCom:\\n    :param str indRegra:\\n    :param Decimal vDesc: Opcional.\\n    :param Decimal vOutro: Opcional.\\n    :param list observacoes_fisco: Opcional. Lista de objetos\\n        :class:`ObsFiscoDet` representando os campos de uso livre do fisco.\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, observacoes_fisco=None, **kwargs):\\n        self._observacoes_fisco = observacoes_fisco\\n        super(ProdutoServico, self).__init__(schema={\\n                'cProd': {  # I02\\n                        'type': 'string',\\n                        'required': True,\\n                        'minlength': 1,\\n                        'maxlength': 60,\\n                    },\\n                'cEAN': {  # I03\\n                        'type': 'string',\\n                        'required': False,\\n                        'regex': r'^(\\\\d{8}|\\\\d{12}|\\\\d{13}|\\\\d{14})$',\\n                    },\\n                'xProd': {  # I04\\n                        'type': 'string',\\n                        'required': True,\\n                        'minlength': 1,\\n                        'maxlength': 120,\\n                    },\\n                'NCM': {  # I05\\n                        'type': 'string',\\n                        'required': False,\\n                        'regex': r'^(\\\\d{2}|\\\\d{8})$',\\n                    },\\n                'CFOP': {  # I06\\n                        'type': 'string',\\n                        'required': True,\\n                        'regex': r'^\\\\d{4}$',\\n                    },\\n                'uCom': {  # I07\\n                        'type': 'string',\\n                        'required': True,\\n                        'minlength': 1,\\n                        'maxlength': 6,\\n                    },\\n                'qCom': {  # I08\\n                        'type': 'decimal',\\n                        'required': True,\\n                    },\\n                'vUnCom': {  # I09\\n                        'type': 'decimal',\\n                        'required': True,\\n                    },\\n                'indRegra': {  # I11\\n                        'type': 'string',\\n                        'required': True,\\n                        'allowed': [v for v, s in constantes.I11_INDREGRA],\\n                    },\\n                'vDesc': {  # I12\\n                        'type': 'decimal',\\n                        'required': False,\\n                    },\\n                'vOutro': {  # I13\\n                        'type': 'decimal',\\n                        'required': False,\\n                    },\\n            }, **kwargs)\\n\\n    @property\\n    def observacoes_fisco(self):\\n        return tuple(self._observacoes_fisco or ())\\n\\n    def _construir_elemento_xml(self, *args, **kwargs):\\n        prod = ET.Element('prod')\\n        ET.SubElement(prod, 'cProd').text = self.cProd\\n\\n        if hasattr(self, 'cEAN'):\\n            ET.SubElement(prod, 'cEAN').text = self.cEAN\\n\\n        ET.SubElement(prod, 'xProd').text = self.xProd\\n\\n        if hasattr(self, 'NCM'):\\n            ET.SubElement(prod, 'NCM').text = self.NCM\\n\\n        ET.SubElement(prod, 'CFOP').text = self.CFOP\\n        ET.SubElement(prod, 'uCom').text = self.uCom\\n        ET.SubElement(prod, 'qCom').text = str(self.qCom)\\n        ET.SubElement(prod, 'vUnCom').text = str(self.vUnCom)\\n        ET.SubElement(prod, 'indRegra').text = self.indRegra\\n\\n        if hasattr(self, 'vDesc'):\\n            ET.SubElement(prod, 'vDesc').text = str(self.vDesc)\\n\\n        if hasattr(self, 'vOutro'):\\n            ET.SubElement(prod, 'vOutro').text = str(self.vOutro)\\n\\n        if self.observacoes_fisco:\\n            for obs in self.observacoes_fisco:\\n                prod.append(obs._xml())\\n\\n        return prod\"}, {\"identifier\":\"Imposto\", \"path\":\"satcfe/entidades.py\", \"snippet\":\"class Imposto(Entidade):\\n    \\\"\\\"\\\"\\n    Grupo de tributos incidentes no produto ou serviço (``imposto``,\\n    grupo ``M01``).\\n\\n    :param icms: Opcional. Deve ser uma instância de uma das classes dos\\n        grupos de ICMS (:class:`ICMS00`, :class:`ICMS40`, :class:`ICMSSN102`\\n        ou :class:`ICMSSN900`) se o item for um produto tributado pelo ICMS\\n        ou ``None`` em caso contrário.\\n\\n    :param pis: Deve ser uma instância de uma das classes dos grupos de\\n        PIS (:class:`PISAliq`, :class:`PISQtde`, :class:`PISNT`, :class:`PISSN`\\n        ou :class:`PISOutr`).\\n\\n    :param pisst: Opcional. Instância de :class:`PISST` ou ``None``.\\n\\n    :param cofins: Deve ser uma instância de uma das classes dos grupos\\n        de COFINS (:class:`COFINSAliq`, :class:`COFINSQtde`, :class:`COFINSNT`,\\n        :class:`COFINSSN` ou :class:`COFINSOutr`).\\n\\n    :param cofinsst: Opcional. Instância de :class:`COFINSST` ou ``None``.\\n\\n    :param issqn: Opcional. Uma instância de :class:`ISSQN` se o item for\\n        um serviço tributado pelo ISSQN ou ``None`` em caso contrário.\\n\\n    :param Decimal vItem12741: Opcional. Valor aproximado dos tributos do\\n        produto ou serviço, conforme a Lei 12.741/12.\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(\\n            self,\\n            icms=None,\\n            pis=None,\\n            pisst=None,\\n            cofins=None,\\n            cofinsst=None,\\n            issqn=None,\\n            **kwargs):\\n        self._icms = icms\\n        self._pis = pis\\n        self._pisst = pisst\\n        self._cofins = cofins\\n        self._cofinsst = cofinsst\\n        self._issqn = issqn\\n        super(Imposto, self).__init__(schema={\\n                'vItem12741': {  # M02\\n                        'type': 'decimal',\\n                        'required': False,\\n                    }\\n            }, **kwargs)\\n\\n    @property\\n    def icms(self):\\n        \\\"\\\"\\\"\\n        Um dos grupos de ICMS (:class:`ICMS00`, :class:`ICMS40`,\\n        :class:`ICMSSN102` ou :class:`ICMSSN900`) se o item for um produto\\n        tributado pelo ICMS ou ``None`` em caso contrário.\\n        \\\"\\\"\\\"\\n        return self._icms\\n\\n    @property\\n    def pis(self):\\n        \\\"\\\"\\\"\\n        Um dos grupos de PIS (:class:`PISAliq`, :class:`PISQtde`,\\n        :class:`PISNT`, :class:`PISSN` ou :class:`PISOutr`).\\n        \\\"\\\"\\\"\\n        return self._pis\\n\\n    @property\\n    def pisst(self):\\n        \\\"\\\"\\\"\\n        O grupo do PIS Substituição Tributária (:class:`PISST`) se for o\\n        caso, ou ``None``.\\n        \\\"\\\"\\\"\\n        return self._pisst\\n\\n    @property\\n    def cofins(self):\\n        \\\"\\\"\\\"\\n        Um dos grupos de COFINS (:class:`COFINSAliq`, :class:`COFINSQtde`,\\n        :class:`COFINSNT`, :class:`COFINSSN` ou :class:`COFINSOutr`).\\n        \\\"\\\"\\\"\\n        return self._cofins\\n\\n    @property\\n    def cofinsst(self):\\n        \\\"\\\"\\\"\\n        O grupo do COFINS Substituição Tributária (:class:`COFINSST`) se\\n        for o caso, ou ``None``.\\n        \\\"\\\"\\\"\\n        return self._cofinsst\\n\\n    @property\\n    def issqn(self):\\n        \\\"\\\"\\\"\\n        O grupo de ISSQN (:class:`ISSQN`) se o item for um serviço\\n        tributado pelo ISSQN ou ``None`` em caso contrário.\\n        \\\"\\\"\\\"\\n        return self._issqn\\n\\n    def _construir_elemento_xml(self, *args, **kwargs):\\n        if self.pis is None:\\n            raise cerberus.DocumentError((\\n                    '{:s} (grupo M01) atributo \\\"pis\\\" não pode ser None.'\\n                ).format(self.__class__.__name__))\\n\\n        if self.cofins is None:\\n            raise cerberus.DocumentError((\\n                    '{:s} (grupo M01) atributo \\\"cofins\\\" não pode ser None.'\\n                ).format(self.__class__.__name__))\\n\\n        imposto = ET.Element('imposto')\\n\\n        if hasattr(self, 'vItem12741'):\\n            ET.SubElement(imposto, 'vItem12741').text = str(self.vItem12741)\\n\\n        if self.icms is not None:\\n            icms = ET.SubElement(imposto, 'ICMS')\\n            icms.append(self.icms._xml())\\n\\n        pis = ET.SubElement(imposto, 'PIS')\\n        pis.append(self.pis._xml())\\n\\n        if self.pisst is not None:\\n            imposto.append(self.pisst._xml())\\n\\n        cofins = ET.SubElement(imposto, 'COFINS')\\n        cofins.append(self.cofins._xml())\\n\\n        if self.cofinsst is not None:\\n            imposto.append(self.cofinsst._xml())\\n\\n        if self.issqn is not None:\\n            imposto.append(self.issqn._xml())\\n\\n        return imposto\"}, {\"identifier\":\"ICMSSN102\", \"path\":\"satcfe/entidades.py\", \"snippet\":\"class ICMSSN102(Entidade):\\n    \\\"\\\"\\\"\\n    Grupo de tributação do ICMS Simples Nacional, CSOSN 102, 300, 400 e 500\\n    (``ICMSSN102``, grupo ``N04``).\\n\\n    :param str Orig:\\n    :param str CSOSN:\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, **kwargs):\\n        super(ICMSSN102, self).__init__(schema={\\n                'Orig': {  # N06\\n                        'type': 'string',\\n                        'required': True,\\n                        'allowed': [v for v, s in constantes.N06_ORIG],\\n                    },\\n                'CSOSN': {  # N10\\n                        'type': 'string',\\n                        'required': True,\\n                        'allowed': [\\n                                v for v, s in constantes.N10_CSOSN_ICMSSN102\\n                            ],\\n                    },\\n            }, **kwargs)\\n\\n    def _construir_elemento_xml(self, *args, **kwargs):\\n        icmssn102 = ET.Element('ICMSSN102')\\n        ET.SubElement(icmssn102, 'Orig').text = self.Orig\\n        ET.SubElement(icmssn102, 'CSOSN').text = self.CSOSN\\n        return icmssn102\"}, {\"identifier\":\"PISSN\", \"path\":\"satcfe/entidades.py\", \"snippet\":\"class PISSN(Entidade):\\n    \\\"\\\"\\\"\\n    Grupo de PIS para contribuíntes do Simples Nacional, CST 49 (``PISSN``,\\n    grupo ``Q05``).\\n\\n    :param str CST:\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, **kwargs):\\n        super(PISSN, self).__init__(schema={\\n                'CST': {\\n                        'type': 'string',\\n                        'required': True,\\n                        'allowed': [v for v, s in constantes.Q07_CST_PISSN],\\n                    },\\n            }, **kwargs)\\n\\n    def _construir_elemento_xml(self, *args, **kwargs):\\n        pissn = ET.Element('PISSN')\\n        ET.SubElement(pissn, 'CST').text = self.CST\\n        return pissn\"}, {\"identifier\":\"COFINSSN\", \"path\":\"satcfe/entidades.py\", \"snippet\":\"class COFINSSN(Entidade):\\n    \\\"\\\"\\\"\\n    Grupo de COFINS para contribuíntes do Simples Nacional, CST 49\\n    (``COFINSSN``, grupo ``S05``).\\n\\n    :param str CST:\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, **kwargs):\\n        super(COFINSSN, self).__init__(schema={\\n                'CST': {\\n                        'type': 'string',\\n                        'required': True,\\n                        'allowed': [v for v, s in constantes.S07_CST_COFINSSN],\\n                    },\\n            }, **kwargs)\\n\\n    def _construir_elemento_xml(self, *args, **kwargs):\\n        cofinssn = ET.Element(self.__class__.__name__)\\n        ET.SubElement(cofinssn, 'CST').text = self.CST\\n        return cofinssn\"}, {\"identifier\":\"MeioPagamento\", \"path\":\"satcfe/entidades.py\", \"snippet\":\"class MeioPagamento(Entidade):\\n    \\\"\\\"\\\"Meio de pagamento (``MP``, grupo ``WA02``).\\n\\n    :param str cMP:\\n    :param Decimal vMP:\\n    :param str cAdmC: Opcional.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, **kwargs):\\n        super(MeioPagamento, self).__init__(schema={\\n                'cMP': {\\n                        'type': 'string',\\n                        'required': True,\\n                        'allowed': [v for v, s in constantes.WA03_CMP_MP],\\n                    },\\n                'vMP': {\\n                        'type': 'decimal',\\n                        'required': True,\\n                    },\\n                'cAdmC': {\\n                        'type': 'string',\\n                        'required': False,\\n                        'allowed': [\\n                                codigo\\n                                for codigo, cnpj, nome\\n                                in constantes.CREDENCIADORAS_CARTAO\\n                            ],\\n                    },\\n            }, **kwargs)\\n\\n    def _construir_elemento_xml(self, *args, **kwargs):\\n        mp = ET.Element('MP')\\n        ET.SubElement(mp, 'cMP').text = self.cMP\\n        ET.SubElement(mp, 'vMP').text = str(self.vMP)\\n        if hasattr(self, 'cAdmC'):\\n            ET.SubElement(mp, 'cAdmC').text = self.cAdmC\\n        return mp\"}, {\"identifier\":\"ConfiguracaoRede\", \"path\":\"satcfe/rede.py\", \"snippet\":\"class ConfiguracaoRede(Entidade):\\n    \\\"\\\"\\\"Uma entidade que contém os parâmetros de configurações da interface de\\n    rede do equipamento SAT. Uma instância desta classe é usada como argumento\\n    para :meth:`~satcfe.base._FuncoesSAT.configurar_interface_de_rede`.\\n\\n    :param str tipoInter: Tipo de interface de rede que o equipamento SAT\\n        deverá utilizar. As opções de tipos de rede estão disponíveis na\\n        constante :attr:`~satcomum.constantes.REDE_TIPOINTER_OPCOES`.\\n\\n    :param str SSID: Opcional. Nome da rede sem fio contendo até 32 caracteres.\\n\\n    :param str seg: Opcional. Tipo de segurança da rede sem fio. As opções\\n        estão na constante :attr:`~satcomum.constantes.REDE_SEG_OPCOES`.\\n\\n    :param str codigo: Opcional. Senha de acesso à rede sem fio, contendo\\n        até 64 caracteres.\\n\\n    :param str tipoLan: Tipo da rede LAN. As opções estão disponíveis na\\n        constante :attr:`~satcomum.constantes.REDE_TIPOLAN_OPCOES`.\\n\\n    :param str lanIP: Opcional. Endereço IP do equipamento SAT.\\n\\n    :param str lanMask: Opcional. Máscara de sub-rede.\\n\\n    :param str lanGW: Opcional. Endereço IP do gateway padrão.\\n\\n    :param str lanDNS1: Opcional. Endereço IP do DNS primário.\\n\\n    :param str lanDNS2: Opcional. Endereço IP do DNS secundário.\\n\\n    :param str usuario: Opcional. Nome do usuário para obtenção do endereço\\n        IP, se necessário, contendo até 64 caracteres.\\n\\n    :param str senha: Opcional. Senha do usuário para obtenção do endereço IP,\\n        relacionado ao parâmetro ``usuario``, se necessário, contendo até 32\\n        caracteres.\\n\\n    :param str proxy: Opcional. Indica a configuração de proxy da rede.\\n        As opções estão disponíveis na\\n        constante :attr:`~satcomum.constantes.REDE_PROXY_OPCOES`.\\n\\n    :param str proxy_ip: Opcional. Endereço IP do servidor proxy.\\n\\n    :param int proxy_porta: Opcional. Número da porta por onde o servidor de\\n        proxy responde.\\n\\n    :param str proxy_user: Opcional. Nome do usuário para acesso ao proxy, se\\n        necessário, contendo até 64 caracteres.\\n\\n    :param str proxy_senha: Opcional. Senha do usuário para acesso ao proxy,\\n        relacionado ao parâmetro ``proxy_user``, se necessário, contendo\\n        até 64 caracteres.\\n\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, **kwargs):\\n        super(ConfiguracaoRede, self).__init__(schema={\\n                'tipoInter': {\\n                        'type': 'string',\\n                        'required': True,\\n                        'allowed': [v for v, s in REDE_TIPOINTER_OPCOES],\\n                    },\\n                'SSID': {\\n                        'type': 'string',\\n                        'required': False,\\n                        'minlength': 1,\\n                        'maxlength': 32,\\n                    },\\n                'seg': {\\n                        'type': 'string',\\n                        'required': False,\\n                        'allowed': [v for v, s in REDE_SEG_OPCOES],\\n                    },\\n                'codigo': {\\n                        'type': 'string',\\n                        'required': False,\\n                        'minlength': 1,\\n                        'maxlength': 64,\\n                    },\\n                'tipoLan': {\\n                        'type': 'string',\\n                        'required': True,\\n                        'allowed': [v for v, s in REDE_TIPOLAN_OPCOES],\\n                    },\\n                'lanIP': {\\n                        'type': 'string',\\n                        'check_with': 'ipv4',\\n                        'required': False,\\n                    },\\n                'lanMask': {\\n                        'type': 'string',\\n                        'check_with': 'ipv4',\\n                        'required': False,\\n                    },\\n                'lanGW': {\\n                        'type': 'string',\\n                        'check_with': 'ipv4',\\n                        'required': False,\\n                    },\\n                'lanDNS1': {\\n                        'type': 'string',\\n                        'check_with': 'ipv4',\\n                        'required': False,\\n                    },\\n                'lanDNS2': {\\n                        'type': 'string',\\n                        'check_with': 'ipv4',\\n                        'required': False,\\n                    },\\n                'usuario': {\\n                        'type': 'string',\\n                        'required': False,\\n                        'minlength': 1, 'maxlength': 64,\\n                    },\\n                'senha': {\\n                        'type': 'string',\\n                        'required': False,\\n                        'minlength': 1,\\n                        'maxlength': 64,\\n                    },\\n                'proxy': {\\n                        'type': 'string',\\n                        'required': False,\\n                        'allowed': [v for v, s in REDE_PROXY_OPCOES],\\n                    },\\n                'proxy_ip': {\\n                        'type': 'string',\\n                        'check_with': 'ipv4',\\n                        'required': False,\\n                    },\\n                'proxy_porta': {\\n                        'type': 'integer',\\n                        'required': False,\\n                        'min': 0,\\n                        'max': 65535,\\n                    },\\n                'proxy_user': {\\n                        'type': 'string',\\n                        'required': False,\\n                        'minlength': 1,\\n                        'maxlength': 64,\\n                    },\\n                'proxy_senha': {\\n                        'type': 'string',\\n                        'required': False,\\n                        'minlength': 1,\\n                        'maxlength': 64,\\n                    },\\n            }, **kwargs)\\n\\n    def _construir_elemento_xml(self, *args, **kwargs):\\n        config = ET.Element('config')\\n        for elemento in self._schema.keys():\\n            valor = getattr(self, elemento, None)\\n            if valor:\\n                if isinstance(valor, int):\\n                    valor = str(valor)\\n                ET.SubElement(config, elemento).text = valor\\n        return config\"}]", "import_statement": "import os\nimport re\nimport shutil\nimport pytest\nfrom collections import namedtuple\nfrom decimal import Decimal\nfrom builtins import str as text\nfrom unidecode import unidecode\nfrom satcomum import constantes\nfrom satcfe.base import BibliotecaSAT\nfrom satcfe.clientelocal import ClienteSATLocal\nfrom satcfe.entidades import CFeCancelamento\nfrom satcfe.entidades import CFeVenda\nfrom satcfe.entidades import Destinatario\nfrom satcfe.entidades import Emitente\nfrom satcfe.entidades import LocalEntrega\nfrom satcfe.entidades import Detalhamento\nfrom satcfe.entidades import ProdutoServico\nfrom satcfe.entidades import Imposto\nfrom satcfe.entidades import ICMSSN102\nfrom satcfe.entidades import PISSN\nfrom satcfe.entidades import COFINSSN\nfrom satcfe.entidades import MeioPagamento\nfrom satcfe.rede import ConfiguracaoRede", "code": "            type=int,\n            help=(\n                    'Convencao de chamada para a biblioteca SAT ({:s})'\n                ).format(_valores_possiveis(constantes.CONVENCOES_CHAMADA)))\n\n    # TODO: implementar testes para acesso compartilhado ao equipamento SAT\n    # --sathub-host     127.0.0.1\n    # --sathub-port     8080\n    # --sathub-baseurl  /hub/v1/\n    # --sathub-username\n    # --sathub-password\n\n    # Opções para permitir, via linha de comando, a execução de testes\n    # marcados como testes que acessam à biblioteca SAT; por exemplo:\n    #\n    #     python setup.py test -a \"--acessa-sat --invoca-ativarsat\"\n    #\n    # Irá permitir que sejam executados os testes que indiquem acesso à\n    # biblioteca SAT e/ou que acessem a função AtivarSAT, por exemplo:\n    #\n    #     @pytest.mark.acessa_sat\n    #     @pytest.mark.invoca_ativarsat\n    #     def test_meu_teste_que_acessa_funcao_ativarsat(clientesatlocal):\n    #         assert 1 == 0, \"Quem diria!\"\n    #\n    for el in _MARKERS:\n        option_name = _to_cmd_line_option(el.name)\n        parser.addoption(\n                option_name,\n                action='store_true',\n                default=False,\n                help=el.option_help)\n\n\ndef pytest_collection_modifyitems(config, items):\n    markers_set = {}\n\n    for el in _MARKERS:\n        option_name = _to_cmd_line_option(el.name)\n        if not config.getoption(option_name):\n            markers_set[el.name] = pytest.mark.skip(reason=el.reason)\n\n    for item in items:\n        for key, marker in markers_set.items():\n            if key in item.keywords:\n                item.add_marker(marker)\n\n\n@pytest.fixture(scope='function')\ndef datadir(tmpdir, request):\n    \"\"\"Este fixture procura por um diretório de dados com o mesmo nome do\n    módulo de teste (mas sem a extensão \".py\" e com o prefixo \"test_\" e/ou o\n    sufixo \"_test\" removidos) dentro de ``satcfe/tests/data/``.\n\n    Por exemplo, o módulo de testes ``tests/test_ativarsat.py`` possui um\n    diretório de dados em ``tests/data/ativarsat/``.\n\n    Existindo um diretório de dados, todo seu conteúdo será copiado para o\n    diretório temporário (*fixture* ``tmpdir`` de pytest), de modo que os\n    testes possam usar esses arquivos.\n\n    No exemplo abaixo, os arquivos ``data.txt`` e ``huge.txt`` deverão ser\n    criados no diretório ``tests/data/foo/``. Note que o diretório ``data/``\n    está no mesmo diretório em que módulo ``test_foo.py`` se encontra:\n\n    .. sourcecode:: python\n\n        def test_foo(datadir):\n            with open(datadir.join('data.txt'), 'r') as f_data, \\\n                 open(datadir.join('huge.txt'), 'r') as f_huge:\n                data = f_data.read()\n                expected_result = f_huge.read()\n            result = do_something_with(data)\n            assert result == expected_result\n\n    .. note::\n\n        Baseado `nesta resposta <https://stackoverflow.com/a/29631801>`_ de\n        Stack Overflow.\n\n    \"\"\"\n    path, name = os.path.split(request.module.__file__)\n    name, _ = os.path.splitext(name)\n    name = re.sub(r'^test_|_test$', '', name)\n    dirname = os.path.join(path, 'data', name)\n\n    if os.path.isdir(dirname):\n        for name in os.listdir(dirname):\n            filename = os.path.join(dirname, name)\n            if os.path.isfile(filename):\n                # copia apenas arquivos regulares (os.path.isfile)\n                dst = text(tmpdir)\n                shutil.copy(filename, dst)\n\n    return tmpdir\n\n\n@pytest.fixture(scope='module')\ndef clientesatlocal(request):\n    funcoes = ClienteSATLocal(\n            BibliotecaSAT(\n                    request.config.getoption('--lib-caminho'),\n                    convencao=request.config.getoption('--lib-convencao')),\n            codigo_ativacao=request.config.getoption('--codigo-ativacao'))\n    return funcoes\n\n\n@pytest.fixture(scope='module')\ndef cfevenda(request):\n    _opcao = request.config.getoption\n    cfe = CFeVenda(\n            CNPJ=_opcao('--cnpj-ac'),\n            signAC=_opcao('--assinatura-ac'),\n            numeroCaixa=_opcao('--numero-caixa'),\n            emitente=Emitente(\n                    CNPJ=_opcao('--emitente-cnpj'),\n                    IE=_opcao('--emitente-ie'),\n                    IM=_opcao('--emitente-im'),\n                    cRegTribISSQN=_opcao('--emitente-issqn-regime'),\n                    indRatISSQN=_opcao('--emitente-issqn-rateio')),\n", "next_line": "            destinatario=Destinatario(", "gold_snippet_index": 4, "id": 0, "__internal_uuid__": "edfd2b58-3825-4b3c-9bc5-45273fe80e86"}
{"repo_name": "vmagamedov/kinko", "file_path": "tests/test_checker.py", "context": "[{\"identifier\":\"ArgRef\", \"path\":\"kinko/refs.py\", \"snippet\":\"class ArgRef(Reference):\\n\\n    def __init__(self, name):\\n        super(ArgRef, self).__init__(None)\\n        self.name = name\\n\\n    def __repr__(self):\\n        return \\\"#{}\\\".format(self.name)\"}, {\"identifier\":\"Tuple\", \"path\":\"kinko/nodes.py\", \"snippet\":\"class Tuple(Node):\\n\\n    def __init__(self, values, **kw):\\n        self.values = tuple(values)\\n        super(Tuple, self).__init__(**kw)\\n\\n    def __repr__(self):\\n        return '({})'.format(' '.join(map(repr, self.values)))\\n\\n    def accept(self, visitor):\\n        return visitor.visit_tuple(self)\"}, {\"identifier\":\"Symbol\", \"path\":\"kinko/nodes.py\", \"snippet\":\"class Symbol(Node):\\n\\n    def __init__(self, name, **kw):\\n        self.name = name\\n        head, sep, tail = name.partition('/')\\n        if sep:\\n            self.ns, self.rel = head, tail\\n        else:\\n            self.ns, self.rel = None, name\\n        super(Symbol, self).__init__(**kw)\\n\\n    def __repr__(self):\\n        return self.name\\n\\n    def accept(self, visitor):\\n        return visitor.visit_symbol(self)\"}, {\"identifier\":\"Number\", \"path\":\"kinko/nodes.py\", \"snippet\":\"class Number(Node):\\n\\n    def __init__(self, value, **kw):\\n        self.value = value\\n        super(Number, self).__init__(**kw)\\n\\n    def __repr__(self):\\n        return repr(self.value)\\n\\n    def accept(self, visitor):\\n        return visitor.visit_number(self)\"}, {\"identifier\":\"Keyword\", \"path\":\"kinko/nodes.py\", \"snippet\":\"class Keyword(Node):\\n\\n    def __init__(self, name, **kw):\\n        self.name = name\\n        super(Keyword, self).__init__(**kw)\\n\\n    def __repr__(self):\\n        return ':{}'.format(self.name)\\n\\n    def accept(self, visitor):\\n        return visitor.visit_keyword(self)\"}, {\"identifier\":\"List\", \"path\":\"kinko/nodes.py\", \"snippet\":\"class List(Node):\\n\\n    def __init__(self, values, **kw):\\n        self.values = tuple(values)\\n        super(List, self).__init__(**kw)\\n\\n    def __repr__(self):\\n        return '[{}]'.format(' '.join(map(repr, self.values)))\\n\\n    def accept(self, visitor):\\n        return visitor.visit_list(self)\"}, {\"identifier\":\"Placeholder\", \"path\":\"kinko/nodes.py\", \"snippet\":\"class Placeholder(Node):\\n\\n    def __init__(self, name, **kw):\\n        self.name = name\\n        super(Placeholder, self).__init__(**kw)\\n\\n    def __repr__(self):\\n        return '#{}'.format(self.name)\\n\\n    def accept(self, visitor):\\n        return visitor.visit_placeholder(self)\"}, {\"identifier\":\"String\", \"path\":\"kinko/nodes.py\", \"snippet\":\"class String(Node):\\n\\n    def __init__(self, value, **kw):\\n        self.value = text_type(value)\\n        super(String, self).__init__(**kw)\\n\\n    def __repr__(self):\\n        return encode_basestring_ascii(self.value)\\n\\n    def accept(self, visitor):\\n        return visitor.visit_string(self)\"}, {\"identifier\":\"Func\", \"path\":\"kinko/types.py\", \"snippet\":\"class Func(with_metaclass(FuncMeta, object)):\\n    pass\"}, {\"identifier\":\"IntType\", \"path\":\"kinko/types.py\", \"snippet\":\"class IntType(with_metaclass(IntTypeMeta, object)):\\n    pass\"}, {\"identifier\":\"StringType\", \"path\":\"kinko/types.py\", \"snippet\":\"class StringType(with_metaclass(StringTypeMeta, object)):\\n    pass\"}, {\"identifier\":\"NamedArg\", \"path\":\"kinko/types.py\", \"snippet\":\"class NamedArg(with_metaclass(NamedArgMeta, object)):\\n    pass\"}, {\"identifier\":\"TypeVar\", \"path\":\"kinko/types.py\", \"snippet\":\"class TypeVar(with_metaclass(TypeVarMeta, object)):\\n    pass\"}, {\"identifier\":\"Markup\", \"path\":\"kinko/types.py\", \"snippet\":\"class Markup(with_metaclass(MarkupMeta, object)):\\n    pass\"}, {\"identifier\":\"Record\", \"path\":\"kinko/types.py\", \"snippet\":\"class Record(with_metaclass(RecordMeta, object)):\\n    pass\"}, {\"identifier\":\"ListType\", \"path\":\"kinko/types.py\", \"snippet\":\"class ListType(with_metaclass(ListTypeMeta, object)):\\n    pass\"}, {\"identifier\":\"Union\", \"path\":\"kinko/types.py\", \"snippet\":\"class Union(with_metaclass(UnionMeta, object)):\\n    pass\"}, {\"identifier\":\"DictType\", \"path\":\"kinko/types.py\", \"snippet\":\"class DictType(with_metaclass(DictTypeMeta, object)):\\n    pass\"}, {\"identifier\":\"Option\", \"path\":\"kinko/types.py\", \"snippet\":\"class Option(with_metaclass(OptionMeta, object)):\\n    pass\"}, {\"identifier\":\"VarArgs\", \"path\":\"kinko/types.py\", \"snippet\":\"class VarArgs(with_metaclass(VarArgsMeta, object)):\\n    pass\"}, {\"identifier\":\"VarNamedArgs\", \"path\":\"kinko/types.py\", \"snippet\":\"class VarNamedArgs(with_metaclass(VarNamedArgsMeta, object)):\\n    pass\"}, {\"identifier\":\"BoolType\", \"path\":\"kinko/types.py\", \"snippet\":\"class BoolType(with_metaclass(BoolTypeMeta, object)):\\n    pass\"}, {\"identifier\":\"RecordMeta\", \"path\":\"kinko/types.py\", \"snippet\":\"class RecordMeta(TypingMeta):\\n\\n    def __cls_init__(cls, items):\\n        cls.__items__ = dict(items)\\n\\n    def __repr__(cls):\\n        return '{}{{{}}}'.format(\\n            cls.__name__,\\n            ' '.join(':{} {!r}'.format(*i) for i in cls.__items__.items()),\\n        )\\n\\n    def accept(cls, visitor):\\n        return visitor.visit_record(cls)\"}, {\"identifier\":\"Errors\", \"path\":\"kinko/errors.py\", \"snippet\":\"class Errors(object):\\n\\n    def __init__(self):\\n        self.list = []\\n        self._stack = [None]\\n\\n    @contextmanager\\n    def module_ctx(self, module):\\n        self._stack.append(Func(module, None))\\n        try:\\n            yield\\n        finally:\\n            self._stack.pop()\\n\\n    @contextmanager\\n    def func_ctx(self, module, name):\\n        self._stack.append(Func(module, name))\\n        try:\\n            yield\\n        finally:\\n            self._stack.pop()\\n\\n    @contextmanager\\n    def location(self, location):\\n        try:\\n            yield\\n        except UserError as e:\\n            self.error(location, text_type(e))\\n            raise\\n\\n    def warn(self, location, message):\\n        self.list.append(Error(self._stack[-1], location, message, WARNING))\\n\\n    def error(self, location, message):\\n        self.list.append(Error(self._stack[-1], location, message, ERROR))\"}, {\"identifier\":\"UserError\", \"path\":\"kinko/errors.py\", \"snippet\":\"class UserError(Exception):\\n    pass\"}, {\"identifier\":\"ERROR\", \"path\":\"kinko/errors.py\", \"snippet\":\"ERROR = 2\"}, {\"identifier\":\"WARNING\", \"path\":\"kinko/errors.py\", \"snippet\":\"WARNING = 1\"}, {\"identifier\":\"Environ\", \"path\":\"kinko/checker.py\", \"snippet\":\"class Environ(object):\\n\\n    def __init__(self, defs=None, errors=None):\\n        self.defs = defs or {}\\n        self.vars = deque([{}])\\n        self._root = TypeVar[None]\\n        self.errors = Errors() if errors is None else errors\\n\\n    @contextmanager\\n    def push(self, mapping):\\n        self.vars.append(mapping)\\n        try:\\n            yield\\n        finally:\\n            self.vars.pop()\\n\\n    def __getitem__(self, key):\\n        for d in reversed(self.vars):\\n            try:\\n                return d[key]\\n            except KeyError:\\n                continue\\n        else:\\n            type_ = self.defs[key]\\n            if isinstance(type_, FuncMeta):\\n                return type_\\n            else:\\n                var = TypeVar[type_]\\n                var.__backref__ = CtxRef(key)\\n                return var\\n\\n    def __contains__(self, key):\\n        return any(key in d for d in self.vars) or key in self.defs\\n\\n    def define(self, name, value):\\n        self.defs[name] = value\"}, {\"identifier\":\"check\", \"path\":\"kinko/checker.py\", \"snippet\":\"def check(node, env):\\n    if isinstance(node, Tuple):\\n        return check_expr(node, env)\\n\\n    elif isinstance(node, Symbol):\\n        try:\\n            t = env[node.name]\\n        except KeyError:\\n            with env.errors.location(node.location):\\n                raise TypeCheckError('Undefined variable {}'\\n                                     .format(node.name))\\n        else:\\n            return node.clone_with(node.name, type=t)\\n\\n    elif isinstance(node, Placeholder):\\n        return node.clone_with(node.name, type=env[node.name])\\n\\n    elif isinstance(node, String):\\n        return node.clone_with(node.value, type=StringType)\\n\\n    elif isinstance(node, Number):\\n        return node.clone_with(node.value, type=IntType)\\n\\n    elif isinstance(node, List):\\n        values = [check(v, env) for v in node.values]\\n        type_ = ListType[Union[(v.__type__ for v in values)]]\\n        return node.clone_with(values, type=type_)\\n\\n    raise NotImplementedError(repr(node))\"}, {\"identifier\":\"TypeCheckError\", \"path\":\"kinko/checker.py\", \"snippet\":\"class TypeCheckError(UserError):\\n    pass\"}, {\"identifier\":\"EACH_TYPE\", \"path\":\"kinko/checker.py\", \"snippet\":\"EACH_TYPE = Func[[__var.symbol, ListType[__var.item], _MarkupLike],\\n                 Markup]\"}, {\"identifier\":\"_FreshVars\", \"path\":\"kinko/checker.py\", \"snippet\":\"class _FreshVars(TypeTransformer):\\n\\n    def __init__(self):\\n        self._mapping = {}\\n\\n    def visit_typevar(self, type_):\\n        if type_.__instance__ is None:\\n            if type_ not in self._mapping:\\n                self._mapping[type_] = TypeVar[None]\\n            return self._mapping[type_]\\n        return self.visit(type_.__instance__)\"}, {\"identifier\":\"LET_TYPE\", \"path\":\"kinko/checker.py\", \"snippet\":\"LET_TYPE = Func[[__var.bindings, __var.expr], __var.result]\"}, {\"identifier\":\"DEF_TYPE\", \"path\":\"kinko/checker.py\", \"snippet\":\"DEF_TYPE = Func[[__var.name, __var.body], __var.result]\"}, {\"identifier\":\"GET_TYPE\", \"path\":\"kinko/checker.py\", \"snippet\":\"GET_TYPE = Func[[Record[{}], __var.key], __var.result]\"}, {\"identifier\":\"IF2_TYPE\", \"path\":\"kinko/checker.py\", \"snippet\":\"IF2_TYPE = Func[[BoolType, __var.then_, __var.else_], __var.result]\"}, {\"identifier\":\"IF_SOME1_TYPE\", \"path\":\"kinko/checker.py\", \"snippet\":\"IF_SOME1_TYPE = Func[[__var.bind, __var.then_], __var.result]\"}, {\"identifier\":\"unify\", \"path\":\"kinko/checker.py\", \"snippet\":\"def unify(t1, t2, backref=None):\\n    \\\"\\\"\\\"Unify `t1` to match `t2`\\n\\n    After unification `t1` should be equal to `t2` or `t1` would be\\n    a subtype of `t2`.\\n\\n    `t1` may contain type variables. If `t1` comes from arguments and\\n    contains a record, this record will be extended. `t1`, also, may contain\\n    type references.\\n\\n    `t2` can not contain bound type variables, only unbound type variables\\n    are allowed - they're represent polymorphic types in the function signature.\\n    `t2`, also, can not contain type references.\\n    \\\"\\\"\\\"\\n    if isinstance(t1, TypeRefMeta):\\n        assert t1.__ref__ is not None, 'Unbound type reference {!r}'.format(t1)\\n        unify(t1.__ref__(), t2, backref)\\n\\n    elif isinstance(t2, TypeRefMeta):\\n        assert False  # type references are not expected in t2\\n\\n    elif isinstance(t2, TypeVarMeta) and t2.__instance__ is None:\\n        assert isinstance(backref, Reference) or backref is None, repr(backref)\\n        t2.__instance__ = t1\\n        t2.__backref__ = backref\\n\\n    elif isinstance(t2, TypeVarMeta):\\n        assert False  # bound type-vars are not expected in t2\\n\\n    elif isinstance(t1, TypeVarMeta) and t1.__instance__ is None:\\n        backref = t1 if t1.__backref__ else backref\\n        if backref and isinstance(t2, RecordMeta):\\n            t1.__instance__ = Record[field_refs(backref, t2.__items__)]\\n            unify(t1, t2, backref)\\n        elif backref and isinstance(t2, ListTypeMeta):\\n            t1.__instance__ = ListType[item_ref(backref)]\\n            unify(t1, t2, backref)\\n        elif backref and isinstance(t2, DictTypeMeta):\\n            raise NotImplementedError('TODO')\\n        else:\\n            t1.__instance__ = t2\\n\\n    elif isinstance(t1, TypeVarMeta):\\n        backref = t1 if t1.__backref__ else backref\\n        try:\\n            unify(t1.__instance__, t2, backref)\\n        except TypeCheckError:\\n            if (\\n                backref and is_from_arg(backref) and\\n                not isinstance(t1.__instance__, TypingMeta) and\\n                isinstance(t2, type(t1.__instance__))\\n            ):\\n                t1.__instance__ = t2\\n            else:\\n                raise\\n\\n    else:\\n        if isinstance(t1, UnionMeta):\\n            # all types from t1 union should unify with t2\\n            for t in t1.__types__:\\n                unify(t, t2, backref)\\n            return\\n        elif isinstance(t2, UnionMeta):\\n            # t1 should unify with at least one type from t2 union\\n            for t in t2.__types__:\\n                try:\\n                    unify(t1, t, backref)\\n                except TypeCheckError:\\n                    continue\\n                else:\\n                    return\\n            # not unified\\n        else:\\n            if isinstance(t1, type(t2)):\\n                if isinstance(t1, RecordMeta):\\n                    s1, s2 = set(t1.__items__), set(t2.__items__)\\n                    if backref and is_from_arg(backref):\\n                        t1.__items__.update(field_refs(backref, s2 - s1))\\n                    else:\\n                        if s2 - s1:\\n                            raise TypeCheckError('Missing keys {} in {!r}'\\n                                                 .format(s2 - s1, t1))\\n                    for k, v2 in t2.__items__.items():\\n                        unify(t1.__items__[k], v2, FieldRef(backref, k))\\n                    return\\n\\n                elif isinstance(t1, ListTypeMeta):\\n                    unify(t1.__item_type__, t2.__item_type__, ItemRef(backref))\\n                    return\\n\\n                elif isinstance(t1, DictTypeMeta):\\n                    unify(t1.__key_type__, t2.__key_type__, backref)\\n                    unify(t1.__value_type__, t2.__value_type__, backref)\\n                    return\\n\\n                if not isinstance(t1, TypingMeta):\\n                    # means simple type with nullary constructor\\n                    return\\n\\n        raise TypeCheckError('Unexpected type: {!r}, instead of: {!r}'\\n                             .format(t1, t2))\"}, {\"identifier\":\"NamesResolver\", \"path\":\"kinko/checker.py\", \"snippet\":\"class NamesResolver(NodeTransformer):\\n\\n    def __init__(self, ns):\\n        self.ns = ns\\n\\n    def visit_symbol(self, node):\\n        ns, sep, name = node.name.partition('/')\\n        if name and ns == '.':\\n            return node.clone_with(sep.join([self.ns, name]))\\n        else:\\n            return node\\n\\n    def visit_tuple(self, node):\\n        if node.values[0].name == 'def':\\n            (def_sym, name_sym), body = node.values[:2], node.values[2:]\\n            qualified_name = '/'.join([self.ns, name_sym.name])\\n            name_sym = name_sym.clone_with(qualified_name)\\n            values = ([self.visit(def_sym), name_sym] +\\n                      [self.visit(i) for i in body])\\n            return node.clone_with(values)\\n        else:\\n            return super(NamesResolver, self).visit_tuple(node)\"}, {\"identifier\":\"def_types\", \"path\":\"kinko/checker.py\", \"snippet\":\"def def_types(node):\\n    assert isinstance(node, List), type(node)\\n    return {d.values[1].name: Unchecked(d, False)\\n            for d in node.values if isinstance(d.values[1], Symbol)}\"}, {\"identifier\":\"IF1_TYPE\", \"path\":\"kinko/checker.py\", \"snippet\":\"IF1_TYPE = Func[[BoolType, __var.then_], __var.result]\"}, {\"identifier\":\"IF3_TYPE\", \"path\":\"kinko/checker.py\", \"snippet\":\"IF3_TYPE = Func[[BoolType, NamedArg['then', __var.then_],\\n                 NamedArg['else', __var.else_]],\\n                __var.result]\"}, {\"identifier\":\"match_fn\", \"path\":\"kinko/checker.py\", \"snippet\":\"def match_fn(fn_types, args):\\n    pos_args, kw_args = split_args(args)\\n    for fn_type in fn_types:\\n        try:\\n            norm_args, norm_args_pos = \\\\\\n                normalize_args(fn_type, args, pos_args, kw_args)\\n        except SignatureMismatch:\\n            continue\\n        else:\\n            return fn_type, norm_args, norm_args_pos\\n    else:\\n        raise TypeCheckError('Function signature mismatch')\"}, {\"identifier\":\"restore_args\", \"path\":\"kinko/checker.py\", \"snippet\":\"def restore_args(fn_type, args, norm_args, norm_args_pos):\\n    args_map = {}\\n    _norm_args = list(norm_args)\\n    _norm_args_pos = list(norm_args_pos)\\n    for arg_type in fn_type.__args__:\\n        if isinstance(arg_type, NamedArgMeta):\\n            value = _norm_args.pop(0)\\n            value_pos = _norm_args_pos.pop(0)\\n            args_map[value_pos - 1] = args[value_pos - 1]  # keyword\\n            args_map[value_pos] = value\\n        elif isinstance(arg_type, VarArgsMeta):\\n            values = _norm_args.pop(0)\\n            values_pos = _norm_args_pos.pop(0)\\n            for v, v_pos in zip(values, values_pos):\\n                args_map[v_pos] = v\\n        elif isinstance(arg_type, VarNamedArgsMeta):\\n            values_map = _norm_args.pop(0)\\n            values_pos = _norm_args_pos.pop(0)\\n            for key in values_map.keys():\\n                value = values_map[key]\\n                value_pos = values_pos[key]\\n                args_map[value_pos - 1] = args[value_pos - 1]  # keyword\\n                args_map[value_pos] = value\\n        else:\\n            value = _norm_args.pop(0)\\n            value_pos = _norm_args_pos.pop(0)\\n            args_map[value_pos] = value\\n    assert not _norm_args\\n    assert len(args_map) == len(args)\\n    return [args_map[i] for i in range(len(args))]\"}, {\"identifier\":\"HTML_TAG_TYPE\", \"path\":\"kinko/checker.py\", \"snippet\":\"HTML_TAG_TYPE = Func[[VarNamedArgs[_StringLike], VarArgs[_MarkupLike]],\\n                     Markup]\"}, {\"identifier\":\"IF_SOME2_TYPE\", \"path\":\"kinko/checker.py\", \"snippet\":\"IF_SOME2_TYPE = Func[[__var.bind, __var.then_, __var.else_], __var.result]\"}, {\"identifier\":\"IF_SOME3_TYPE\", \"path\":\"kinko/checker.py\", \"snippet\":\"IF_SOME3_TYPE = Func[[__var.bind, NamedArg['then', __var.then_],\\n                      NamedArg['else', __var.else_]],\\n                     __var.result]\"}, {\"identifier\":\"NODE_EQ_PATCHER\", \"path\":\"tests/base.py\", \"snippet\":\"NODE_EQ_PATCHER = patch.multiple(Node, __eq__=_node_eq, __ne__=_ne)\"}, {\"identifier\":\"TYPE_EQ_PATCHER\", \"path\":\"tests/base.py\", \"snippet\":\"TYPE_EQ_PATCHER = patch.multiple(GenericMeta, __eq__=_type_eq, __ne__=_ne)\"}, {\"identifier\":\"parse\", \"path\":\"tests/test_parser.py\", \"snippet\":\"def parse(src):\\n    node = _parse(list(tokenize(src)))\\n    LocationChecker().visit(node)\\n    return node\"}, {\"identifier\":\"LocationChecker\", \"path\":\"tests/test_parser.py\", \"snippet\":\"class LocationChecker(NodeVisitor):\\n\\n    def visit(self, node):\\n        assert node.location\\n        super(LocationChecker, self).visit(node)\"}]", "import_statement": "from textwrap import dedent\nfrom kinko.refs import ArgRef\nfrom kinko.nodes import Tuple, Symbol, Number, Keyword, List, Placeholder\nfrom kinko.nodes import String\nfrom kinko.types import Func, IntType, StringType, NamedArg, TypeVar, Markup\nfrom kinko.types import Record, ListType, Union, DictType, Option\nfrom kinko.types import VarArgs, VarNamedArgs, BoolType, RecordMeta\nfrom kinko.errors import Errors, UserError, ERROR, WARNING\nfrom kinko.checker import Environ, check, TypeCheckError, EACH_TYPE, _FreshVars\nfrom kinko.checker import LET_TYPE, DEF_TYPE, GET_TYPE, IF2_TYPE, IF_SOME1_TYPE\nfrom kinko.checker import unify, NamesResolver, def_types, IF1_TYPE, IF3_TYPE\nfrom kinko.checker import match_fn, restore_args, HTML_TAG_TYPE, IF_SOME2_TYPE\nfrom kinko.checker import IF_SOME3_TYPE\nfrom .base import NODE_EQ_PATCHER, TYPE_EQ_PATCHER\nfrom .test_parser import parse, LocationChecker\nimport py.test", "code": "\n\n\n\n\ndef check_expr(src, env=None, errors=None):\n    node = check(parse(src).values[0], Environ(env, errors))\n    LocationChecker().visit(node)\n    return node\n\n\ndef check_exprs(src, env=None, errors=None):\n    node = parse(src)\n    env = env or {}\n", "next_line": "    env.update(def_types(node))", "gold_snippet_index": 39, "id": 1, "__internal_uuid__": "30b15c2c-9b2d-409c-b880-01832dd39c04"}
{"repo_name": "itkinside/ufs", "file_path": "itkufs/accounting/views/edit.py", "context": "[{\"identifier\":\"callsign_sorted\", \"path\":\"itkufs/common/utils.py\", \"snippet\":\"def callsign_sorted(accounts):\\n    \\\"\\\"\\\"\\n    ARK friendly sort method that should sort in the following order:\\n\\n    1. LAxxxx signs\\n    2. LBxxxx signs\\n    3. xxxxxx signs\\n    4. Other names\\n    \\\"\\\"\\\"\\n    return sorted(accounts, key=callsign_key)\"}, {\"identifier\":\"limit_to_owner\", \"path\":\"itkufs/common/decorators.py\", \"snippet\":\"def limit_to_owner(function):\\n    def wrapped(request, *args, **kwargs):\\n        # Let admins through immediately\\n        assert \\\"is_admin\\\" in kwargs\\n        if kwargs[\\\"is_admin\\\"]:\\n            return function(request, *args, **kwargs)\\n\\n        # Check if non-admin are members of the group\\n        if \\\"is_owner\\\" in kwargs and kwargs[\\\"is_owner\\\"]:\\n            return function(request, *args, **kwargs)\\n\\n        # All other\\n        return HttpResponseForbidden(\\n            _(\\\"Forbidden if not account owner or group admin.\\\")\\n        )\\n\\n    return wrapped\"}, {\"identifier\":\"limit_to_admin\", \"path\":\"itkufs/common/decorators.py\", \"snippet\":\"def limit_to_admin(function):\\n    def wrapped(request, *args, **kwargs):\\n        # Let admins through immediately\\n        assert \\\"is_admin\\\" in kwargs\\n        if kwargs[\\\"is_admin\\\"]:\\n            return function(request, *args, **kwargs)\\n\\n        # All other\\n        return HttpResponseForbidden(_(\\\"Forbidden if not group admin.\\\"))\\n\\n    return wrapped\"}, {\"identifier\":\"Account\", \"path\":\"itkufs/accounting/models.py\", \"snippet\":\"class Account(models.Model):\\n    ASSET_ACCOUNT = \\\"As\\\"  # Eiendeler/aktiva\\n    LIABILITY_ACCOUNT = \\\"Li\\\"  # Gjeld/passiva\\n    EQUITY_ACCOUNT = \\\"Eq\\\"  # Egenkapital\\n    INCOME_ACCOUNT = \\\"In\\\"  # Inntekt\\n    EXPENSE_ACCOUNT = \\\"Ex\\\"  # Utgift\\n    ACCOUNT_TYPE = (\\n        (ASSET_ACCOUNT, _(\\\"Asset\\\")),\\n        (LIABILITY_ACCOUNT, _(\\\"Liability\\\")),\\n        (EQUITY_ACCOUNT, _(\\\"Equity\\\")),\\n        (INCOME_ACCOUNT, _(\\\"Income\\\")),\\n        (EXPENSE_ACCOUNT, _(\\\"Expense\\\")),\\n    )\\n\\n    objects = AccountManager()\\n\\n    name = models.CharField(_(\\\"name\\\"), max_length=100)\\n    short_name = models.CharField(_(\\\"short name\\\"), max_length=100, blank=True)\\n    slug = models.SlugField(\\n        _(\\\"slug\\\"), help_text=_(\\\"A shortname used in URLs etc.\\\")\\n    )\\n    group = models.ForeignKey(\\n        Group, on_delete=models.CASCADE, verbose_name=_(\\\"group\\\")\\n    )\\n    type = models.CharField(\\n        _(\\\"type\\\"), max_length=2, choices=ACCOUNT_TYPE, default=LIABILITY_ACCOUNT\\n    )\\n    owner = models.ForeignKey(\\n        User,\\n        on_delete=models.CASCADE,\\n        null=True,\\n        blank=True,\\n        verbose_name=_(\\\"owner\\\"),\\n    )\\n    active = models.BooleanField(_(\\\"active\\\"), default=True)\\n    ignore_block_limit = models.BooleanField(\\n        _(\\\"ignore block limit\\\"),\\n        default=False,\\n        help_text=_(\\\"Never block account automatically\\\"),\\n    )\\n    blocked = models.BooleanField(\\n        _(\\\"blocked\\\"), default=False, help_text=_(\\\"Block account manually\\\")\\n    )\\n    group_account = models.BooleanField(\\n        _(\\\"group account\\\"),\\n        default=False,\\n        help_text=_(\\\"Does this account belong to the group?\\\"),\\n    )\\n\\n    class Meta:\\n        ordering = (\\\"group\\\", \\\"name\\\")\\n        unique_together = ((\\\"slug\\\", \\\"group\\\"), (\\\"owner\\\", \\\"group\\\"))\\n        verbose_name = _(\\\"account\\\")\\n        verbose_name_plural = _(\\\"accounts\\\")\\n\\n    def __str__(self):\\n        return f\\\"{self.group}: {self.name}\\\"\\n\\n    def get_absolute_url(self):\\n        return reverse(\\n            \\\"account-summary\\\",\\n            kwargs={\\\"group\\\": self.group.slug, \\\"account\\\": self.slug},\\n        )\\n\\n    def save(self, *args, **kwargs):\\n        if not len(self.slug):\\n            raise ValueError(\\\"Slug cannot be empty.\\\")\\n        super().save(*args, **kwargs)\\n\\n    def total_used(self):\\n        with connection.cursor() as cursor:\\n            cursor.execute(ACCOUNT_TOTAL_USED, [self.id])\\n            total_usage = cursor.fetchone()[0]\\n            if total_usage is None:\\n                return 0\\n            else:\\n                return total_usage\\n\\n    def last_30_days_usage(self):\\n        from_date = datetime.datetime.now() - datetime.timedelta(days=30)\\n        usage = (\\n            TransactionEntry.objects.filter(\\n                account_id=self.id, transaction__date__gte=from_date\\n            )\\n            .aggregate(usage=models.Sum(\\\"debit\\\"))\\n            .get(\\\"usage\\\")\\n        )\\n        return usage if usage is not None else 0\\n\\n    def balance(self):\\n        if hasattr(self, \\\"confirmed_balance_sql\\\"):\\n            return self.confirmed_balance_sql or 0\\n        else:\\n            with connection.cursor() as cursor:\\n                cursor.execute(CONFIRMED_BALANCE_SQL, [self.id])\\n                return cursor.fetchone()[0]\\n\\n    def normal_balance(self):\\n        \\\"\\\"\\\"Returns account balance, but multiplies by -1 if the account is\\n        of type liability, equity or expense.\\\"\\\"\\\"\\n\\n        balance = self.balance()\\n        if balance is None:\\n            return 0\\n        elif balance == 0 or self.type in (\\\"As\\\", \\\"Ex\\\"):\\n            return balance\\n        else:\\n            return -1 * balance\\n\\n    def is_user_account(self):\\n        \\\"\\\"\\\"Returns true if a user account\\\"\\\"\\\"\\n        return not self.group_account\\n\\n    def is_blocked(self):\\n        \\\"\\\"\\\"Returns true if user account balance is below group block limit\\\"\\\"\\\"\\n\\n        if self.blocked:\\n            return True\\n\\n        if (\\n            not self.is_user_account()\\n            or self.ignore_block_limit\\n            or self.group.block_limit is None\\n        ):\\n            return False\\n        return self.normal_balance() < self.group.block_limit\\n\\n    def needs_warning(self):\\n        \\\"\\\"\\\"Returns true if user account balance is below group warn limit\\\"\\\"\\\"\\n\\n        if (\\n            not self.is_user_account()\\n            or self.ignore_block_limit\\n            or self.group.warn_limit is None\\n        ):\\n            return False\\n        return self.normal_balance() < self.group.warn_limit\\n\\n    # --- Transaction set methods\\n    # Please keep in sync with Group's set methods\\n\\n    def get_transaction_set_with_rejected(self):\\n        \\\"\\\"\\\"Returns all transactions connected to account, including rejected\\\"\\\"\\\"\\n        return (\\n            Transaction.objects.filter(entry_set__account=self)\\n            .exclude(state=Transaction.UNDEFINED_STATE)\\n            .distinct()\\n        )\\n\\n    transaction_set_with_rejected = property(\\n        get_transaction_set_with_rejected, None, None\\n    )\\n\\n    def get_transaction_set(self):\\n        \\\"\\\"\\\"Returns all transactions connected to account, excluding rejected\\\"\\\"\\\"\\n        return self.transaction_set_with_rejected.exclude(\\n            state=Transaction.REJECTED_STATE\\n        )\\n\\n    transaction_set = property(get_transaction_set, None, None)\\n\\n    def get_pending_transaction_set(self):\\n        \\\"\\\"\\\"Returns all pending transactions connected to account\\\"\\\"\\\"\\n        return self.transaction_set.filter(state=Transaction.PENDING_STATE)\\n\\n    pending_transaction_set = property(get_pending_transaction_set, None, None)\\n\\n    def get_committed_transaction_set(self):\\n        \\\"\\\"\\\"Returns all committed transactions connected to account\\\"\\\"\\\"\\n        return self.transaction_set.filter(state=Transaction.COMMITTED_STATE)\\n\\n    committed_transaction_set = property(\\n        get_committed_transaction_set, None, None\\n    )\\n\\n    def get_rejected_transaction_set(self):\\n        \\\"\\\"\\\"Returns all rejected transactions connected to account\\\"\\\"\\\"\\n        return self.transaction_set_with_rejected.filter(\\n            state=Transaction.REJECTED_STATE\\n        )\\n\\n    rejected_transaction_set = property(\\n        get_rejected_transaction_set, None, None\\n    )\\n\\n    def get_balance_history_set(self):\\n        \\\"\\\"\\\"Returns historical balance data for this user\\\"\\\"\\\"\\n        return list(Account.objects.raw(ACCOUNT_BALANCE_HISTORY_SQL % self.id))\\n\\n    balance_history_set = property(get_balance_history_set, None, None)\"}, {\"identifier\":\"RoleAccount\", \"path\":\"itkufs/accounting/models.py\", \"snippet\":\"class RoleAccount(models.Model):\\n    BANK_ACCOUNT = \\\"Bank\\\"\\n    CASH_ACCOUNT = \\\"Cash\\\"\\n    SALE_ACCOUNT = \\\"Sale\\\"\\n    ACCOUNT_ROLE = (\\n        (BANK_ACCOUNT, _(\\\"Bank account\\\")),\\n        (CASH_ACCOUNT, _(\\\"Cash account\\\")),\\n        (SALE_ACCOUNT, _(\\\"Sale account\\\")),\\n    )\\n\\n    group = models.ForeignKey(\\n        Group, on_delete=models.CASCADE, verbose_name=_(\\\"group\\\")\\n    )\\n    role = models.CharField(_(\\\"role\\\"), max_length=4, choices=ACCOUNT_ROLE)\\n    account = models.ForeignKey(\\n        Account, on_delete=models.CASCADE, verbose_name=_(\\\"account\\\")\\n    )\\n\\n    class Meta:\\n        ordering = (\\\"group\\\", \\\"role\\\")\\n        # FIXME: waiting for http://code.djangoproject.com/ticket/6523\\n        # unique_together = (('group', 'role'),)\\n        verbose_name = _(\\\"role account\\\")\\n        verbose_name_plural = _(\\\"role accounts\\\")\\n\\n    def __str__(self):\\n        return _(\\\"%(account)s is %(role)s for %(group)s\\\") % {\\n            \\\"account\\\": self.account.name,\\n            \\\"role\\\": self.get_role_display().lower(),\\n            \\\"group\\\": self.group,\\n        }\"}, {\"identifier\":\"InvalidTransaction\", \"path\":\"itkufs/accounting/models.py\", \"snippet\":\"class InvalidTransaction(Exception):\\n    def __init__(self, value):\\n        self.value = value\\n\\n    def __str__(self):\\n        return \\\"Invalid transaction: %s\\\" % self.value\"}, {\"identifier\":\"Transaction\", \"path\":\"itkufs/accounting/models.py\", \"snippet\":\"class Transaction(models.Model):\\n    UNDEFINED_STATE = \\\"\\\"\\n    PENDING_STATE = \\\"Pen\\\"\\n    COMMITTED_STATE = \\\"Com\\\"\\n    REJECTED_STATE = \\\"Rej\\\"\\n    TRANSACTION_STATE = (\\n        (PENDING_STATE, _(\\\"Pending\\\")),\\n        (COMMITTED_STATE, _(\\\"Committed\\\")),\\n        (REJECTED_STATE, _(\\\"Rejected\\\")),\\n    )\\n\\n    objects = TransactionManager()\\n\\n    group = models.ForeignKey(\\n        Group,\\n        on_delete=models.CASCADE,\\n        verbose_name=_(\\\"group\\\"),\\n        related_name=\\\"real_transaction_set\\\",\\n    )\\n    settlement = models.ForeignKey(\\n        Settlement,\\n        on_delete=models.CASCADE,\\n        verbose_name=_(\\\"settlement\\\"),\\n        null=True,\\n        blank=True,\\n    )\\n    date = models.DateField(\\n        _(\\\"date\\\"),\\n        help_text=_(\\\"May be used for date of the transaction if not today.\\\"),\\n    )\\n    last_modified = models.DateTimeField(_(\\\"Last modified\\\"), auto_now_add=True)\\n    state = models.CharField(\\n        _(\\\"state\\\"), max_length=3, choices=TRANSACTION_STATE, blank=True\\n    )\\n\\n    class Meta:\\n        ordering = (\\\"-last_modified\\\",)\\n        verbose_name = _(\\\"transaction\\\")\\n        verbose_name_plural = _(\\\"transactions\\\")\\n\\n    def __str__(self):\\n        if self.entry_set.all().count():\\n            entries = []\\n            for entry in self.entry_set.all():\\n                if entry.debit:\\n                    entries.append(f\\\"{entry.account} debit {entry.debit:.2f}\\\")\\n                else:\\n                    entries.append(f\\\"{entry.account} credit {entry.credit:.2f}\\\")\\n\\n            return \\\", \\\".join(entries)\\n        else:\\n            return \\\"Empty transaction\\\"\\n\\n    def get_absolute_url(self):\\n        return reverse(\\n            \\\"transaction-details\\\",\\n            kwargs={\\\"group\\\": self.group.slug, \\\"transaction\\\": self.id},\\n        )\\n\\n    @db_transaction.atomic\\n    def save(self, *args, **kwargs):\\n        debit_sum = 0\\n        credit_sum = 0\\n        debit_accounts = []\\n        credit_accounts = []\\n\\n        for entry in self.entry_set.all():\\n            if entry.debit > 0:\\n                debit_sum += entry.debit\\n                debit_accounts.append(entry.account)\\n            elif entry.credit > 0:\\n                credit_sum += entry.credit\\n                credit_accounts.append(entry.account)\\n\\n        for account in debit_accounts + credit_accounts:\\n            if account.group != self.group:\\n                raise InvalidTransaction(\\n                    \\\"Group of transaction entry account \\\"\\n                    \\\"does not match group of transaction.\\\"\\n                )\\n\\n        account_intersection = set(debit_accounts).intersection(\\n            set(credit_accounts)\\n        )\\n        if len(account_intersection):\\n            raise InvalidTransaction(\\n                \\\"The following accounts is both a debit \\\"\\n                \\\"and a credit account for this transaction: %s\\\"\\n                % list(account_intersection)\\n            )\\n\\n        if debit_sum != credit_sum:\\n            raise InvalidTransaction(\\n                \\\"Credit and debit do not match, \\\"\\n                \\\"credit: %d, debit: %d.\\\" % (credit_sum, debit_sum)\\n            )\\n\\n        if self.date is None:\\n            self.date = datetime.date.today()\\n\\n        self.last_modified = datetime.datetime.now()\\n        super().save(*args, **kwargs)\\n\\n    def set_pending(self, user, message=\\\"\\\"):\\n        if self.id is None:\\n            self.save()\\n\\n        if not self.is_committed() and not self.is_rejected():\\n            log = TransactionLog(type=self.PENDING_STATE, transaction=self)\\n            log.user = user\\n            if message is not None and message.strip() != \\\"\\\":\\n                log.message = message\\n            log.save()\\n            self.state = self.PENDING_STATE\\n            self.last_modified = datetime.datetime.now()\\n            self.save()\\n        else:\\n            raise InvalidTransaction(\\\"Could not set transaction as pending\\\")\\n\\n    def set_committed(self, user, message=\\\"\\\"):\\n        if self.is_pending() and not self.is_committed():\\n            log = TransactionLog(type=self.COMMITTED_STATE, transaction=self)\\n            log.user = user\\n            if message.strip() != \\\"\\\":\\n                log.message = message\\n            log.save()\\n\\n            for transaction_entry in TransactionEntry.objects.filter(\\n                transaction=self\\n            ):\\n                transaction_entry.check_if_blacklisted()\\n\\n            self.state = self.COMMITTED_STATE\\n            self.last_modified = datetime.datetime.now()\\n            self.save()\\n        else:\\n            raise InvalidTransaction(\\\"Could not set transaction as committed\\\")\\n\\n    def set_rejected(self, user, message=\\\"\\\"):\\n        if self.is_pending() and not self.is_committed():\\n            log = TransactionLog(type=self.REJECTED_STATE, transaction=self)\\n            log.user = user\\n            if message.strip() != \\\"\\\":\\n                log.message = message\\n            log.save()\\n\\n            self.state = self.REJECTED_STATE\\n            self.last_modified = datetime.datetime.now()\\n            self.save()\\n        else:\\n            raise InvalidTransaction(\\\"Could not set transaction as rejected\\\")\\n\\n    def is_pending(self):\\n        return self.state == self.PENDING_STATE\\n\\n    def has_pending(self):\\n        return self.state in (\\n            self.PENDING_STATE,\\n            self.COMMITTED_STATE,\\n            self.REJECTED_STATE,\\n        )\\n\\n    is_editable = is_pending\\n\\n    def is_committed(self):\\n        return self.state == self.COMMITTED_STATE\\n\\n    has_committed = is_committed\\n\\n    def is_rejected(self):\\n        return self.state == self.REJECTED_STATE\\n\\n    has_committed = is_rejected\\n\\n    def get_pending(self):\\n        if self.has_pending():\\n            return self.log_set.filter(type=self.PENDING_STATE).lastest(\\n                \\\"timestamp\\\"\\n            )\\n\\n    pending = property(get_pending, None, None)\\n\\n    def get_committed(self):\\n        if self.has_committed():\\n            return self.log_set.filter(type=self.COMMITTED_STATE).lastest(\\n                \\\"timestamp\\\"\\n            )\\n\\n    committed = property(get_committed, None, None)\\n\\n    def get_rejected(self):\\n        if self.has_rejected():\\n            return self.log_set.filter(type=self.REJECTED_STATE).lastest(\\n                \\\"timestamp\\\"\\n            )\\n\\n    rejected = property(get_rejected, None, None)\\n\\n    def get_valid_logtype_choices(self):\\n        if self.is_committed() or self.is_rejected():\\n            return [(\\\"\\\", \\\"\\\")]\\n        else:\\n            states = dict(self.TRANSACTION_STATE)\\n            del states[self.PENDING_STATE]\\n            states = list(states.items())\\n            states.insert(0, (\\\"\\\", \\\"\\\"))\\n            return states\\n\\n    def css_class(self):\\n        if self.is_rejected():\\n            return \\\"rejected\\\"\\n        elif self.is_pending():\\n            return \\\"pending\\\"\\n        else:\\n            return \\\"committed\\\"\"}, {\"identifier\":\"TransactionEntry\", \"path\":\"itkufs/accounting/models.py\", \"snippet\":\"class TransactionEntry(models.Model):\\n    transaction = models.ForeignKey(\\n        Transaction,\\n        on_delete=models.CASCADE,\\n        verbose_name=_(\\\"transaction\\\"),\\n        related_name=\\\"entry_set\\\",\\n    )\\n    account = models.ForeignKey(\\n        \\\"Account\\\", on_delete=models.CASCADE, verbose_name=_(\\\"account\\\")\\n    )\\n    debit = models.DecimalField(\\n        _(\\\"debit amount\\\"), max_digits=10, decimal_places=2, default=0\\n    )\\n    credit = models.DecimalField(\\n        _(\\\"credit amount\\\"), max_digits=10, decimal_places=2, default=0\\n    )\\n\\n    def check_if_blacklisted(self):\\n        old_balance = self.account.normal_balance()\\n        new_balance = old_balance - self.debit + self.credit\\n\\n        if (\\n            self.account.is_user_account\\n            and self.account.ignore_block_limit is False\\n            and self.account.group.block_limit is not None\\n            and old_balance > self.account.group.block_limit\\n            and new_balance < self.account.group.block_limit\\n        ):\\n\\n            subject = \\\"Svartelistet i µFS\\\"\\n            msg = (\\n                f\\\"Dette er en automatisk melding om at du har blitt \\\"\\n                f\\\"svartelistet i {self.account.group.name} sin µFS. \\\"\\n                f\\\"Din saldo er nå {new_balance}.\\\"\\n            )\\n            to_address = [\\\"%s@samfundet.no\\\" % self.account.owner]\\n            send_mail(\\n                subject,\\n                (msg),\\n                \\\"ufs@samfundet.no\\\",\\n                to_address,\\n                fail_silently=True,\\n            )\\n\\n    def save(self, *args, **kwargs):\\n        if self.transaction.is_rejected():\\n            raise InvalidTransactionEntry(\\n                \\\"Can not add entries to rejected transactions\\\"\\n            )\\n        if self.transaction.is_committed():\\n            raise InvalidTransactionEntry(\\n                \\\"Can not add entries to committed transactions\\\"\\n            )\\n\\n        if self.debit < 0 or self.credit < 0:\\n            raise InvalidTransactionEntry(\\n                \\\"Credit and debit must be positive or zero\\\"\\n            )\\n\\n        if self.debit == 0 and self.credit == 0:\\n            raise InvalidTransactionEntry(\\\"Create or debit must be positive\\\")\\n\\n        super().save(*args, **kwargs)\\n\\n    class Meta:\\n        unique_together = ((\\\"transaction\\\", \\\"account\\\"),)\\n        verbose_name = _(\\\"transaction entry\\\")\\n        verbose_name_plural = _(\\\"transaction entries\\\")\\n        ordering = (\\\"credit\\\", \\\"debit\\\")\\n\\n    def __str__(self):\\n        return _(\\\"%(account)s: debit %(debit)s, credit %(credit)s\\\") % {\\n            \\\"account\\\": self.account.name,\\n            \\\"debit\\\": self.debit,\\n            \\\"credit\\\": self.credit,\\n        }\"}, {\"identifier\":\"ChangeTransactionForm\", \"path\":\"itkufs/accounting/forms.py\", \"snippet\":\"class ChangeTransactionForm(forms.Form):\\n    change_to = forms.CharField(max_length=3, required=False)\\n\\n    def __init__(self, *args, **kwargs):\\n        choices = kwargs.pop(\\\"choices\\\", ((\\\"\\\", \\\"\\\"),))\\n        label = kwargs.pop(\\\"label\\\", True)\\n        super(forms.Form, self).__init__(*args, **kwargs)\\n\\n        if not label:\\n            self.fields[\\\"change_to\\\"].label = \\\"\\\"\\n        self.fields[\\\"change_to\\\"].widget = forms.Select(choices=choices)\"}, {\"identifier\":\"DepositWithdrawForm\", \"path\":\"itkufs/accounting/forms.py\", \"snippet\":\"class DepositWithdrawForm(forms.Form):\\n    amount = forms.DecimalField(\\n        label=_(\\\"Amount\\\"),\\n        required=True,\\n        min_value=0,\\n        decimal_places=2,\\n        max_digits=10,\\n    )\\n    details = forms.CharField(\\n        label=_(\\\"Details\\\"),\\n        required=False,\\n        widget=forms.widgets.Textarea(attrs={\\\"rows\\\": 2}),\\n    )\\n\\n    def clean_amount(self):\\n        amount = self.cleaned_data[\\\"amount\\\"]\\n\\n        if amount <= 0:\\n            raise forms.ValidationError(_(\\\"Amount must be greater than zero\\\"))\\n\\n        return amount\"}, {\"identifier\":\"EntryForm\", \"path\":\"itkufs/accounting/forms.py\", \"snippet\":\"class EntryForm(Form):\\n    # FIXME add clean_debit/credit so that we can ignore whitespaces :)\\n    debit = forms.DecimalField(\\n        min_value=0,\\n        required=False,\\n        widget=forms.TextInput(attrs={\\\"size\\\": 4, \\\"class\\\": \\\"number\\\"}),\\n    )\\n    credit = forms.DecimalField(\\n        min_value=0,\\n        required=False,\\n        widget=forms.TextInput(attrs={\\\"size\\\": 4, \\\"class\\\": \\\"number\\\"}),\\n    )\"}, {\"identifier\":\"RejectTransactionForm\", \"path\":\"itkufs/accounting/forms.py\", \"snippet\":\"class RejectTransactionForm(forms.Form):\\n    reason = forms.CharField(\\n        label=_(\\\"Reason\\\"),\\n        widget=forms.widgets.Textarea(attrs={\\\"rows\\\": 2}),\\n        required=True,\\n    )\"}, {\"identifier\":\"SettlementForm\", \"path\":\"itkufs/accounting/forms.py\", \"snippet\":\"class SettlementForm(ModelForm):\\n    class Meta:\\n        model = Settlement\\n        exclude = [\\\"group\\\"]\\n\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n        if \\\"instance\\\" not in kwargs:\\n            del self.fields[\\\"closed\\\"]\"}, {\"identifier\":\"TransactionSettlementForm\", \"path\":\"itkufs/accounting/forms.py\", \"snippet\":\"class TransactionSettlementForm(ModelForm):\\n    details = forms.CharField(\\n        label=_(\\\"Details\\\"),\\n        required=False,\\n        widget=forms.widgets.Textarea(attrs={\\\"rows\\\": 2}),\\n    )\\n\\n    def __init__(self, *args, **kwargs):\\n        super().__init__(*args, **kwargs)\\n\\n        if \\\"instance\\\" in kwargs:\\n            self.fields[\\\"settlement\\\"].choices = [(\\\"\\\", \\\"\\\")] + [\\n                (s.id, s)\\n                for s in kwargs[\\\"instance\\\"].group.settlement_set.filter(\\n                    closed=False\\n                )\\n            ]\\n\\n    class Meta:\\n        model = Transaction\\n        fields = (\\\"settlement\\\",)\"}, {\"identifier\":\"TransferForm\", \"path\":\"itkufs/accounting/forms.py\", \"snippet\":\"class TransferForm(DepositWithdrawForm):\\n    credit_account = forms.ChoiceField(label=_(\\\"To\\\"), required=True)\\n\\n    def __init__(self, *args, **kwargs):\\n        account = kwargs.pop(\\\"account\\\")\\n        super(DepositWithdrawForm, self).__init__(*args, **kwargs)\\n        if account:\\n            choices = []\\n            for a in account.group.user_account_set:\\n                if a != account:\\n                    choices.append((a.id, a.name))\\n            self.fields[\\\"credit_account\\\"].choices = choices\"}]", "import_statement": "from django.contrib import messages\nfrom django.contrib.auth.decorators import login_required\nfrom django.db import transaction as db_transaction\nfrom django.http import HttpResponseForbidden, HttpResponseRedirect, Http404\nfrom django.shortcuts import render\nfrom django.urls import reverse\nfrom django.utils.translation import ugettext as _\nfrom itkufs.common.utils import callsign_sorted as ufs_sorted\nfrom itkufs.common.decorators import limit_to_owner, limit_to_admin\nfrom itkufs.accounting.models import (\n    Account,\n    RoleAccount,\n    InvalidTransaction,\n    Transaction,\n    TransactionEntry,\n)\nfrom itkufs.accounting.forms import (\n    ChangeTransactionForm,\n    DepositWithdrawForm,\n    EntryForm,\n    RejectTransactionForm,\n    SettlementForm,\n    TransactionSettlementForm,\n    TransferForm,\n)", "code": "        {\n            \"is_admin\": is_admin,\n            \"group\": group,\n            \"approve\": True,\n            \"transaction_list\": transactions,\n        },\n    )\n\n\n@login_required\n@limit_to_owner\ndef reject_transactions(request, group, transaction=None, is_admin=False):\n    \"\"\"Reject transactions from members and other groups\"\"\"\n\n    if request.method == \"POST\":\n        data = request.POST\n        to_be_rejected = request.POST.getlist(\"transactions\")\n        to_be_rejected = group.pending_transaction_set.filter(\n            id__in=to_be_rejected\n        )\n    elif transaction is not None:\n        data = None\n        try:\n            # FIXME add count == 0 check for log_set__account__owner !=\n            # request.user\n            # FIXME add can_reject to transaction\n            to_be_rejected = [\n                group.pending_transaction_set.get(\n                    id=transaction.id, entry_set__account__owner=request.user\n                )\n            ]\n        except Transaction.DoesNotExist:\n            raise Http404\n    else:\n        return HttpResponseRedirect(\n            reverse(\"group-summary\", args=(group.slug,))\n        )\n\n    form = RejectTransactionForm(data)\n\n    if not form.is_valid():\n        return render(\n            request,\n            \"accounting/reject_transactions.html\",\n            {\n                \"is_admin\": is_admin,\n                \"group\": group,\n                \"transactions\": to_be_rejected,\n                \"form\": form,\n            },\n        )\n\n    for transaction in to_be_rejected:\n        transaction.set_rejected(\n            user=request.user, message=request.POST[\"reason\"]\n        )\n\n    if transaction is not None:\n        try:\n            return HttpResponseRedirect(\n                reverse(\n                    \"account-summary\",\n                    kwargs={\n                        \"group\": group.slug,\n                        \"account\": group.account_set.get(\n                            owner=request.user\n                        ).slug,\n                    },\n                )\n            )\n        except Account.DoesNotExist:\n            return HttpResponseRedirect(\n                reverse(\"group-summary\", kwargs={\"group\": group.slug})\n            )\n\n    if group.pending_transaction_set.count():\n        return HttpResponseRedirect(\n            reverse(\"approve-transactions\", kwargs={\"group\": group.slug})\n        )\n\n    return HttpResponseRedirect(\n        reverse(\"group-summary\", kwargs={\"group\": group.slug})\n    )\n\n\n@login_required\n@limit_to_admin\n@db_transaction.atomic\ndef new_edit_transaction(request, group, transaction=None, is_admin=False):\n    \"\"\"Admin view for creating transactions\"\"\"\n\n    savepoint_id = db_transaction.savepoint()\n    if transaction is None:\n        transaction = Transaction(group=group)\n    elif not transaction.is_editable():\n        messages.error(\n            request, _(\"Transaction %d can't be changed.\" % transaction.id)\n        )\n\n        db_transaction.savepoint_rollback(savepoint_id)\n\n        url = reverse(\"group-summary\", args=(group.slug,))\n        return HttpResponseRedirect(url)\n\n    if request.method == \"POST\":\n        data = request.POST\n    elif transaction.id:\n        data = {}\n        # Load \"fake\" post data if we are editing a transaction\n        for e in transaction.entry_set.all():\n            if e.debit > 0:\n                data[\"%d-debit\" % e.account.id] = e.debit\n            if e.credit > 0:\n                data[\"%d-credit\" % e.account.id] = e.credit\n\n        data[\"settlement-settlement\"] = transaction.settlement_id\n    else:\n        data = None\n\n    # Init forms\n", "next_line": "    settlement_form = TransactionSettlementForm(", "gold_snippet_index": 13, "id": 2, "__internal_uuid__": "de85b030-6f0d-4364-b3c5-5dd941dc0cbe"}
{"repo_name": "doubleDragon/QuantBot", "file_path": "quant/markets/market_factory.py", "context": "[{\"identifier\":\"constant\", \"path\":\"quant/common/constant.py\", \"snippet\":\"ORDER_STATE_PENDING = 1\\nORDER_STATE_CLOSED = 2\\nORDER_STATE_CANCELED = 4\\nORDER_STATE_UNKNOWN = 8\\nEX_OKEX = 'Okex'\\nEX_BFX = 'Bitfinex'\\nEX_LQ = 'Liqui'\\nEX_GDAX = 'Gdax'\\nEX_BINANCE = 'Binance'\\nEX_CEX = 'Cex'\\nEX_EXMO = 'Exmo'\\nEX_KKEX = 'Kkex'\\nEX_HITBITC = 'Hitbtc'\\nEX_BITTREX = 'Bittrex'\\nEX_GATE = 'Gate'\\nEX_BITFLYER = 'Bitflyer'\\nEX_KRAKEN = 'Kraken'\\nEX_COINEGG = 'Coinegg'\\nEX_BITHUMB = 'Bithumb'\\nEX_HUOBI = 'Huobi'\\nEX_SET = (\\n    EX_OKEX,\\n    EX_BFX,\\n    EX_LQ,\\n    EX_GDAX,\\n    EX_BINANCE,\\n    EX_CEX,\\n    EX_EXMO,\\n    EX_BITTREX,\\n    EX_BITFLYER,\\n    EX_KRAKEN,\\n    EX_COINEGG,\\n    EX_BITHUMB\\n)\\nCODE_LQ_SELL_NOT_ENOUGH = 832\\nCODE_LQ_BUY_NOT_ENOUGH = 831\"}, {\"identifier\":\"Bitfinex\", \"path\":\"quant/markets/_bitfinex.py\", \"snippet\":\"class Bitfinex(Market):\\n    def __init__(self, pair_code):\\n        base_currency, market_currency = self.get_available_pairs(pair_code)\\n        super(Bitfinex, self).__init__(base_currency, market_currency, pair_code, 0.002)\\n        self.client = Client()\\n\\n    def update_depth(self):\\n        try:\\n            depth_raw = self.client.depth(self.pair_code)\\n            if depth_raw:\\n                self.depth = self.format_depth(depth_raw)\\n            else:\\n                raise ValueError('response is None')\\n        except Exception as e:\\n            raise e\\n\\n    @classmethod\\n    def format_depth(cls, depth):\\n        bids = market_util.sort_and_format_dict(depth['bids'], True)\\n        asks = market_util.sort_and_format_dict(depth['asks'], False)\\n        return {'asks': asks, 'bids': bids}\\n\\n    @classmethod\\n    def get_available_pairs(cls, pair_code):\\n        if pair_code == 'ethusd':\\n            base_currency = 'USD'\\n            market_currency = 'ETH'\\n        elif pair_code == 'ethbtc':\\n            base_currency = 'BTC'\\n            market_currency = 'ETH'\\n        elif pair_code == 'etcbtc':\\n            base_currency = 'BTC'\\n            market_currency = 'ETC'\\n        elif pair_code == 'etcusd':\\n            base_currency = 'USD'\\n            market_currency = 'ETC'\\n        elif pair_code == 'bchbtc':\\n            base_currency = 'BTC'\\n            market_currency = 'BCH'\\n        elif pair_code == 'btcusd':\\n            base_currency = 'USD'\\n            market_currency = 'BTC'\\n        elif pair_code == 'bt1usd':\\n            base_currency = 'USD'\\n            market_currency = 'BT1'\\n        elif pair_code == 'bt2usd':\\n            base_currency = 'USD'\\n            market_currency = 'BT2'\\n        elif pair_code == 'bt1btc':\\n            base_currency = 'BTC'\\n            market_currency = 'BT1'\\n        elif pair_code == 'bt2btc':\\n            base_currency = 'BTC'\\n            market_currency = 'BT2'\\n        elif pair_code == 'bchusd':\\n            base_currency = 'USD'\\n            market_currency = 'BCH'\\n        elif pair_code == 'eosusd':\\n            base_currency = 'USD'\\n            market_currency = 'EOS'\\n        elif pair_code == 'eoseth':\\n            base_currency = 'ETH'\\n            market_currency = 'EOS'\\n        elif pair_code == 'zecusd':\\n            base_currency = 'USD'\\n            market_currency = 'ZEC'\\n        elif pair_code == 'neousd':\\n            base_currency = 'USD'\\n            market_currency = 'NEO'\\n        elif pair_code == 'neobtc':\\n            base_currency = 'BTC'\\n            market_currency = 'NEO'\\n        elif pair_code == 'neoeth':\\n            base_currency = 'ETH'\\n            market_currency = 'NEO'\\n        elif pair_code == 'iotusd':\\n            base_currency = 'USD'\\n            market_currency = 'IOT'\\n        elif pair_code == 'iotbtc':\\n            base_currency = 'BTC'\\n            market_currency = 'IOT'\\n        elif pair_code == 'zrxeth':\\n            base_currency = 'ETH'\\n            market_currency = 'ZRX'\\n        elif pair_code == 'zrxbtc':\\n            base_currency = 'BTC'\\n            market_currency = 'ZRX'\\n        else:\\n            assert False\\n        return base_currency, market_currency\"}, {\"identifier\":\"Kkex\", \"path\":\"quant/markets/_kkex.py\", \"snippet\":\"class Kkex(Market):\\n    def __init__(self, pair_code):\\n        base_currency, market_currency = self.get_available_pairs(pair_code)\\n        super(Kkex, self).__init__(base_currency, market_currency, pair_code, 0.0025)\\n        self.client = Client()\\n\\n    @classmethod\\n    def get_available_pairs(cls, pair_code):\\n        if pair_code == 'BCHBTC':\\n            base_currency = 'BTC'\\n            market_currency = 'BCH'\\n        else:\\n            assert False\\n        return base_currency, market_currency\\n\\n    def update_depth(self):\\n        depth_raw = self.client.depth(self.pair_code)\\n\\n        if depth_raw:\\n            self.depth = self.format_depth(depth_raw)\\n\\n    @classmethod\\n    def format_depth(cls, depth):\\n        bids = market_util.sort_and_format_list(depth['bids'], True)\\n        asks = market_util.sort_and_format_list(depth['asks'], False)\\n        return {'asks': asks, 'bids': bids}\"}, {\"identifier\":\"Liqui\", \"path\":\"quant/markets/_liqui.py\", \"snippet\":\"class Liqui(Market):\\n    def __init__(self, pair_code):\\n        base_currency, market_currency = self.get_available_pairs(pair_code)\\n        super(Liqui, self).__init__(base_currency, market_currency, pair_code, 0.002)\\n        self.client = PublicClient()\\n\\n    @classmethod\\n    def get_available_pairs(cls, pair_code):\\n        if pair_code == 'bcc_btc':\\n            base_currency = 'BTC'\\n            market_currency = 'BCC'\\n        elif pair_code == 'bcc_eth':\\n            base_currency = 'ETH'\\n            market_currency = 'BCC'\\n        elif pair_code == 'eos_btc':\\n            base_currency = 'BTC'\\n            market_currency = 'EOS'\\n        else:\\n            assert False\\n        return base_currency, market_currency\\n\\n    def update_depth(self):\\n        depth_raw = self.client.depth(self.pair_code)\\n        if depth_raw and self.pair_code in depth_raw:\\n            self.depth = self.format_depth(depth_raw[self.pair_code])\\n\\n    @classmethod\\n    def format_depth(cls, depth):\\n        bids = market_util.sort_and_format_list(depth['bids'], True)\\n        asks = market_util.sort_and_format_list(depth['asks'], False)\\n        return {'asks': asks, 'bids': bids}\"}, {\"identifier\":\"Hitbtc\", \"path\":\"quant/markets/_hitbtc.py\", \"snippet\":\"class Hitbtc(Market):\\n    def __init__(self, pair_code):\\n        base_currency, market_currency = self.get_available_pairs(pair_code)\\n        super(Hitbtc, self).__init__(base_currency, market_currency, pair_code, 0.002)\\n        self.client = PublicClient()\\n\\n    @classmethod\\n    def get_available_pairs(cls, pair_code):\\n        if pair_code == 'bccbtc':\\n            base_currency = 'BTC'\\n            market_currency = 'BCC'\\n        else:\\n            assert False\\n        return base_currency, market_currency\\n\\n    def symbol(self):\\n        return \\\"%s%s\\\" % (self.market_currency.upper(), self.base_currency.upper())\\n\\n    def update_depth(self):\\n        depth_raw = self.client.depth(self.symbol())\\n        if depth_raw:\\n            self.depth = self.format_depth(depth_raw)\\n\\n    @classmethod\\n    def format_depth(cls, depth):\\n        bids = market_util.sort_and_format_list(depth['bids'], True)\\n        asks = market_util.sort_and_format_list(depth['asks'], False)\\n        return {'asks': asks, 'bids': bids}\"}, {\"identifier\":\"Cex\", \"path\":\"quant/markets/_cex.py\", \"snippet\":\"class Cex(Market):\\n    def __init__(self, pair_code):\\n        base_currency, market_currency = self.get_available_pairs(pair_code)\\n        super(Cex, self).__init__(base_currency, market_currency, pair_code, 0.002)\\n        self.client = PublicClient()\\n\\n    @classmethod\\n    def get_available_pairs(cls, pair_code):\\n        if pair_code == 'bccbtc':\\n            base_currency = 'BTC'\\n            market_currency = 'BCH'\\n        else:\\n            assert False\\n        return base_currency, market_currency\\n\\n    def symbol(self):\\n        return \\\"%s/%s\\\" % (self.market_currency.upper(), self.base_currency.upper())\\n\\n    def update_depth(self):\\n        depth_raw = self.client.depth(self.symbol())\\n        if depth_raw:\\n            self.depth = self.format_depth(depth_raw)\\n\\n    @classmethod\\n    def format_depth(cls, depth):\\n        bids = market_util.sort_and_format_list(depth['bids'], True)\\n        asks = market_util.sort_and_format_list(depth['asks'], False)\\n        return {'asks': asks, 'bids': bids}\"}, {\"identifier\":\"Bittrex\", \"path\":\"quant/markets/_bittrex.py\", \"snippet\":\"class Bittrex(Market):\\n    def __init__(self, pair_code):\\n        base_currency, market_currency = self.get_available_pairs(pair_code)\\n\\n        super(Bittrex, self).__init__(base_currency, market_currency, pair_code, 0.0025)\\n\\n        self.client = bittrex.Bittrex(None, None)\\n\\n    def update_depth(self):\\n        raw_depth = self.client.get_orderbook(self.pair_code, 'both')\\n        self.depth = self.format_depth(raw_depth)\\n\\n    # override method\\n    def sort_and_format(self, l, reverse=False):\\n        l.sort(key=lambda x: float(x['Rate']), reverse=reverse)\\n        r = []\\n        for i in l:\\n            r.append({'price': float(i['Rate']), 'amount': float(i['Quantity'])})\\n        return r\\n\\n    # override method\\n    def format_depth(self, depth):\\n        bids = self.sort_and_format(depth['result']['buy'], True)\\n        asks = self.sort_and_format(depth['result']['sell'], False)\\n        return {'asks': asks, 'bids': bids}\\n\\n    @classmethod\\n    def get_available_pairs(cls, pair_code):\\n        if pair_code == 'BTC-BCC':\\n            base_currency = 'BTC'\\n            market_currency = 'BCH'\\n        elif pair_code == 'BTC-ZEC':\\n            base_currency = 'BTC'\\n            market_currency = 'ZEC'\\n        else:\\n            assert False\\n        return base_currency, market_currency\"}, {\"identifier\":\"Binance\", \"path\":\"quant/markets/_binance.py\", \"snippet\":\"class Binance(Market):\\n    def __init__(self, pair_code):\\n        base_currency, market_currency = self.get_available_pairs(pair_code)\\n        super(Binance, self).__init__(base_currency, market_currency, pair_code, 0.001)\\n\\n        self.client = Client(None, None)\\n\\n    def update_depth(self):\\n        raw_depth = self.client.get_order_book(symbol=self.pair_code, limit=5)\\n        if raw_depth:\\n            self.depth = self.format_depth(raw_depth)\\n\\n    @classmethod\\n    def get_available_pairs(cls, pair_code):\\n        if pair_code == 'BCCBTC':\\n            base_currency = 'BTC'\\n            market_currency = 'BCC'\\n        elif pair_code == 'BTCUSDT':\\n            base_currency = 'USDT'\\n            market_currency = 'BTC'\\n        elif pair_code == 'ETHBTC':\\n            base_currency = 'BTC'\\n            market_currency = 'ETH'\\n        elif pair_code == 'ETHUSDT':\\n            base_currency = 'USDT'\\n            market_currency = 'ETH'\\n        elif pair_code == 'BNBBTC':\\n            base_currency = 'BTC'\\n            market_currency = 'BNB'\\n        elif pair_code == 'BNBETH':\\n            base_currency = 'ETH'\\n            market_currency = 'BNB'\\n        elif pair_code == 'MCOBTC':\\n            base_currency = 'BTC'\\n            market_currency = 'MCO'\\n        elif pair_code == 'MCOETH':\\n            base_currency = 'ETH'\\n            market_currency = 'MCO'\\n        elif pair_code == 'QTUMBTC':\\n            base_currency = 'BTC'\\n            market_currency = 'QTUMBCH'\\n        elif pair_code == 'QTUMETH':\\n            base_currency = 'ETH'\\n            market_currency = 'QTUM'\\n        elif pair_code == 'WTCBTC':\\n            base_currency = 'BTC'\\n            market_currency = 'WTC'\\n        elif pair_code == 'WTCETH':\\n            base_currency = 'ETH'\\n            market_currency = 'WTC'\\n        elif pair_code == 'NEOBTC':\\n            base_currency = 'BTC'\\n            market_currency = 'NEO'\\n        elif pair_code == 'NEOETH':\\n            base_currency = 'ETH'\\n            market_currency = 'NEO'\\n        elif pair_code == 'IOTAETH':\\n            base_currency = 'ETH'\\n            market_currency = 'IOTA'\\n        elif pair_code == 'IOTABTC':\\n            base_currency = 'BTC'\\n            market_currency = 'IOTA'\\n        elif pair_code == 'ZRXBTC':\\n            base_currency = 'BTC'\\n            market_currency = 'ZRX'\\n        elif pair_code == 'ZRXETH':\\n            base_currency = 'ETH'\\n            market_currency = 'ZRX'\\n        else:\\n            assert False\\n        return base_currency, market_currency\\n\\n    @classmethod\\n    def format_depth(cls, depth):\\n        bids = market_util.sort_and_format_list(depth['bids'], True)\\n        asks = market_util.sort_and_format_list(depth['asks'], False)\\n        return {'asks': asks, 'bids': bids}\"}, {\"identifier\":\"Gate\", \"path\":\"quant/markets/_gate.py\", \"snippet\":\"class Gate(Market):\\n    def __init__(self, pair_code):\\n        base_currency, market_currency = self.get_available_pairs(pair_code)\\n        super(Gate, self).__init__(base_currency, market_currency, pair_code, 0.002)\\n        self.client = PublicClient()\\n\\n    @classmethod\\n    def get_available_pairs(cls, pair_code):\\n        if pair_code == 'eth_btc':\\n            base_currency = 'BTC'\\n            market_currency = 'ETH'\\n        elif pair_code == 'bcc_btc':\\n            base_currency = 'BTC'\\n            market_currency = 'BCC'\\n        else:\\n            assert False\\n        return base_currency, market_currency\\n\\n    def update_depth(self):\\n        depth_raw = self.client.depth(self.pair_code)\\n        if depth_raw:\\n            self.depth = self.format_depth(depth_raw)\\n\\n    @classmethod\\n    def format_depth(cls, depth):\\n        bids = market_util.sort_and_format_list(depth['bids'], True)\\n        asks = market_util.sort_and_format_list(depth['asks'], False)\\n        return {'asks': asks, 'bids': bids}\"}, {\"identifier\":\"Bitflyer\", \"path\":\"quant/markets/_bitflyer.py\", \"snippet\":\"class Bitflyer(Market):\\n    def __init__(self, pair_code):\\n        base_currency, market_currency = self.get_available_pairs(pair_code)\\n\\n        super(Bitflyer, self).__init__(base_currency, market_currency, pair_code, 0.0025)\\n\\n        self.client = bitflyer.PublicClient()\\n\\n    def update_depth(self):\\n        raw_depth = self.client.depth(self.pair_code)\\n        if raw_depth:\\n            self.depth = self.format_depth(raw_depth)\\n\\n    @classmethod\\n    def sort_and_format(cls, l, reverse=False):\\n        l.sort(key=lambda x: float(x['price']), reverse=reverse)\\n        r = []\\n        for i in l:\\n            r.append({'price': float(i['price']), 'amount': float(i['size'])})\\n        return r\\n\\n    def format_depth(self, depth):\\n        bids = self.sort_and_format(depth['bids'], True)\\n        asks = self.sort_and_format(depth['asks'], False)\\n        return {'asks': asks, 'bids': bids}\\n\\n    @classmethod\\n    def get_available_pairs(cls, pair_code):\\n        if pair_code == 'btc_jpy':\\n            base_currency = 'JPY'\\n            market_currency = 'BTC'\\n        elif pair_code == 'eth_btc':\\n            base_currency = 'BTC'\\n            market_currency = 'ETH'\\n        elif pair_code == 'bch_btc':\\n            base_currency = 'BTC'\\n            market_currency = 'BCH'\\n        else:\\n            assert False\\n        return base_currency, market_currency\"}, {\"identifier\":\"Kraken\", \"path\":\"quant/markets/_kraken.py\", \"snippet\":\"class Kraken(Market):\\n    \\\"\\\"\\\"XBT就是BTC\\\"\\\"\\\"\\n    def __init__(self, pair_code):\\n        base_currency, market_currency = self.get_available_pairs(pair_code)\\n        super(Kraken, self).__init__(base_currency, market_currency, pair_code, 0.002)\\n        self.client = PublicClient()\\n\\n    @classmethod\\n    def get_available_pairs(cls, pair_code):\\n        if pair_code == 'xbteur':\\n            base_currency = 'EUR'\\n            market_currency = 'XBT'\\n        elif pair_code == 'xbtusd':\\n            base_currency = 'USD'\\n            market_currency = 'XBT'\\n        elif pair_code == 'etheur':\\n            base_currency = 'EUR'\\n            market_currency = 'ETH'\\n        elif pair_code == 'ethusd':\\n            base_currency = 'USD'\\n            market_currency = 'ETH'\\n        elif pair_code == 'bcheur':\\n            base_currency = 'EUR'\\n            market_currency = 'BCH'\\n        elif pair_code == 'bchusd':\\n            base_currency = 'USD'\\n            market_currency = 'BCH'\\n        elif pair_code == 'eoseur':\\n            base_currency = 'EUR'\\n            market_currency = 'EOS'\\n        elif pair_code == 'eosusd':\\n            base_currency = 'USD'\\n            market_currency = 'EOS'\\n        else:\\n            assert False\\n        return base_currency, market_currency\\n\\n    def update_depth(self):\\n        depth_raw = self.client.depth(self.pair_code)\\n        if depth_raw and 'result' in depth_raw:\\n            depth_raw = depth_raw['result']\\n            if len(depth_raw) > 0:\\n                depth_raw = depth_raw.values()[0]\\n                self.depth = self.format_depth(depth_raw)\\n\\n    @classmethod\\n    def format_depth(cls, depth):\\n        bids = market_util.sort_and_format_list(depth['bids'], True)\\n        asks = market_util.sort_and_format_list(depth['asks'], False)\\n        return {'asks': asks, 'bids': bids}\"}, {\"identifier\":\"Coinegg\", \"path\":\"quant/markets/_coinegg.py\", \"snippet\":\"class Coinegg(Market):\\n    \\\"\\\"\\\"XBT就是BTC\\\"\\\"\\\"\\n    def __init__(self, pair_code):\\n        base_currency, market_currency = self.get_available_pairs(pair_code)\\n        super(Coinegg, self).__init__(base_currency, market_currency, pair_code, 0.001)\\n        self.client = PublicClient()\\n\\n    @classmethod\\n    def get_available_pairs(cls, pair_code):\\n        if pair_code == 'bcc':\\n            base_currency = 'BTC'\\n            market_currency = 'BCC'\\n        elif pair_code == 'eth':\\n            base_currency = 'BTC'\\n            market_currency = 'ETH'\\n        elif pair_code == 'neo':\\n            base_currency = 'BTC'\\n            market_currency = 'NEO'\\n        elif pair_code == 'etc':\\n            base_currency = 'BTC'\\n            market_currency = 'ETC'\\n        else:\\n            assert False\\n        return base_currency, market_currency\\n\\n    def update_depth(self):\\n        depth_raw = self.client.depth(self.pair_code)\\n        if depth_raw:\\n            self.depth = self.format_depth(depth_raw)\\n\\n    @classmethod\\n    def format_depth(cls, depth):\\n        bids = market_util.sort_and_format_list(depth['bids'], True)\\n        asks = market_util.sort_and_format_list(depth['asks'], False)\\n        return {'asks': asks, 'bids': bids}\"}, {\"identifier\":\"Bithumb\", \"path\":\"quant/markets/_bithumb.py\", \"snippet\":\"class Bithumb(Market):\\n    def __init__(self, pair_code):\\n        base_currency, market_currency = self.get_available_pairs(pair_code)\\n\\n        super(Bithumb, self).__init__(base_currency, market_currency, pair_code, 0.0025)\\n\\n        self.client = bithumb.PublicClient()\\n\\n    def update_depth(self):\\n        raw_depth = self.client.depth(self.pair_code)\\n        if raw_depth and 'data' in raw_depth:\\n            raw_depth = raw_depth['data']\\n            if raw_depth:\\n                self.depth = self.format_depth(raw_depth)\\n\\n    @classmethod\\n    def sort_and_format(cls, l, reverse=False):\\n        l.sort(key=lambda x: float(x['price']), reverse=reverse)\\n        r = []\\n        for i in l:\\n            r.append({'price': float(i['price']), 'amount': float(i['quantity'])})\\n        return r\\n\\n    def format_depth(self, depth):\\n        bids = self.sort_and_format(depth['bids'], True)\\n        asks = self.sort_and_format(depth['asks'], False)\\n        return {'asks': asks, 'bids': bids}\\n\\n    @classmethod\\n    def get_available_pairs(cls, pair_code):\\n        if pair_code == 'btc':\\n            base_currency = 'KRW'\\n            market_currency = 'BTC'\\n        elif pair_code == 'eth':\\n            base_currency = 'KRW'\\n            market_currency = 'ETH'\\n        elif pair_code == 'bch':\\n            base_currency = 'KRW'\\n            market_currency = 'BCH'\\n        else:\\n            assert False\\n        return base_currency, market_currency\"}, {\"identifier\":\"Huobi\", \"path\":\"quant/markets/_huobi.py\", \"snippet\":\"class Huobi(Market):\\n    def __init__(self, pair_code):\\n        base_currency, market_currency = self.get_available_pairs(pair_code)\\n        super(Huobi, self).__init__(base_currency, market_currency, pair_code, 0.002)\\n        self.client = Client()\\n\\n    def update_depth(self):\\n        try:\\n            depth_raw = self.client.depth(self.pair_code)\\n            if depth_raw:\\n                if 'status' not in depth_raw and depth_raw['status'] != 'ok':\\n                    raise Exception('status not exist in raw response or is not ok')\\n                if 'tick' not in depth_raw:\\n                    raise Exception('tick not exist in raw response')\\n                self.depth = self.format_depth(depth_raw['tick'])\\n            else:\\n                raise Exception('response is None')\\n        except Exception as e:\\n            raise e\\n\\n    @classmethod\\n    def format_depth(cls, depth):\\n        bids = market_util.sort_and_format_list(depth['bids'], True)\\n        asks = market_util.sort_and_format_list(depth['asks'], False)\\n        return {'asks': asks, 'bids': bids}\\n\\n    @classmethod\\n    def get_available_pairs(cls, pair_code):\\n        if pair_code == 'ethusdt':\\n            base_currency = 'USDT'\\n            market_currency = 'ETH'\\n        elif pair_code == 'ethbtc':\\n            base_currency = 'BTC'\\n            market_currency = 'ETH'\\n        elif pair_code == 'etcbtc':\\n            base_currency = 'BTC'\\n            market_currency = 'ETC'\\n        elif pair_code == 'etcusdt':\\n            base_currency = 'USDT'\\n            market_currency = 'ETC'\\n        elif pair_code == 'bchbtc':\\n            base_currency = 'BTC'\\n            market_currency = 'BCH'\\n        elif pair_code == 'btcusdt':\\n            base_currency = 'USDT'\\n            market_currency = 'BTC'\\n        else:\\n            assert False\\n        return base_currency, market_currency\"}]", "import_statement": "import logging\nfrom quant.common import constant\nfrom ._bitfinex import Bitfinex\nfrom ._kkex import Kkex\nfrom ._liqui import Liqui\nfrom ._hitbtc import Hitbtc\nfrom ._cex import Cex\nfrom ._bittrex import Bittrex\nfrom ._binance import Binance\nfrom ._gate import Gate\nfrom ._bitflyer import Bitflyer\nfrom ._kraken import Kraken\nfrom ._coinegg import Coinegg\nfrom ._bithumb import Bithumb\nfrom ._huobi import Huobi", "code": "#!/usr/bin/env python\n# -*- coding: UTF-8 -*-\n\n\n\n\ndef create_markets(exchange_names):\n    \"\"\"\n    [\n        'Bitfinex_BCH_BTC'\n        ...\n    ]\n    \"\"\"\n    markets = {}\n    for name in exchange_names:\n        if name == \"%s_ETH_USD\" % constant.EX_BFX:\n            ex = Bitfinex('ethusd')\n        elif name == \"%s_ETC_BTC\" % constant.EX_BFX:\n            ex = Bitfinex('etcbtc')\n        elif name == \"%s_ETC_USD\" % constant.EX_BFX:\n            ex = Bitfinex('etcusd')\n        elif name == \"%s_ETH_BTC\" % constant.EX_BFX:\n            ex = Bitfinex('ethbtc')\n        elif name == \"%s_BCH_USD\" % constant.EX_BFX:\n            ex = Bitfinex('bchusd')\n        elif name == \"%s_ZEC_USD\" % constant.EX_BFX:\n            ex = Bitfinex('zecusd')\n        elif name == \"%s_BCH_BTC\" % constant.EX_BFX:\n            ex = Bitfinex('bchbtc')\n        elif name == \"%s_BTC_USD\" % constant.EX_BFX:\n            ex = Bitfinex('btcusd')\n        elif name == \"%s_BT1_USD\" % constant.EX_BFX:\n            ex = Bitfinex('bt1usd')\n        elif name == \"%s_BT2_USD\" % constant.EX_BFX:\n            ex = Bitfinex('bt2usd')\n        elif name == \"%s_BT1_BTC\" % constant.EX_BFX:\n            ex = Bitfinex('bt1btc')\n        elif name == \"%s_BT2_BTC\" % constant.EX_BFX:\n            ex = Bitfinex('bt2btc')\n        elif name == \"%s_EOS_USD\" % constant.EX_BFX:\n            ex = Bitfinex('eosusd')\n        elif name == \"%s_EOS_BTC\" % constant.EX_BFX:\n            ex = Bitfinex('eosbtc')\n        elif name == \"%s_EOS_ETH\" % constant.EX_BFX:\n            ex = Bitfinex('eoseth')\n        elif name == \"%s_NEO_BTC\" % constant.EX_BFX:\n            ex = Bitfinex('neobtc')\n        elif name == \"%s_NEO_ETH\" % constant.EX_BFX:\n            ex = Bitfinex('neoeth')\n        elif name == \"%s_NEO_USD\" % constant.EX_BFX:\n            ex = Bitfinex('neousd')\n        elif name == \"%s_IOT_USD\" % constant.EX_BFX:\n            ex = Bitfinex('iotusd')\n        elif name == \"%s_IOT_BTC\" % constant.EX_BFX:\n            ex = Bitfinex('iotbtc')\n        elif name == \"%s_ZRX_ETH\" % constant.EX_BFX:\n            ex = Bitfinex('zrxeth')\n        elif name == \"%s_ZRX_BTC\" % constant.EX_BFX:\n            ex = Bitfinex('zrxbtc')\n        elif name == \"%s_BCH_BTC\" % constant.EX_KKEX:\n            ex = Kkex('BCHBTC')\n        elif name == \"%s_BCC_BTC\" % constant.EX_LQ:\n            ex = Liqui('bcc_btc')\n        elif name == \"%s_BCC_ETH\" % constant.EX_LQ:\n            ex = Liqui('bcc_eth')\n        elif name == \"%s_EOS_BTC\" % constant.EX_LQ:\n            ex = Liqui('eos_btc')\n        elif name == \"%s_BCC_BTC\" % constant.EX_HITBITC:\n            ex = Hitbtc('bccbtc')\n        elif name == \"%s_BCC_BTC\" % constant.EX_CEX:\n            ex = Cex('bccbtc')\n        elif name == \"%s_ZEC_BTC\" % constant.EX_BITTREX:\n            ex = Bittrex('BTC-ZEC')\n        elif name == \"%s_BTC_USDT\" % constant.EX_BINANCE:\n            ex = Binance('BTCUSDT')\n        elif name == \"%s_BCC_BTC\" % constant.EX_BINANCE:\n            ex = Binance('BCCBTC')\n        elif name == \"%s_ETH_BTC\" % constant.EX_BINANCE:\n            ex = Binance('ETHBTC')\n        elif name == \"%s_ETH_USDT\" % constant.EX_BINANCE:\n            ex = Binance('ETHUSDT')\n        elif name == \"%s_BNB_BTC\" % constant.EX_BINANCE:\n            ex = Binance('BNBBTC')\n        elif name == \"%s_BNB_ETH\" % constant.EX_BINANCE:\n            ex = Binance('BNBETH')\n        elif name == \"%s_MCO_BTC\" % constant.EX_BINANCE:\n            ex = Binance('MCOBTC')\n        elif name == \"%s_MCO_ETH\" % constant.EX_BINANCE:\n            ex = Binance('MCOETH')\n        elif name == \"%s_QTUM_BTC\" % constant.EX_BINANCE:\n            ex = Binance('QTUMBTC')\n        elif name == \"%s_QTUM_ETH\" % constant.EX_BINANCE:\n            ex = Binance('QTUMETH')\n        elif name == \"%s_WTC_BTC\" % constant.EX_BINANCE:\n            ex = Binance('WTCBTC')\n        elif name == \"%s_WTC_ETH\" % constant.EX_BINANCE:\n            ex = Binance('WTCETH')\n        elif name == \"%s_NEO_BTC\" % constant.EX_BINANCE:\n            ex = Binance('NEOBTC')\n        elif name == \"%s_NEO_ETH\" % constant.EX_BINANCE:\n            ex = Binance('NEOETH')\n        elif name == \"%s_IOTA_ETH\" % constant.EX_BINANCE:\n            ex = Binance('IOTAETH')\n        elif name == \"%s_IOTA_BTC\" % constant.EX_BINANCE:\n            ex = Binance('IOTABTC')\n        elif name == \"%s_ZRX_BTC\" % constant.EX_BINANCE:\n            ex = Binance('ZRXBTC')\n        elif name == \"%s_ZRX_ETH\" % constant.EX_BINANCE:\n            ex = Binance('ZRXETH')\n        elif name == \"%s_ETH_BTC\" % constant.EX_GATE:\n            ex = Gate('eth_btc')\n        elif name == \"%s_BCC_BTC\" % constant.EX_GATE:\n            ex = Gate('bcc_btc')\n        elif name == \"%s_BTC_JPY\" % constant.EX_BITFLYER:\n", "next_line": "            ex = Bitflyer('btc_jpy')", "gold_snippet_index": 9, "id": 3, "__internal_uuid__": "7d7c4459-db3b-4403-b2bc-9250228385ba"}
{"repo_name": "lemenkov/sippy", "file_path": "sippy/b2bua_radius.py", "context": "[{\"identifier\":\"Signal\", \"path\":\"sippy/Signal.py\", \"snippet\":\"class Signal(object):\\n    callback = None\\n    parameters = None\\n    previous_handler = None\\n\\n    def __init__(self, signum, callback, *parameters):\\n        self.callback = callback\\n        self.parameters = parameters\\n        self.previous_handler = signal(signum, self.signal_handler)\\n\\n    def signal_handler(self, signum, *frame):\\n        try:\\n            reactor.callFromThread(self.callback, *self.parameters)\\n        except:\\n            print datetime.now(), 'Signal: unhandled exception in signal callback'\\n            print '-' * 70\\n            print_exc(file = stdout)\\n            print '-' * 70\\n            stdout.flush()\\n        if self.previous_handler not in (SIG_IGN, SIG_DFL):\\n            try:\\n                self.previous_handler(signum, *frame)\\n            except:\\n                print datetime.now(), 'Signal: unhandled exception in signal chain'\\n                print '-' * 70\\n                print_exc(file = stdout)\\n                print '-' * 70\\n                stdout.flush()\"}, {\"identifier\":\"SipFrom\", \"path\":\"sippy/SipFrom.py\", \"snippet\":\"class SipFrom(SipAddressHF):\\n    hf_names = ('from', 'f')\\n\\n    def __init__(self, body = None, address = None):\\n        SipAddressHF.__init__(self, body, address)\\n        if body == None and address == None:\\n            self.address = SipAddress(name = 'Anonymous', url = SipURL(host = SipConf.my_address, port = SipConf.my_port))\\n\\n    def getTag(self):\\n        return self.address.getParam('tag')\\n\\n    def genTag(self):\\n        self.address.setParam('tag', md5(str((random() * 1000000000L) + time())).hexdigest())\\n\\n    def setTag(self, value):\\n        self.address.setParam('tag', value)\\n\\n    def delTag(self):\\n        self.address.delParam('tag')\\n\\n    def getCanName(self, name, compact = False):\\n        if compact:\\n            return 'f'\\n        return 'From'\"}, {\"identifier\":\"SipTo\", \"path\":\"sippy/SipTo.py\", \"snippet\":\"class SipTo(SipFrom):\\n    hf_names = ('to', 't')\\n\\n    def getCanName(self, name, compact = False):\\n        if compact:\\n            return 't'\\n        return 'To'\"}, {\"identifier\":\"SipCiscoGUID\", \"path\":\"sippy/SipCiscoGUID.py\", \"snippet\":\"class SipCiscoGUID(SipGenericHF):\\n    hf_names = ('cisco-guid', 'h323-conf-id')\\n    ciscoGUID = None\\n\\n    def __init__(self, body = None, ciscoGUID = None):\\n        SipGenericHF.__init__(self, body)\\n        if body != None:\\n            return\\n        self.parsed = True\\n        if ciscoGUID != None:\\n            self.ciscoGUID = ciscoGUID\\n        else:\\n            s = md5(str((random() * 1000000000L) + time())).hexdigest()\\n            self.ciscoGUID = (long(s[0:8], 16), long(s[8:16], 16), long(s[16:24], 16), long(s[24:32], 16))\\n\\n    def parse(self):\\n        self.parsed = True\\n        self.ciscoGUID = tuple([int(x) for x in  self.body.split('-', 3)])\\n\\n    def __str__(self):\\n        if not self.parsed:\\n            return self.body\\n        return '%d-%d-%d-%d' % self.ciscoGUID\\n\\n    def getCiscoGUID(self):\\n        return self.ciscoGUID\\n\\n    def hexForm(self):\\n        return '%.8X %.8X %.8X %.8X' % self.ciscoGUID\\n\\n    def getCanName(self, name, compact = False):\\n        if name.lower() == 'h323-conf-id':\\n            return 'h323-conf-id'\\n        else:\\n            return 'cisco-GUID'\\n\\n    def getCopy(self):\\n        if not self.parsed:\\n            return SipCiscoGUID(self.body)\\n        return SipCiscoGUID(ciscoGUID = self.ciscoGUID)\"}, {\"identifier\":\"UA\", \"path\":\"sippy/UA.py\", \"snippet\":\"class UA(object):\\n    global_config = None\\n    state = None\\n    event_cb = None\\n    uasReq = None\\n    uacResp = None\\n    username = None\\n    password = None\\n    equeue = None\\n    dId = None\\n    credit_time = None\\n    credit_times = None\\n    credit_timer = None\\n    conn_cbs = None\\n    disc_cbs = None\\n    fail_cbs = None\\n    ring_cbs = None\\n    dead_cbs = None\\n    rCSeq = None\\n    lTag = None\\n    lUri = None\\n    rUri = None\\n    cId = None\\n    lCSeq = None\\n    lContact = None\\n    cGUID = None\\n    rAddr = None\\n    rAddr0 = None\\n    routes = None\\n    rTarget = None\\n    uasResp = None\\n    lSDP = None\\n    rSDP = None\\n    kaInterval = 0\\n    branch = None\\n    reqs = None\\n    extra_headers = None\\n    useRefer = True\\n    expire_time = None\\n    expire_timer = None\\n    no_progress_time = None\\n    no_progress_timer = None\\n    no_reply_time = None\\n    no_reply_timer = None\\n    on_local_sdp_change = None\\n    on_remote_sdp_change = None\\n    last_scode = 100\\n    setup_ts = None\\n    p100_ts = None\\n    p1xx_ts = None\\n    connect_ts = None\\n    disconnect_ts = None\\n    user_agent = None\\n    elast_seq = None\\n    origin = None\\n    source_address = None\\n\\n    def __init__(self, global_config, event_cb = None, username = None, password = None, nh_address = None, credit_time = None, \\\\\\n      conn_cbs = None, disc_cbs = None, fail_cbs = None, ring_cbs = None, dead_cbs = None, ltag = None, extra_headers = None, \\\\\\n      expire_time = None, no_progress_time = None):\\n        self.global_config = global_config\\n        self.event_cb = event_cb\\n        self.equeue = []\\n        self.username = username\\n        self.password = password\\n        self.rAddr = nh_address\\n        self.rAddr0 = self.rAddr\\n        self.credit_time = credit_time\\n        self.credit_times = {}\\n        if conn_cbs != None:\\n            self.conn_cbs = conn_cbs\\n        else:\\n            self.conn_cbs = ()\\n        if disc_cbs != None:\\n            self.disc_cbs = disc_cbs\\n        else:\\n            self.disc_cbs = ()\\n        if fail_cbs != None:\\n            self.fail_cbs = fail_cbs\\n        else:\\n            self.fail_cbs = ()\\n        if ring_cbs != None:\\n            self.ring_cbs = ring_cbs\\n        else:\\n            self.ring_cbs = ()\\n        if dead_cbs != None:\\n            self.dead_cbs = dead_cbs\\n        else:\\n            self.dead_cbs = ()\\n        if ltag != None:\\n            self.lTag = ltag\\n        else:\\n            self.lTag = md5(str((random() * 1000000000L) + time())).hexdigest()\\n        self.reqs = {}\\n        self.extra_headers = extra_headers\\n        self.expire_time = expire_time\\n        self.no_progress_time = no_progress_time\\n        #print self.username, self.password\\n\\n    def recvRequest(self, req):\\n        #print 'Received request %s in state %s instance %s' % (req.getMethod(), self.state, self)\\n        #print self.rCSeq, req.getHFBody('cseq').getCSeqNum()\\n        if self.user_agent == None:\\n            self.update_ua(req)\\n        if self.rCSeq != None and self.rCSeq >= req.getHFBody('cseq').getCSeqNum():\\n            return (req.genResponse(500, 'Server Internal Error'), None, None)\\n        self.rCSeq = req.getHFBody('cseq').getCSeqNum()\\n        if self.state == None:\\n            if req.getMethod() == 'INVITE':\\n                self.changeState((UasStateIdle,))\\n            else:\\n                return None\\n        newstate = self.state.recvRequest(req)\\n        if newstate != None:\\n            self.changeState(newstate)\\n        self.emitPendingEvents()\\n        if newstate != None and req.getMethod() == 'INVITE':\\n            return (None, self.state.cancel, self.disconnect)\\n        else:\\n            return None\\n\\n    def recvResponse(self, resp):\\n        if self.state == None:\\n            return\\n        self.update_ua(resp)\\n        code, reason = resp.getSCode()\\n        cseq, method = resp.getHFBody('cseq').getCSeq()\\n        if method == 'INVITE' and self.reqs.has_key(cseq) and code == 401 and resp.countHFs('www-authenticate') != 0 and \\\\\\n          self.username != None and self.password != None and self.reqs[cseq].countHFs('authorization') == 0:\\n            challenge = resp.getHFBody('www-authenticate')\\n            req = self.genRequest('INVITE', self.lSDP, challenge.getNonce(), challenge.getRealm())\\n            self.lCSeq += 1\\n            self.tr = self.global_config['_sip_tm'].newTransaction(req, self.recvResponse, \\\\\\n              laddress = self.source_address)\\n            del self.reqs[cseq]\\n            return None\\n        if method == 'INVITE' and self.reqs.has_key(cseq) and code == 407 and resp.countHFs('proxy-authenticate') != 0 and \\\\\\n          self.username != None and self.password != None and self.reqs[cseq].countHFs('proxy-authorization') == 0:\\n            challenge = resp.getHFBody('proxy-authenticate')\\n            req = self.genRequest('INVITE', self.lSDP, challenge.getNonce(), challenge.getRealm(), SipProxyAuthorization)\\n            self.lCSeq += 1\\n            self.tr = self.global_config['_sip_tm'].newTransaction(req, self.recvResponse, \\\\\\n              laddress = self.source_address)\\n            del self.reqs[cseq]\\n            return None\\n        if code >= 200 and self.reqs.has_key(cseq):\\n            del self.reqs[cseq]\\n        newstate = self.state.recvResponse(resp)\\n        if newstate != None:\\n            self.changeState(newstate)\\n        self.emitPendingEvents()\\n\\n    def recvEvent(self, event):\\n        #print self, event\\n        if self.state == None:\\n            if isinstance(event, CCEventTry) or isinstance(event, CCEventFail) or isinstance(event, CCEventDisconnect):\\n                self.changeState((UacStateIdle,))\\n            else:\\n                return\\n        newstate = self.state.recvEvent(event)\\n        if newstate != None:\\n            self.changeState(newstate)\\n        self.emitPendingEvents()\\n\\n    def disconnect(self, rtime = None):\\n        if rtime == None:\\n            rtime = time()\\n        self.equeue.append(CCEventDisconnect(rtime = rtime))\\n        self.recvEvent(CCEventDisconnect(rtime = rtime))\\n\\n    def expires(self):\\n        self.expire_timer = None\\n        self.disconnect()\\n\\n    def no_progress_expires(self):\\n        self.no_progress_timer = None\\n        self.disconnect()\\n\\n    def no_reply_expires(self):\\n        self.no_reply_timer = None\\n        self.disconnect()\\n\\n    def credit_expires(self, rtime):\\n        self.credit_timer = None\\n        self.disconnect(rtime)\\n\\n    def changeState(self, newstate):\\n        if self.state != None:\\n            self.state.onStateChange(newstate[0])\\n        self.state = newstate[0](self)\\n        if len(newstate) > 1:\\n            for callback in newstate[1]:\\n                callback(self, *newstate[2:])\\n\\n    def emitEvent(self, event):\\n        if self.event_cb != None:\\n            if self.elast_seq != None and self.elast_seq >= event.seq:\\n                #print 'ignoring out-of-order event', event, event.seq, self.elast_seq, self.cId\\n                return\\n            self.elast_seq = event.seq\\n            self.event_cb(event, self)\\n\\n    def emitPendingEvents(self):\\n        while len(self.equeue) != 0 and self.event_cb != None:\\n            event = self.equeue.pop(0)\\n            if self.elast_seq != None and self.elast_seq >= event.seq:\\n                #print 'ignoring out-of-order event', event, event.seq, self.elast_seq, self.cId\\n                continue\\n            self.elast_seq = event.seq\\n            self.event_cb(event, self)\\n\\n    def genRequest(self, method, body = None, nonce = None, realm = None, SipXXXAuthorization = SipAuthorization, \\\\\\n      reason = None):\\n        req = SipRequest(method = method, ruri = self.rTarget, to = self.rUri, fr0m = self.lUri,\\n                         cseq = self.lCSeq, callid = self.cId, contact = self.lContact,\\n                         routes = self.routes, target = self.rAddr, cguid = self.cGUID)\\n        if nonce != None and realm != None and self.username != None and self.password != None:\\n            auth = SipXXXAuthorization(realm = realm, nonce = nonce, method = method, uri = str(self.rTarget),\\n              username = self.username, password = self.password)\\n            req.appendHeader(SipHeader(body = auth))\\n        if body != None:\\n            req.setBody(body)\\n        if self.extra_headers != None:\\n            req.appendHeaders(self.extra_headers)\\n        if reason != None:\\n            req.appendHeader(SipHeader(body = reason))\\n        self.reqs[self.lCSeq] = req\\n        return req\\n\\n    def sendUasResponse(self, scode, reason, body = None, contact = None, \\\\\\n      reason_rfc3326 = None, extra_header = None):\\n        self.uasResp.setSCode(scode, reason)\\n        self.uasResp.setBody(body)\\n        self.uasResp.delHFs('www-authenticate')\\n        self.uasResp.delHFs('contact')\\n        self.uasResp.delHFs('reason')\\n        if contact != None:\\n            self.uasResp.appendHeader(SipHeader(name = 'contact', body = contact))\\n        if reason_rfc3326 != None:\\n            self.uasResp.appendHeader(SipHeader(body = reason_rfc3326))\\n        if extra_header != None:\\n            self.uasResp.appendHeader(extra_header)\\n        self.global_config['_sip_tm'].sendResponse(self.uasResp)\\n\\n    def isYours(self, req = None, call_id = None, from_tag = None, to_tag = None):\\n        #print self.branch, req.getHFBody('via').getBranch()\\n        if req != None:\\n            if req.getMethod() != 'BYE' and self.branch != None and \\\\\\n              self.branch != req.getHFBody('via').getBranch():\\n                return None\\n            call_id = str(req.getHFBody('call-id'))\\n            from_tag = req.getHFBody('from').getTag()\\n            to_tag = req.getHFBody('to').getTag()\\n        #print str(self.cId), call_id\\n        if call_id != str(self.cId):\\n            return None\\n        #print self.rUri.getTag(), from_tag\\n        if self.rUri != None and self.rUri.getTag() != from_tag:\\n            return None\\n        #print self.lUri.getTag(), to_tag\\n        if self.lUri != None and self.lUri.getTag() != to_tag:\\n            return None\\n        return self\\n\\n    def isDead(self):\\n        if self.state != None:\\n            return self.state.dead\\n        return False\\n\\n    def isConnected(self):\\n        if self.state != None:\\n            return self.state.connected\\n        return False\\n\\n    def getCLD(self):\\n        if self.rUri == None:\\n            return None\\n        return self.rUri.getUrl().username\\n\\n    def getCLI(self):\\n        if self.lUri == None:\\n            return None\\n        return self.lUri.getUrl().username\\n\\n    def getCallingName(self):\\n        if self.lUri == None:\\n            return None\\n        return self.lUri.getUri().name\\n\\n    def getRAddr0(self):\\n        return self.rAddr0\\n\\n    def getCID(self):\\n        # Return tuple containing call-id, local tag and remote tag\\n        rval = [str(self.cId), None, None]\\n        if self.lUri != None:\\n            rval[1] = self.lUri.getTag()\\n        if self.rUri != None:\\n            rval[2] = self.rUri.getTag()\\n        return tuple(rval)\\n\\n    def delayed_remote_sdp_update(self, event, remote_sdp_body):\\n        self.rSDP = remote_sdp_body.getCopy()\\n        self.equeue.append(event)\\n        self.emitPendingEvents()\\n\\n    def getAcct(self):\\n        if self.disconnect_ts != None:\\n            disconnect_ts = self.disconnect_ts\\n            disconnected = True\\n        else:\\n            disconnect_ts = time()\\n            disconnected = False\\n        if self.connect_ts != None:\\n            return (disconnect_ts - self.connect_ts, self.connect_ts - self.setup_ts, True, disconnected)\\n        return (0, disconnect_ts - self.setup_ts, False, disconnected)\\n\\n    def update_ua(self, msg):\\n        if msg.countHFs('user-agent') > 0:\\n            self.user_agent = msg.getHFBody('user-agent').name\\n        elif msg.countHFs('server') > 0:\\n            self.user_agent = msg.getHFBody('server').name\\n        return\\n\\n    def cancelCreditTimer(self):\\n        if self.credit_timer != None:\\n            self.credit_timer.cancel()\\n            self.credit_timer = None\\n\\n    def startCreditTimer(self, rtime):\\n        if self.credit_time != None:\\n            self.credit_times[0] = rtime + self.credit_time\\n            self.credit_time = None\\n        try:\\n            credit_time = min([x for x in self.credit_times.values() if x != None])\\n        except ValueError:\\n            return\\n        self.credit_timer = TimeoutAbs(self.credit_expires, credit_time, credit_time)\\n\\n    def resetCreditTime(self, rtime, new_credit_times):\\n        self.credit_times.update(new_credit_times)\\n        if self.state.connected:\\n            self.cancelCreditTimer()\\n            self.startCreditTimer(rtime)\\n\\n    def cleanup(self):\\n        pass\"}, {\"identifier\":\"CCEventRing\", \"path\":\"sippy/CCEvents.py\", \"snippet\":\"class CCEventRing(CCEventGeneric):\\n    name = 'CCEventRing'\\n    pass\"}, {\"identifier\":\"CCEventConnect\", \"path\":\"sippy/CCEvents.py\", \"snippet\":\"class CCEventConnect(CCEventGeneric):\\n    name = 'CCEventConnect'\\n    pass\"}, {\"identifier\":\"CCEventDisconnect\", \"path\":\"sippy/CCEvents.py\", \"snippet\":\"class CCEventDisconnect(CCEventGeneric):\\n    name = 'CCEventDisconnect'\\n    pass\"}, {\"identifier\":\"CCEventTry\", \"path\":\"sippy/CCEvents.py\", \"snippet\":\"class CCEventTry(CCEventGeneric):\\n    name = 'CCEventTry'\\n    pass\"}, {\"identifier\":\"CCEventUpdate\", \"path\":\"sippy/CCEvents.py\", \"snippet\":\"class CCEventUpdate(CCEventGeneric):\\n    name = 'CCEventUpdate'\\n    pass\"}, {\"identifier\":\"CCEventFail\", \"path\":\"sippy/CCEvents.py\", \"snippet\":\"class CCEventFail(CCEventGeneric):\\n    name = 'CCEventFail'\\n    pass\"}, {\"identifier\":\"UasStateTrying\", \"path\":\"sippy/UasStateTrying.py\", \"snippet\":\"class UasStateTrying(UaStateGeneric):\\n    sname = 'Trying(UAS)'\\n\\n    def recvEvent(self, event):\\n        if isinstance(event, CCEventRing):\\n            scode = event.getData()\\n            if scode == None:\\n                code, reason, body = (180, 'Ringing', None)\\n            else:\\n                code, reason, body = scode\\n                if code == 100:\\n                    return None\\n                if body != None and self.ua.on_local_sdp_change != None and body.needs_update:\\n                    self.ua.on_local_sdp_change(body, lambda x: self.ua.recvEvent(event))\\n                    return None\\n            self.ua.lSDP = body\\n            self.ua.sendUasResponse(code, reason, body)\\n            if self.ua.no_progress_timer != None:\\n                self.ua.no_progress_timer.cancel()\\n                self.ua.no_progress_timer = None\\n                if self.ua.expire_time != None:\\n                    self.ua.expire_timer = TimeoutAbs(self.ua.expires, self.ua.expire_time)\\n            if self.ua.p1xx_ts == None:\\n                self.ua.p1xx_ts = event.rtime\\n            return (UasStateRinging, self.ua.ring_cbs, event.rtime, event.origin, code)\\n        elif isinstance(event, CCEventConnect):\\n            code, reason, body = event.getData()\\n            if body != None and self.ua.on_local_sdp_change != None and body.needs_update:\\n                self.ua.on_local_sdp_change(body, lambda x: self.ua.recvEvent(event))\\n                return None\\n            self.ua.lSDP = body\\n            self.ua.sendUasResponse(code, reason, body, self.ua.lContact)\\n            if self.ua.expire_timer != None:\\n                self.ua.expire_timer.cancel()\\n                self.ua.expire_timer = None\\n            if self.ua.no_progress_timer != None:\\n                self.ua.no_progress_timer.cancel()\\n                self.ua.no_progress_timer = None\\n            self.ua.startCreditTimer(event.rtime)\\n            self.ua.connect_ts = event.rtime\\n            return (UaStateConnected, self.ua.conn_cbs, event.rtime, event.origin)\\n        elif isinstance(event, CCEventRedirect):\\n            scode = event.getData()\\n            if scode == None:\\n                scode = (500, 'Failed', None, None)\\n            self.ua.sendUasResponse(scode[0], scode[1], scode[2], SipContact(address = SipAddress(url = scode[3])))\\n            if self.ua.expire_timer != None:\\n                self.ua.expire_timer.cancel()\\n                self.ua.expire_timer = None\\n            if self.ua.no_progress_timer != None:\\n                self.ua.no_progress_timer.cancel()\\n                self.ua.no_progress_timer = None\\n            self.ua.disconnect_ts = event.rtime\\n            return (UaStateFailed, self.ua.fail_cbs, event.rtime, event.origin, scode[0])\\n        elif isinstance(event, CCEventFail):\\n            scode = event.getData()\\n            if scode == None:\\n                scode = (500, 'Failed')\\n            self.ua.sendUasResponse(scode[0], scode[1], reason_rfc3326 = event.reason, \\\\\\n              extra_header = event.extra_header)\\n            if self.ua.expire_timer != None:\\n                self.ua.expire_timer.cancel()\\n                self.ua.expire_timer = None\\n            if self.ua.no_progress_timer != None:\\n                self.ua.no_progress_timer.cancel()\\n                self.ua.no_progress_timer = None\\n            self.ua.disconnect_ts = event.rtime\\n            return (UaStateFailed, self.ua.fail_cbs, event.rtime, event.origin, scode[0])\\n        elif isinstance(event, CCEventDisconnect):\\n            #import sys, traceback\\n            #traceback.print_stack(file = sys.stdout)\\n            self.ua.sendUasResponse(500, 'Disconnected', reason_rfc3326 = event.reason)\\n            if self.ua.expire_timer != None:\\n                self.ua.expire_timer.cancel()\\n                self.ua.expire_timer = None\\n            if self.ua.no_progress_timer != None:\\n                self.ua.no_progress_timer.cancel()\\n                self.ua.no_progress_timer = None\\n            self.ua.disconnect_ts = event.rtime\\n            return (UaStateDisconnected, self.ua.disc_cbs, event.rtime, event.origin, self.ua.last_scode)\\n        #print 'wrong event %s in the Trying state' % event\\n        return None\\n\\n    def cancel(self, rtime, req):\\n        self.ua.disconnect_ts = rtime\\n        self.ua.changeState((UaStateDisconnected, self.ua.disc_cbs, rtime, self.ua.origin))\\n        event = CCEventDisconnect(rtime = rtime, origin = self.ua.origin)\\n        if req != None:\\n            try:\\n                event.reason = req.getHFBody('reason')\\n            except:\\n                pass\\n        self.ua.emitEvent(event)\"}, {\"identifier\":\"UasStateRinging\", \"path\":\"sippy/UasStateRinging.py\", \"snippet\":\"class UasStateRinging(UaStateGeneric):\\n    sname = 'Ringing(UAS)'\\n\\n    def recvEvent(self, event):\\n        if isinstance(event, CCEventRing):\\n            scode = event.getData()\\n            if scode == None:\\n                code, reason, body = (180, 'Ringing', None)\\n            else:\\n                code, reason, body = scode\\n                if code == 100:\\n                    return None\\n                if body != None and self.ua.on_local_sdp_change != None and body.needs_update:\\n                    self.ua.on_local_sdp_change(body, lambda x: self.ua.recvEvent(event))\\n                    return None\\n            self.ua.lSDP = body\\n            if self.ua.p1xx_ts == None:\\n                self.ua.p1xx_ts = event.rtime\\n            self.ua.sendUasResponse(code, reason, body)\\n            for ring_cb in self.ua.ring_cbs:\\n                ring_cb(self.ua, event.rtime, event.origin, code)\\n            return None\\n        elif isinstance(event, CCEventConnect):\\n            code, reason, body = event.getData()\\n            if body != None and self.ua.on_local_sdp_change != None and body.needs_update:\\n                self.ua.on_local_sdp_change(body, lambda x: self.ua.recvEvent(event))\\n                return None\\n            self.ua.lSDP = body\\n            self.ua.sendUasResponse(code, reason, body, self.ua.lContact)\\n            if self.ua.expire_timer != None:\\n                self.ua.expire_timer.cancel()\\n                self.ua.expire_timer = None\\n            self.ua.startCreditTimer(event.rtime)\\n            self.ua.connect_ts = event.rtime\\n            return (UaStateConnected, self.ua.conn_cbs, event.rtime, event.origin)\\n        elif isinstance(event, CCEventRedirect):\\n            scode = event.getData()\\n            if scode == None:\\n                scode = (500, 'Failed', None, None)\\n            self.ua.sendUasResponse(scode[0], scode[1], scode[2], SipContact(address = SipAddress(url = scode[3])))\\n            if self.ua.expire_timer != None:\\n                self.ua.expire_timer.cancel()\\n                self.ua.expire_timer = None\\n            self.ua.disconnect_ts = event.rtime\\n            return (UaStateFailed, self.ua.fail_cbs, event.rtime, event.origin, scode[0])\\n        elif isinstance(event, CCEventFail):\\n            scode = event.getData()\\n            if scode == None:\\n                scode = (500, 'Failed')\\n            self.ua.sendUasResponse(scode[0], scode[1], reason_rfc3326 = event.reason, \\\\\\n              extra_header = event.extra_header)\\n            if self.ua.expire_timer != None:\\n                self.ua.expire_timer.cancel()\\n                self.ua.expire_timer = None\\n            self.ua.disconnect_ts = event.rtime\\n            return (UaStateFailed, self.ua.fail_cbs, event.rtime, event.origin, scode[0])\\n        elif isinstance(event, CCEventDisconnect):\\n            #import sys, traceback\\n            #traceback.print_stack(file = sys.stdout)\\n            self.ua.sendUasResponse(500, 'Disconnected', reason_rfc3326 = event.reason)\\n            if self.ua.expire_timer != None:\\n                self.ua.expire_timer.cancel()\\n                self.ua.expire_timer = None\\n            self.ua.disconnect_ts = event.rtime\\n            return (UaStateDisconnected, self.ua.disc_cbs, event.rtime, event.origin, self.ua.last_scode)\\n        #print 'wrong event %s in the Ringing state' % event\\n        return None\\n\\n    def recvRequest(self, req):\\n        if req.getMethod() == 'BYE':\\n            self.ua.sendUasResponse(487, 'Request Terminated')\\n            self.ua.global_config['_sip_tm'].sendResponse(req.genResponse(200, 'OK'))\\n            #print 'BYE received in the Ringing state, going to the Disconnected state'\\n            if req.countHFs('also') > 0:\\n                also = req.getHFBody('also').getUrl().getCopy()\\n            else:\\n                also = None\\n            event = CCEventDisconnect(also, rtime = req.rtime, origin = self.ua.origin)\\n            try:\\n                event.reason = req.getHFBody('reason')\\n            except:\\n                pass\\n            self.ua.equeue.append(event)\\n            if self.ua.expire_timer != None:\\n                self.ua.expire_timer.cancel()\\n                self.ua.expire_timer = None\\n            self.ua.disconnect_ts = req.rtime\\n            return (UaStateDisconnected, self.ua.disc_cbs, req.rtime, self.ua.origin)\\n        return None\\n\\n    def cancel(self, rtime, req):\\n        self.ua.disconnect_ts = rtime\\n        self.ua.changeState((UaStateDisconnected, self.ua.disc_cbs, rtime, self.ua.origin))\\n        event = CCEventDisconnect(rtime = rtime, origin = self.ua.origin)\\n        if req != None:\\n            try:\\n                event.reason = req.getHFBody('reason')\\n            except:\\n                pass\\n        self.ua.emitEvent(event)\"}, {\"identifier\":\"UaStateDead\", \"path\":\"sippy/UaStateDead.py\", \"snippet\":\"class UaStateDead(UaStateGeneric):\\n    sname = 'Dead'\\n    dead = True\\n\\n    def __init__(self, ua):\\n        UaStateGeneric.__init__(self, None)\\n        if ua.cId != None:\\n            ua.global_config['_sip_tm'].unregConsumer(ua, str(ua.cId))\\n        ua.tr = None\\n        ua.event_cb = None\\n        ua.conn_cbs = ()\\n        ua.disc_cbs = ()\\n        ua.fail_cbs = ()\\n        ua.on_local_sdp_change = None\\n        ua.on_remote_sdp_change = None\\n        ua.expire_timer = None\\n        ua.no_progress_timer = None\\n        ua.credit_timer = None\\n        # Keep this at the very end of processing\\n        for callback in ua.dead_cbs:\\n            callback(ua)\\n        ua.dead_cbs = ()\\n        ua.cleanup()\\n        # Break cross-ref chain\\n        self.ua = None\"}, {\"identifier\":\"SipHeader\", \"path\":\"sippy/SipHeader.py\", \"snippet\":\"class SipHeader(object):\\n    name = None\\n    body = None\\n\\n    def __init__(self, s = None, name = None, body = None, bodys = None, fixname = False):\\n        if s != None:\\n            name, bodys = [x.strip() for x in s.split(':', 1)]\\n        if name != None:\\n            self.name = name.lower()\\n        if body == None:\\n            try:\\n                try:\\n                    body = hf_types[self.name](bodys)\\n                except KeyError:\\n                    body = SipGenericHF(bodys, name)\\n            except ESipHeaderCSV, einst:\\n                einst.name = self.name\\n                raise einst\\n        self.body = body\\n        # If no name is provided use canonic name from the body-specific\\n        # class.\\n        if self.name == None or fixname:\\n            self.name = body.hf_names[0]\\n\\n    def __str__(self):\\n        return str(self.body.getCanName(self.name)) + ': ' + str(self.body)\\n\\n    def localStr(self, local_addr = None, local_port = None, compact = False):\\n        return str(self.body.getCanName(self.name, compact)) + ': ' + \\\\\\n          self.body.localStr(local_addr, local_port)\\n\\n    def getBody(self):\\n        if not self.body.parsed:\\n            self.body.parse()\\n        return self.body\"}, {\"identifier\":\"RadiusAuthorisation\", \"path\":\"sippy/RadiusAuthorisation.py\", \"snippet\":\"class RadiusAuthorisation(Radius_client):\\n    def do_auth(self, username, caller, callee, h323_cid, sip_cid, remote_ip, res_cb, \\\\\\n      realm = None, nonce = None, uri = None, response = None, extra_attributes = None):\\n        sip_cid = str(sip_cid)\\n        attributes = None\\n        if None not in (realm, nonce, uri, response):\\n            attributes = [('User-Name', username), ('Digest-Realm', realm), \\\\\\n              ('Digest-Nonce', nonce), ('Digest-Method', 'INVITE'), ('Digest-URI', uri), \\\\\\n              ('Digest-Algorithm', 'MD5'), ('Digest-User-Name', username), ('Digest-Response', response)]\\n        else:\\n            attributes = [('User-Name', remote_ip), ('Password', 'cisco')]\\n        if caller == None:\\n            caller = ''\\n        attributes.extend((('Calling-Station-Id', caller), ('Called-Station-Id', callee), ('h323-conf-id', h323_cid), \\\\\\n          ('call-id', sip_cid), ('h323-remote-address', remote_ip), ('h323-session-protocol', 'sipv2')))\\n        if extra_attributes != None:\\n            for a, v in extra_attributes:\\n                attributes.append((a, v))\\n        message = 'sending AAA request:\\\\n' \\n        message += reduce(lambda x, y: x + y, ['%-32s = \\\\'%s\\\\'\\\\n' % (x[0], str(x[1])) for x in attributes])\\n        self.global_config['_sip_logger'].write(message, call_id = sip_cid)\\n        Radius_client.do_auth(self, attributes, self._process_result, res_cb, sip_cid, time())\\n\\n    def _process_result(self, results, res_cb, sip_cid, btime):\\n        delay = time() - btime\\n        rcode = results[1]\\n        if rcode in (0, 1):\\n            if rcode == 0:\\n                message = 'AAA request accepted (delay is %.3f), processing response:\\\\n' % delay\\n            else:\\n                message = 'AAA request rejected (delay is %.3f), processing response:\\\\n' % delay\\n            if len(results[0]) > 0:\\n                message += reduce(lambda x, y: x + y, ['%-32s = \\\\'%s\\\\'\\\\n' % x for x in results[0]])\\n        else:\\n            message = 'Error sending AAA request (delay is %.3f)\\\\n' % delay\\n        self.global_config['_sip_logger'].write(message, call_id = sip_cid)\\n        res_cb(results)\"}, {\"identifier\":\"RadiusAccounting\", \"path\":\"sippy/RadiusAccounting.py\", \"snippet\":\"class RadiusAccounting(object):\\n    global_config = None\\n    drec = None\\n    crec = None\\n    iTime = None\\n    cTime = None\\n    sip_cid = None\\n    origin = None\\n    lperiod = None\\n    el = None\\n    send_start = None\\n    complete = False\\n    ms_precision = False\\n    user_agent = None\\n    p1xx_ts = None\\n    p100_ts = None\\n\\n    def __init__(self, global_config, origin, lperiod = None, send_start = False):\\n        self.global_config = global_config\\n        self._attributes = [('h323-call-origin', origin), ('h323-call-type', 'VoIP'), \\\\\\n          ('h323-session-protocol', 'sipv2')]\\n        self.drec = False\\n        self.crec = False\\n        self.origin = origin\\n        self.lperiod = lperiod\\n        self.send_start = send_start\\n\\n    def setParams(self, username, caller, callee, h323_cid, sip_cid, remote_ip, \\\\\\n      h323_in_cid = None):\\n        if caller == None:\\n            caller = ''\\n        self._attributes.extend((('User-Name', username), ('Calling-Station-Id', caller), \\\\\\n          ('Called-Station-Id', callee), ('h323-conf-id', h323_cid), ('call-id', sip_cid), \\\\\\n          ('Acct-Session-Id', sip_cid), ('h323-remote-address', remote_ip)))\\n        if h323_in_cid != None and h323_in_cid != h323_cid:\\n            self._attributes.append(('h323-incoming-conf-id', h323_in_cid))\\n        self.sip_cid = str(sip_cid)\\n        self.complete = True\\n\\n    def conn(self, ua, rtime, origin):\\n        if self.crec:\\n            return\\n        self.crec = True\\n        self.iTime = ua.setup_ts\\n        self.cTime = ua.connect_ts\\n        if ua.user_agent != None and self.user_agent == None:\\n            self.user_agent = ua.user_agent\\n        if ua.p1xx_ts != None:\\n            self.p1xx_ts = ua.p1xx_ts\\n        if ua.p100_ts != None:\\n            self.p100_ts = ua.p100_ts\\n        if self.send_start:\\n            self.asend('Start', rtime, origin, ua)\\n        self._attributes.extend((('h323-voice-quality', 0), ('Acct-Terminate-Cause', 'User-Request')))\\n        if self.lperiod != None and self.lperiod > 0:\\n            self.el = Timeout(self.asend, self.lperiod, -1, 'Alive')\\n\\n    def disc(self, ua, rtime, origin, result = 0):\\n        if self.drec:\\n            return\\n        self.drec = True\\n        if self.el != None:\\n            self.el.cancel()\\n            self.el = None\\n        if self.iTime == None:\\n            self.iTime = ua.setup_ts\\n        if self.cTime == None:\\n            self.cTime = rtime\\n        if ua.user_agent != None and self.user_agent == None:\\n            self.user_agent = ua.user_agent\\n        if ua.p1xx_ts != None:\\n            self.p1xx_ts = ua.p1xx_ts\\n        if ua.p100_ts != None:\\n            self.p100_ts = ua.p100_ts\\n        self.asend('Stop', rtime, origin, result, ua)\\n\\n    def asend(self, type, rtime = None, origin = None, result = 0, ua = None):\\n        if not self.complete:\\n            return\\n        if rtime == None:\\n            rtime = time()\\n        if ua != None:\\n            duration, delay, connected = ua.getAcct()[:3]\\n        else:\\n            # Alive accounting\\n            duration = rtime - self.cTime\\n            delay = self.cTime - self.iTime\\n            connected = True\\n        if not(self.ms_precision):\\n            duration = round(duration)\\n            delay = round(delay)\\n        attributes = self._attributes[:]\\n        if type != 'Start':\\n            if result >= 400:\\n                try:\\n                    dc = sipErrToH323Err[result][0]\\n                except:\\n                    dc = '7f'\\n            elif result < 200:\\n                dc = '10'\\n            else:\\n                dc = '0'\\n            attributes.extend((('h323-disconnect-time', self.ftime(self.iTime + delay + duration)), \\\\\\n              ('Acct-Session-Time', '%d' % round(duration)), ('h323-disconnect-cause', dc)))\\n        if type == 'Stop':\\n            if origin == 'caller':\\n                release_source = '2'\\n            elif origin == 'callee':\\n                release_source = '4'\\n            else:\\n                release_source = '8'\\n            attributes.append(('release-source', release_source))\\n        attributes.extend((('h323-connect-time', self.ftime(self.iTime + delay)), ('h323-setup-time', self.ftime(self.iTime)), \\\\\\n          ('Acct-Status-Type', type)))\\n        if self.user_agent != None:\\n            attributes.append(('h323-ivr-out', 'sip_ua:' + self.user_agent))\\n        if self.p1xx_ts != None:\\n            attributes.append(('Acct-Delay-Time', round(self.p1xx_ts)))\\n        if self.p100_ts != None:\\n            attributes.append(('provisional-timepoint', self.ftime(self.p100_ts)))\\n        pattributes = ['%-32s = \\\\'%s\\\\'\\\\n' % (x[0], str(x[1])) for x in attributes]\\n        pattributes.insert(0, 'sending Acct %s (%s):\\\\n' % (type, self.origin.capitalize()))\\n        self.global_config['_sip_logger'].write(call_id = self.sip_cid, *pattributes)\\n        self.global_config['_radius_client'].do_acct(attributes, self._process_result, self.sip_cid, time())\\n\\n    def ftime(self, t):\\n        gt = gmtime(t)\\n        day = strftime('%d', gt)\\n        if day[0] == '0':\\n            day = day[1]\\n        if self.ms_precision:\\n            msec = (t % 1) * 1000\\n        else:\\n            msec = 0\\n        return strftime('%%H:%%M:%%S.%.3d GMT %%a %%b %s %%Y' % (msec, day), gt)\\n\\n    def _process_result(self, results, sip_cid, btime):\\n        delay = time() - btime\\n        rcode = results[1]\\n        if rcode in (0, 1):\\n            if rcode == 0:\\n                message = 'Acct/%s request accepted (delay is %.3f)\\\\n' % (self.origin, delay)\\n            else:\\n                message = 'Acct/%s request rejected (delay is %.3f)\\\\n' % (self.origin, delay)\\n        else:\\n            message = 'Error sending Acct/%s request (delay is %.3f)\\\\n' % (self.origin, delay)\\n        self.global_config['_sip_logger'].write(message, call_id = sip_cid)\"}, {\"identifier\":\"SipLogger\", \"path\":\"sippy/SipLogger.py\", \"snippet\":\"class SipLogger(object):\\n    app = None\\n    call_id = None\\n    log = None\\n    level = None\\n    flock = lambda x, y, z: None\\n\\n    def __init__(self, app, call_id = 'GLOBAL', logfile = '/var/log/sip.log'):\\n        self.app = app\\n        self.call_id = call_id\\n        bend = os.environ.get('SIPLOG_BEND', 'stderr').lower()\\n        if bend == 'stderr':\\n            self.log = sys.__stderr__\\n        elif bend == 'none':\\n            self.write = self.donoting\\n        else:\\n            logfile = os.environ.get('SIPLOG_LOGFILE_FILE', logfile)\\n            self.log = file(logfile, 'a')\\n            self.flock = flock\\n            Signal(SIGUSR1, self.reopen, logfile)\\n        self.level = eval('SIPLOG_' + os.environ.get('SIPLOG_LVL', 'INFO'))\\n\\n    def donoting(self, *args, **kwargs):\\n        pass\\n\\n    def write(self, *args, **kwargs):\\n        if kwargs.get('level', SIPLOG_INFO) < self.level:\\n            return\\n        ltime = kwargs.get('ltime', None)\\n        if ltime == None:\\n            ltime = time()\\n        call_id = kwargs.get('call_id', self.call_id)\\n        obuf = '%s.%.3d/%s/%s: %s\\\\n' % (strftime('%d %b %H:%M:%S', localtime(ltime)), \\\\\\n          (ltime % 1) * 1000, call_id, self.app, \\\\\\n          reduce(lambda x, y: x + y, [str(x) for x in args]))\\n        try:\\n            self.flock(self.log, LOCK_EX)\\n        except IOError, e:\\n            # Catch ENOTSUP\\n            if e.args[0] != 45:\\n                raise e\\n            self.flock = lambda x, y: None\\n        try:\\n            self.log.write(obuf)\\n        except IOError, e:\\n            if e.args[0] != EINTR:\\n                raise e\\n        self.log.flush()\\n        self.flock(self.log, LOCK_UN)\\n\\n    def reopen(self, logfile):\\n        self.log = file(logfile, 'a')\"}, {\"identifier\":\"Rtp_proxy_session\", \"path\":\"sippy/Rtp_proxy_session.py\", \"snippet\":\"class Rtp_proxy_session(object):\\n    rtp_proxy_client = None\\n    call_id = None\\n    from_tag = None\\n    to_tag = None\\n    caller_session_exists = False\\n    caller_codecs = None\\n    caller_raddress = None\\n    callee_session_exists = False\\n    callee_codecs = None\\n    callee_raddress = None\\n    max_index = -1\\n    origin = None\\n    notify_socket = None\\n    notify_tag = None\\n    global_config = None\\n\\n    def __init__(self, global_config, call_id = None, from_tag = None, to_tag = None,\\n      notify_socket = None, notify_tag = None):\\n        self.global_config = global_config\\n        if global_config.has_key('_rtp_proxy_clients'):\\n            rtp_proxy_clients = [x for x in global_config['_rtp_proxy_clients'] if x.online]\\n            n = len(rtp_proxy_clients)\\n            if n == 0:\\n                raise Exception('No online RTP proxy client has been found')\\n            self.rtp_proxy_client = rtp_proxy_clients[int(random() * n)]\\n        else:\\n            self.rtp_proxy_client = global_config['rtp_proxy_client']\\n            if not self.rtp_proxy_client.online:\\n                raise Exception('No online RTP proxy client has been found')\\n        if call_id != None:\\n            self.call_id = call_id\\n        else:\\n            self.call_id = md5(str(random()) + str(time())).hexdigest()\\n        if from_tag != None:\\n            self.from_tag = from_tag\\n        else:\\n            self.from_tag = md5(str(random()) + str(time())).hexdigest()\\n        if to_tag != None:\\n            self.to_tag = to_tag\\n        else:\\n            self.to_tag = md5(str(random()) + str(time())).hexdigest()\\n        self.origin = SdpOrigin()\\n        self.notify_socket = notify_socket\\n        self.notify_tag = notify_tag\\n\\n    def version(self, result_callback):\\n        self.rtp_proxy_client.send_command('V', self.version_result, result_callback)\\n\\n    def version_result(self, result, result_callback):\\n        result_callback(result)\\n\\n    def play_caller(self, prompt_name, times = 1, result_callback = None, index = 0):\\n        if not self.caller_session_exists:\\n            return\\n        if not self.callee_session_exists:\\n            self.update_callee('0.0.0.0', 0, self._play_caller, '', index, prompt_name, times, result_callback, index)\\n            return\\n        self._play_caller(None, prompt_name, times, result_callback, index)\\n\\n    def _play_caller(self, result, prompt_name, times, result_callback, index):\\n        command = 'P%d %s %s %s %s %s' % (times, '%s-%d' % (self.call_id, index), prompt_name, self.caller_codecs, self.from_tag, self.to_tag)\\n        self.rtp_proxy_client.send_command(command, self.command_result, result_callback)\\n\\n    def play_callee(self, prompt_name, times = 1, result_callback = None, index = 0):\\n        if not self.callee_session_exists:\\n            return\\n        if not self.caller_session_exists:\\n            self.update_caller('0.0.0.0', 0, self._play_callee, '', index, prompt_name, times, result_callback, index)\\n            return\\n        self._play_callee(None, prompt_name, times, result_callback, index)\\n\\n    def _play_callee(self, result, prompt_name, times, result_callback, index):\\n        command = 'P%d %s %s %s %s %s' % (times, '%s-%d' % (self.call_id, index), prompt_name, self.callee_codecs, self.to_tag, self.from_tag)\\n        self.rtp_proxy_client.send_command(command, self.command_result, result_callback)\\n\\n    def stop_play_caller(self, result_callback = None, index = 0):\\n        if not self.caller_session_exists:\\n            return\\n        command = 'S %s %s %s' % ('%s-%d' % (self.call_id, index), self.from_tag, self.to_tag)\\n        self.rtp_proxy_client.send_command(command, self.command_result, result_callback)\\n\\n    def stop_play_callee(self, result_callback = None, index = 0):\\n        if not self.caller_session_exists:\\n            return\\n        command = 'S %s %s %s' % ('%s-%d' % (self.call_id, index), self.to_tag, self.from_tag)\\n        self.rtp_proxy_client.send_command(command, self.command_result, result_callback)\\n\\n    def copy_caller(self, remote_ip, remote_port, result_callback = None, index = 0):\\n        if not self.caller_session_exists:\\n            self.update_caller('0.0.0.0', 0, self._copy_caller, '', index, remote_ip, remote_port, result_callback, index)\\n            return\\n        self._copy_caller(None, remote_ip, remote_port, result_callback, index)\\n\\n    def _copy_caller(self, result, remote_ip, remote_port, result_callback = None, index = 0):\\n        command = 'C %s udp:%s:%d %s %s' % ('%s-%d' % (self.call_id, index), remote_ip, remote_port, self.from_tag, self.to_tag)\\n        self.rtp_proxy_client.send_command(command, self.command_result, result_callback)\\n\\n    def copy_callee(self, remote_ip, remote_port, result_callback = None, index = 0):\\n        if not self.callee_session_exists:\\n            self.update_callee('0.0.0.0', 0, self._copy_callee, '', index, remote_ip, remote_port, result_callback, index)\\n            return\\n        self._copy_callee(None, remote_ip, remote_port, result_callback, index)\\n\\n    def _copy_callee(self, result, remote_ip, remote_port, result_callback = None, index = 0):\\n        command = 'C %s udp:%s:%d %s %s' % ('%s-%d' % (self.call_id, index), remote_ip, remote_port, self.to_tag, self.from_tag)\\n        self.rtp_proxy_client.send_command(command, self.command_result, result_callback)\\n\\n    def start_recording(self, rname = None, result_callback = None, index = 0):\\n        if not self.caller_session_exists:\\n            self.update_caller('0.0.0.0', 0, self._start_recording, '', index, rname, result_callback, index)\\n            return\\n        self._start_recording(None, rname, result_callback, index)\\n\\n    def _start_recording(self, result, rname, result_callback, index):\\n        if rname == None:\\n            command = 'R %s %s %s' % ('%s-%d' % (self.call_id, index), self.from_tag, self.to_tag)\\n            return self.rtp_proxy_client.send_command(command, self.command_result, result_callback)\\n        command = 'C %s %s.a %s %s' % ('%s-%d' % (self.call_id, index), rname, self.from_tag, self.to_tag)\\n        return self.rtp_proxy_client.send_command(command, self._start_recording1, \\\\\\n          (rname, result_callback, index))\\n\\n    def _start_recording1(self, result, args):\\n        rname, result_callback, index = args\\n        command = 'C %s %s.o %s %s' % ('%s-%d' % (self.call_id, index), rname, self.to_tag, self.from_tag)\\n        return self.rtp_proxy_client.send_command(command, self.command_result, result_callback)\\n\\n    def command_result(self, result, result_callback):\\n        if result_callback != None:\\n            result_callback(result)\\n\\n    def update_caller(self, remote_ip, remote_port, result_callback, options = '', index = 0, *callback_parameters):\\n        command = 'U'\\n        self.max_index = max(self.max_index, index)\\n        if self.rtp_proxy_client.sbind_supported and self.caller_raddress != None:\\n            if self.rtp_proxy_client.is_local:\\n                options += 'L%s' % self.global_config['_sip_tm'].l4r.getServer( \\\\\\n                  self.caller_raddress).laddress[0]\\n            else:\\n                options += 'R%s' % self.caller_raddress[0]\\n        command += options\\n        command += ' %s %s %d %s' % ('%s-%d' % (self.call_id, index), remote_ip, remote_port, self.from_tag)\\n        if self.caller_session_exists:\\n            command += ' %s' % self.to_tag\\n        if self.notify_socket != None and index == 0 and \\\\\\n          self.rtp_proxy_client.tnot_supported:\\n            command += ' %s %s' % (self.notify_socket, self.notify_tag)\\n        self.rtp_proxy_client.send_command(command, self.update_result, (result_callback, 'caller', callback_parameters))\\n\\n    def update_callee(self, remote_ip, remote_port, result_callback, options = '', index = 0, *callback_parameters):\\n        command = 'U'\\n        self.max_index = max(self.max_index, index)\\n        if self.rtp_proxy_client.sbind_supported and self.callee_raddress != None:\\n            if self.rtp_proxy_client.is_local:\\n                options += 'L%s' % self.global_config['_sip_tm'].l4r.getServer( \\\\\\n                  self.callee_raddress).laddress[0]\\n            else:\\n                options += 'R%s' % self.callee_raddress[0]\\n        command += options\\n        command += ' %s %s %d %s %s' % ('%s-%d' % (self.call_id, index), remote_ip, remote_port, self.to_tag, self.from_tag)\\n        if self.notify_socket != None and index == 0 \\\\\\n          and self.rtp_proxy_client.tnot_supported:\\n            command += ' %s %s' % (self.notify_socket, self.notify_tag)\\n        self.rtp_proxy_client.send_command(command, self.update_result, (result_callback, 'callee', callback_parameters))\\n\\n    def update_result(self, result, args):\\n        result_callback, face, callback_parameters = args\\n        if face == 'caller':\\n            self.caller_session_exists = True\\n        else:\\n            self.callee_session_exists = True\\n        if result == None:\\n            result_callback(None, *callback_parameters)\\n            return\\n        t1 = result.split()\\n        rtpproxy_port = int(t1[0])\\n        if rtpproxy_port == 0:\\n            result_callback(None, *callback_parameters)\\n        family = 'IP4'\\n        if len(t1) > 1:\\n            rtpproxy_address = t1[1]\\n            if len(t1) > 2 and t1[2] == '6':\\n                family = 'IP6'\\n        else:\\n            rtpproxy_address = self.rtp_proxy_client.proxy_address\\n        result_callback((rtpproxy_address, rtpproxy_port, family), *callback_parameters)\\n\\n    def delete(self):\\n        while self.max_index >= 0:\\n            command = 'D %s %s %s' % ('%s-%d' % (self.call_id, self.max_index), self.from_tag, self.to_tag)\\n            self.rtp_proxy_client.send_command(command)\\n            self.max_index -= 1\\n\\n    def on_caller_sdp_change(self, sdp_body, result_callback):\\n        self.on_xxx_sdp_change(self.update_caller, sdp_body, result_callback)\\n\\n    def on_callee_sdp_change(self, sdp_body, result_callback):\\n        self.on_xxx_sdp_change(self.update_callee, sdp_body, result_callback)\\n\\n    def on_xxx_sdp_change(self, update_xxx, sdp_body, result_callback):\\n        sects = []\\n        try:\\n            sdp_body.parse()\\n        except Exception, exception:\\n            print datetime.now(), 'can\\\\'t parse SDP body: %s:' % str(exception)\\n            print '-' * 70\\n            print_exc(file = sys.stdout)\\n            print '-' * 70\\n            print sdp_body.content\\n            print '-' * 70\\n            sys.stdout.flush()\\n            return\\n        for i in range(0, len(sdp_body.content.sections)):\\n            sect = sdp_body.content.sections[i]\\n            if sect.m_header.transport.lower() not in ('udp', 'udptl', 'rtp/avp'):\\n                continue\\n            sects.append(sect)\\n        if len(sects) == 0:\\n            sdp_body.needs_update = False\\n            result_callback(sdp_body)\\n            return\\n        formats = sects[0].m_header.formats\\n        if update_xxx == self.update_caller:\\n            if len(formats) > 1:\\n                self.caller_codecs = reduce(lambda x, y: str(x) + ',' + str(y), formats)\\n            else:\\n                self.caller_codecs = str(formats[0])\\n        else:\\n            if len(formats) > 1:\\n                self.callee_codecs = reduce(lambda x, y: str(x) + ',' + str(y), formats)\\n            else:\\n                self.callee_codecs = str(formats[0])\\n        for sect in sects:\\n            options = ''\\n            if sect.c_header.atype == 'IP6':\\n                options = '6'\\n            update_xxx(sect.c_header.addr, sect.m_header.port, self.xxx_sdp_change_finish, options, \\\\\\n              sects.index(sect), sdp_body, sect, sects, result_callback)\\n        return\\n\\n    def xxx_sdp_change_finish(self, address_port, sdp_body, sect, sects, result_callback):\\n        sect.needs_update = False\\n        if address_port != None:\\n            sect.c_header.atype = address_port[2]\\n            sect.c_header.addr = address_port[0]\\n            if sect.m_header.port != 0:\\n                sect.m_header.port = address_port[1]\\n        if len([x for x in sects if x.needs_update]) == 0:\\n            sdp_body.content.o_header = self.origin\\n            sdp_body.needs_update = False\\n            result_callback(sdp_body)\\n\\n    def __del__(self):\\n        self.delete()\\n        self.rtp_proxy_client = None\"}, {\"identifier\":\"SipTransactionManager\", \"path\":\"sippy/SipTransactionManager.py\", \"snippet\":\"class SipTransactionManager(object):\\n    global_config = None\\n    l4r = None\\n    tclient = None\\n    tserver = None\\n    req_cb = None\\n    l1rcache = None\\n    l2rcache = None\\n    nat_traversal = False\\n    req_consumers = None\\n    provisional_retr = 0\\n\\n    def __init__(self, global_config, req_cb = None):\\n        self.global_config = global_config\\n        self.l4r = local4remote(global_config, self.handleIncoming)\\n        self.tclient = {}\\n        self.tserver = {}\\n        self.req_cb = req_cb\\n        self.l1rcache = {}\\n        self.l2rcache = {}\\n        self.req_consumers = {}\\n        Timeout(self.rCachePurge, 32, -1)\\n\\n    def handleIncoming(self, data, address, server):\\n        if len(data) < 32:\\n            return\\n        rtime = time()\\n        self.global_config['_sip_logger'].write('RECEIVED message from %s:%d:\\\\n' % address, data, ltime = rtime)\\n        checksum = md5(data).digest()\\n        retrans = self.l1rcache.get(checksum, None)\\n        if retrans == None:\\n            retrans = self.l2rcache.get(checksum, None)\\n        if retrans != None:\\n            userv, data, address = retrans\\n            if data == None:\\n                return\\n            self.transmitData(userv, data, address)\\n            return\\n        if data.startswith('SIP/2.0 '):\\n            try:\\n                resp = SipResponse(data)\\n                tid = resp.getTId(True, True)\\n            except Exception, exception:\\n                print datetime.now(), 'can\\\\'t parse SIP response from %s:%d: %s:' % (address[0], address[1], str(exception))\\n                print '-' * 70\\n                print_exc(file = sys.stdout)\\n                print '-' * 70\\n                print data\\n                print '-' * 70\\n                sys.stdout.flush()\\n                self.l1rcache[checksum] = (None, None, None)\\n                return\\n            if resp.getSCode()[0] < 100 or resp.getSCode()[0] > 999:\\n                print datetime.now(), 'invalid status code in SIP response from %s:%d:' % address\\n                print data\\n                sys.stdout.flush()\\n                self.l1rcache[checksum] = (None, None, None)\\n                return\\n            resp.rtime = rtime\\n            if not self.tclient.has_key(tid):\\n                #print 'no transaction with tid of %s in progress' % str(tid)\\n                self.l1rcache[checksum] = (None, None, None)\\n                return\\n            t = self.tclient[tid]\\n            if self.nat_traversal and resp.countHFs('contact') > 0 and not check1918(t.address[0]):\\n                curl = resp.getHFBody('contact').getUrl()\\n                if check1918(curl.host):\\n                    curl.host, curl.port = address\\n            resp.setSource(address)\\n            self.incomingResponse(resp, t, checksum)\\n        else:\\n            if self.req_cb == None:\\n                return\\n            try:\\n                req = SipRequest(data)\\n                tids = req.getTIds()\\n            except Exception, exception:\\n                print datetime.now(), 'can\\\\'t parse SIP request from %s:%d: %s:' % (address[0], address[1], str(exception))\\n                print '-' * 70\\n                print_exc(file = sys.stdout)\\n                print '-' * 70\\n                print data\\n                print '-' * 70\\n                sys.stdout.flush()\\n                self.l1rcache[checksum] = (None, None, None)\\n                return\\n            req.rtime = rtime\\n            via0 = req.getHFBody('via')\\n            ahost, aport = via0.getAddr()\\n            rhost, rport = address\\n            if self.nat_traversal and rport != aport and check1918(ahost):\\n                req.nated = True\\n            if ahost != rhost:\\n                via0.params['received'] = rhost\\n            if via0.params.has_key('rport') or req.nated:\\n                via0.params['rport'] = str(rport)\\n            if self.nat_traversal and req.countHFs('contact') > 0 and req.countHFs('via') == 1:\\n                curl = req.getHFBody('contact').getUrl()\\n                if check1918(curl.host):\\n                    curl.host, curl.port = address\\n                    req.nated = True\\n            req.setSource(address)\\n            self.incomingRequest(req, checksum, tids, server)\\n\\n    # 1. Client transaction methods\\n    def newTransaction(self, msg, resp_cb = None, laddress = None, userv = None):\\n        t = SipTransaction()\\n        t.tid = msg.getTId(True, True)\\n        if self.tclient.has_key(t.tid):\\n            raise ValueError('BUG: Attempt to initiate transaction with the same TID as existing one!!!')\\n        t.tout = 0.5\\n        t.fcode = None\\n        t.address = msg.getTarget()\\n        if userv == None:\\n            if laddress == None:\\n                t.userv = self.l4r.getServer(t.address)\\n            else:\\n                t.userv = self.l4r.getServer(laddress, is_local = True)\\n        else:\\n            t.userv = userv\\n        t.data = msg.localStr(t.userv.laddress[0], t.userv.laddress[1])\\n        try:\\n            t.expires = msg.getHFBody('expires').getNum()\\n            if t.expires <= 0:\\n                t.expires = 300\\n        except IndexError:\\n            t.expires = 300\\n        if msg.getMethod() == 'INVITE':\\n            t.needack = True\\n            t.ack = msg.genACK()\\n            t.cancel = msg.genCANCEL()\\n        else:\\n            t.needack = False\\n            t.ack = None\\n            t.cancel = None\\n        t.cancelPending = False\\n        t.resp_cb = resp_cb\\n        t.teA = Timeout(self.timerA, t.tout, 1, t)\\n        if resp_cb != None:\\n            t.r408 = msg.genResponse(408, 'Request Timeout')\\n        t.teB = Timeout(self.timerB, 32.0, 1, t)\\n        t.teC = None\\n        t.state = TRYING\\n        self.tclient[t.tid] = t\\n        self.transmitData(t.userv, t.data, t.address)\\n        return t\\n\\n    def cancelTransaction(self, t, reason = None):\\n        # If we got at least one provisional reply then (state == RINGING)\\n        # then start CANCEL transaction, otherwise deffer it\\n        if t.state != RINGING:\\n            t.cancelPending = True\\n        else:\\n            if reason != None:\\n                t.cancel.appendHeader(SipHeader(body = reason))\\n            self.newTransaction(t.cancel, userv = t.userv)\\n\\n    def incomingResponse(self, msg, t, checksum):\\n        # In those two states upper level already notified, only do ACK retransmit\\n        # if needed\\n        if t.state == TERMINATED:\\n            return\\n\\n        if t.state == TRYING:\\n            # Stop timers\\n            if t.teA != None:\\n                t.teA.cancel()\\n                t.teA = None\\n\\n        if t.state in (TRYING, RINGING):\\n            if t.teB != None:\\n                t.teB.cancel()\\n                t.teB = None\\n\\n            if msg.getSCode()[0] < 200:\\n                # Privisional response - leave everything as is, except that\\n                # change state and reload timeout timer\\n                if t.state == TRYING:\\n                    t.state = RINGING\\n                    if t.cancelPending:\\n                        self.newTransaction(t.cancel, userv = t.userv)\\n                        t.cancelPending = False\\n                t.teB = Timeout(self.timerB, t.expires, 1, t)\\n                self.l1rcache[checksum] = (None, None, None)\\n                if t.resp_cb != None:\\n                    t.resp_cb(msg)\\n            else:\\n                # Final response - notify upper layer and remove transaction\\n                if t.needack:\\n                    # Prepare and send ACK if necessary\\n                    fcode = msg.getSCode()[0]\\n                    tag = msg.getHFBody('to').getTag()\\n                    if tag != None:\\n                        t.ack.getHFBody('to').setTag(tag)\\n                    rAddr = None\\n                    if msg.getSCode()[0] >= 200 and msg.getSCode()[0] < 300:\\n                        # Some hairy code ahead\\n                        if msg.countHFs('contact') > 0:\\n                            rTarget = msg.getHFBody('contact').getUrl().getCopy()\\n                        else:\\n                            rTarget = None\\n                        routes = [x.getCopy() for x in msg.getHFBodys('record-route')]\\n                        routes.reverse()\\n                        if len(routes) > 0:\\n                            if not routes[0].getUrl().lr:\\n                                if rTarget != None:\\n                                    routes.append(SipRoute(address = SipAddress(url = rTarget)))\\n                                rTarget = routes.pop(0).getUrl()\\n                                rAddr = rTarget.getAddr()\\n                            else:\\n                                rAddr = routes[0].getAddr()\\n                        elif rTarget != None:\\n                            rAddr = rTarget.getAddr()\\n                        if rTarget != None:\\n                            t.ack.setRURI(rTarget)\\n                        if rAddr != None:\\n                            t.ack.setTarget(rAddr)\\n                        t.ack.delHFs('route')\\n                        t.ack.appendHeaders([SipHeader(name = 'route', body = x) for x in routes])\\n                    if fcode >= 200 and fcode < 300:\\n                        t.ack.getHFBody('via').genBranch()\\n                    if rAddr == None:\\n                        rAddr = t.address\\n                    self.transmitMsg(t.userv, t.ack, rAddr, checksum)\\n                else:\\n                    self.l1rcache[checksum] = (None, None, None)\\n                if t.resp_cb != None:\\n                    t.resp_cb(msg)\\n                del self.tclient[t.tid]\\n                t.cleanup()\\n\\n    def timerA(self, t):\\n        #print 'timerA', t\\n        self.transmitData(t.userv, t.data, t.address)\\n        t.tout *= 2\\n        t.teA = Timeout(self.timerA, t.tout, 1, t)\\n\\n    def timerB(self, t):\\n        #print 'timerB', t\\n        t.teB = None\\n        if t.teA != None:\\n            t.teA.cancel()\\n            t.teA = None\\n        t.state = TERMINATED\\n        #print '2: Timeout(self.timerC, 32.0, 1, t)', t\\n        t.teC = Timeout(self.timerC, 32.0, 1, t)\\n        if t.resp_cb == None:\\n            return\\n        t.r408.rtime = time()\\n        t.resp_cb(t.r408)\\n        #try:\\n        #    t.resp_cb(SipRequest(t.data).genResponse(408, 'Request Timeout'))\\n        #except:\\n        #    print 'SipTransactionManager: unhandled exception when processing response!'\\n\\n    def timerC(self, t):\\n        #print 'timerC', t\\n        #print self.tclient\\n        t.teC = None\\n        del self.tclient[t.tid]\\n        t.cleanup()\\n\\n    # 2. Server transaction methods\\n    def incomingRequest(self, msg, checksum, tids, server):\\n        for tid in tids:\\n            if self.tclient.has_key(tid):\\n                resp = msg.genResponse(482, 'Loop Detected')\\n                self.transmitMsg(server, resp, resp.getHFBody('via').getTAddr(), checksum)\\n                return\\n        tid = msg.getTId()\\n        # Fasten seatbelts - bumpy transaction matching code ahead!\\n        if msg.getMethod() in ('INVITE', 'CANCEL', 'ACK'):\\n            btid = msg.getTId(wBRN = True)\\n            t = self.tserver.get(btid, None)\\n            if t == None:\\n                t = self.tserver.get(tid, None)\\n                if t != None and t.branch != btid[3]:\\n                    if msg.getMethod() == 'INVITE':\\n                        # Different branch on transaction to which no final reply\\n                        # has been sent yet - merge requests\\n                        resp = msg.genResponse(482, 'Loop Detected')\\n                        self.transmitMsg(server, resp, resp.getHFBody('via').getTAddr(), checksum)\\n                        return\\n                    elif msg.getMethod() == 'CANCEL':\\n                        # CANCEL, but with branch that doesn't match any existing\\n                        # transactions\\n                        resp = msg.genResponse(481, 'Call Leg/Transaction Does Not Exist')\\n                        self.transmitMsg(server, resp, resp.getHFBody('via').getTAddr(), checksum)\\n                        return\\n        else:\\n            t = self.tserver.get(tid, None)\\n        if t != None:\\n            #print 'existing transaction'\\n            if msg.getMethod() == t.method:\\n                # Duplicate received, check that we have sent any response on this\\n                # request already\\n                if t.data != None:\\n                    self.transmitData(t.userv, t.data, t.address, checksum)\\n                return\\n            elif msg.getMethod() == 'CANCEL':\\n                # RFC3261 says that we have to reply 200 OK in all cases if\\n                # there is such transaction\\n                resp = msg.genResponse(200, 'OK')\\n                self.transmitMsg(t.userv, resp, resp.getHFBody('via').getTAddr(), checksum)\\n                if t.state in (TRYING, RINGING):\\n                    self.doCancel(t, msg.rtime, msg)\\n            elif msg.getMethod() == 'ACK' and t.state == COMPLETED:\\n                t.state = CONFIRMED\\n                if t.teA != None:\\n                    t.teA.cancel()\\n                    t.teA = None\\n                t.teD.cancel()\\n                # We have done with the transaction, no need to wait for timeout\\n                del self.tserver[t.tid]\\n                t.cleanup()\\n                self.l1rcache[checksum] = (None, None, None)\\n        elif msg.getMethod() == 'ACK':\\n            # Some ACK that doesn't match any existing transaction.\\n            # Drop and forget it - upper layer is unlikely to be interested\\n            # to seeing this anyway.\\n            print datetime.now(), 'unmatched ACK transaction - ignoring'\\n            sys.stdout.flush()\\n            self.l1rcache[checksum] = (None, None, None)\\n        elif msg.getMethod() == 'CANCEL':\\n            resp = msg.genResponse(481, 'Call Leg/Transaction Does Not Exist')\\n            self.transmitMsg(server, resp, resp.getHFBody('via').getTAddr(), checksum)\\n        else:\\n            #print 'new transaction', msg.getMethod()\\n            t = SipTransaction()\\n            t.tid = tid\\n            t.state = TRYING\\n            t.teA = None\\n            t.teD = None\\n            t.teE = None\\n            t.teF = None\\n            t.method = msg.getMethod()\\n            t.data = None\\n            t.address = None\\n            t.noack_cb = None\\n            t.cancel_cb = None\\n            t.checksum = checksum\\n            if server.laddress[0] not in ('0.0.0.0', '[::]'):\\n                t.userv = server\\n            else:\\n                # For messages received on the wildcard interface find\\n                # or create more specific server.\\n                t.userv = self.l4r.getServer(msg.getSource())\\n            if msg.getMethod() == 'INVITE':\\n                t.r487 = msg.genResponse(487, 'Request Terminated')\\n                t.needack = True\\n                t.branch = msg.getHFBody('via').getBranch()\\n                try:\\n                    e = msg.getHFBody('expires').getNum()\\n                    if e <= 0:\\n                        e = 300\\n                except IndexError:\\n                    e = 300\\n                t.teE = Timeout(self.timerE, e, 1, t)\\n            else:\\n                t.r487 = None\\n                t.needack = False\\n                t.branch = None\\n            self.tserver[t.tid] = t\\n            for consumer in self.req_consumers.get(t.tid[0], ()):\\n                consumer = consumer.isYours(msg)\\n                if consumer != None:\\n                    rval = consumer.recvRequest(msg)\\n                    break\\n            else:\\n                rval = self.req_cb(msg)\\n            if rval == None:\\n                if t.teA != None or t.teD != None or t.teE != None or t.teF != None:\\n                    return\\n                if self.tserver.has_key(t.tid):\\n                    del self.tserver[t.tid]\\n                t.cleanup()\\n                return\\n            resp, cancel_cb, noack_cb = rval\\n            t.cancel_cb = cancel_cb\\n            t.noack_cb = noack_cb\\n            if resp != None:\\n                self.sendResponse(resp, t)\\n\\n    def regConsumer(self, consumer, call_id):\\n        self.req_consumers.setdefault(call_id, []).append(consumer)\\n\\n    def unregConsumer(self, consumer, call_id):\\n        # Usually there will be only one consumer per call_id, so that\\n        # optimize management for this case\\n        consumers = self.req_consumers.pop(call_id)\\n        if len(consumers) > 1:\\n            consumers.remove(consumer)\\n            self.req_consumers[call_id] = consumers\\n\\n    def sendResponse(self, resp, t = None, retrans = False):\\n        #print self.tserver\\n        if t == None:\\n            tid = resp.getTId()\\n            t = self.tserver[tid]\\n        if t.state not in (TRYING, RINGING) and not retrans:\\n            raise ValueError('BUG: attempt to send reply on already finished transaction!!!')\\n        scode = resp.getSCode()[0]\\n        toHF = resp.getHFBody('to')\\n        if scode > 100 and toHF.getTag() == None:\\n            toHF.genTag()\\n        t.data = resp.localStr(t.userv.laddress[0], t.userv.laddress[1])\\n        t.address = resp.getHFBody('via').getTAddr()\\n        self.transmitData(t.userv, t.data, t.address, t.checksum)\\n        if scode < 200:\\n            t.state = RINGING\\n            if self.provisional_retr > 0 and scode > 100:\\n                if t.teF != None:\\n                    t.teF.cancel()\\n                t.teF = Timeout(self.timerF, self.provisional_retr, 1, t)\\n        else:\\n            t.state = COMPLETED\\n            if t.teE != None:\\n                t.teE.cancel()\\n                t.teE = None\\n            if t.teF != None:\\n                t.teF.cancel()\\n                t.teF = None\\n            if t.needack:\\n                # Schedule removal of the transaction\\n                t.teD = Timeout(self.timerD, 32.0, 1, t)\\n                if scode >= 300:\\n                    # Black magick to allow proxy send us another INVITE with diffetent branch\\n                    del self.tserver[t.tid]\\n                    t.tid = list(t.tid)\\n                    t.tid.append(t.branch)\\n                    t.tid = tuple(t.tid)\\n                    self.tserver[t.tid] = t\\n                # Install retransmit timer if necessary\\n                t.tout = 0.5\\n                t.teA = Timeout(self.timerA, t.tout, 1, t)\\n            else:\\n                # We have done with the transaction\\n                del self.tserver[t.tid]\\n                t.cleanup()\\n\\n    def doCancel(self, t, rtime = None, req = None):\\n        if rtime == None:\\n            rtime = time()\\n        if t.r487 != None:\\n            self.sendResponse(t.r487, t, True)\\n        if t.cancel_cb != None:\\n            t.cancel_cb(rtime, req)\\n\\n    def timerD(self, t):\\n        #print 'timerD'\\n        t.teD = None\\n        if t.teA != None:\\n            t.teA.cancel()\\n            t.teA = None\\n        if t.noack_cb != None and t.state != CONFIRMED:\\n            t.noack_cb()\\n        del self.tserver[t.tid]\\n        t.cleanup()\\n\\n    def timerE(self, t):\\n        #print 'timerE'\\n        t.teE = None\\n        if t.teF != None:\\n            t.teF.cancel()\\n            t.teF = None\\n        if t.state in (TRYING, RINGING):\\n            if t.r487 != None:\\n                t.r487.reason = 'Request Expired'\\n            self.doCancel(t)\\n\\n    # Timer to retransmit the last provisional reply every\\n    # 2 seconds\\n    def timerF(self, t):\\n        #print 'timerF', t.state\\n        t.teF = None\\n        if t.state == RINGING and self.provisional_retr > 0:\\n            self.transmitData(t.userv, t.data, t.address)\\n            t.teF = Timeout(self.timerF, self.provisional_retr, 1, t)\\n\\n    def rCachePurge(self):\\n        self.l2rcache = self.l1rcache\\n        self.l1rcache = {}\\n        self.l4r.rotateCache()\\n\\n    def transmitMsg(self, userv, msg, address, cachesum, compact = False):\\n        data = msg.localStr(userv.laddress[0], userv.laddress[1], compact)\\n        self.transmitData(userv, data, address, cachesum)\\n\\n    def transmitData(self, userv, data, address, cachesum = None):\\n        userv.send_to(data, address)\\n        self.global_config['_sip_logger'].write('SENDING message to %s:%d:\\\\n' % address, data)\\n        if cachesum != None:\\n            self.l1rcache[cachesum] = (userv, data, address)\"}, {\"identifier\":\"MyConfigParser\", \"path\":\"sippy/MyConfigParser.py\", \"snippet\":\"class MyConfigParser(RawConfigParser):\\n    default_section = None\\n    _private_keys = None\\n\\n    def __init__(self, default_section = 'general'):\\n        self.default_section = default_section\\n        self._private_keys = {}\\n        RawConfigParser.__init__(self)\\n        self.add_section(self.default_section)\\n\\n    def __getitem__(self, key):\\n        if key.startswith('_'):\\n            return self._private_keys[key]\\n        value_type  = SUPPORTED_OPTIONS[key][0]\\n        if value_type  == 'B':\\n            return self.getboolean(self.default_section, key)\\n        elif value_type == 'I':\\n            return self.getint(self.default_section, key)\\n        return self.get(self.default_section, key)\\n\\n    def __setitem__(self, key, value):\\n        if key.startswith('_'):\\n            self._private_keys[key] = value\\n        else:\\n            self.set(self.default_section, key, str(value))\\n        return\\n\\n    def has_key(self, key):\\n        return self.__contains__(key)\\n\\n    def __contains__(self, key):\\n        if key.startswith('_'):\\n            return self._private_keys.has_key(key)\\n        return self.has_option(self.default_section, key)\\n\\n    def get(self, *args):\\n        if len(args) == 1:\\n            return self.__getitem__(args[0])\\n        return RawConfigParser.get(self, *args)\\n\\n    def getdefault(self, key, default_value):\\n        if self.__contains__(key):\\n            return self.__getitem__(key)\\n        return default_value\\n\\n    def get_longopts(self):\\n        return tuple([x + '=' for x in SUPPORTED_OPTIONS.keys()])\\n\\n    def read(self, fname):\\n        RawConfigParser.readfp(self, open(fname))\\n        for key in tuple(self.options(self.default_section)):\\n            self.check_and_set(key, RawConfigParser.get(self, \\\\\\n              self.default_section, key), False)\\n\\n    def check_and_set(self, key, value, compat = True):\\n        value = value.strip()\\n        if compat:\\n            if key == 'rtp_proxy_client':\\n                # XXX compatibility option\\n                if self.has_key('_rtp_proxy_clients'):\\n                    self['_rtp_proxy_clients'].append(value)\\n                else:\\n                    self['_rtp_proxy_clients'] = [value,]\\n                if self.has_key('rtp_proxy_clients'):\\n                    self['rtp_proxy_clients'] += ',' + value\\n                else:\\n                    self['rtp_proxy_clients'] = value\\n                return\\n            elif key == 'pass_header':\\n                # XXX compatibility option\\n                if self.has_key('_pass_headers'):\\n                    self['_pass_headers'].append(value)\\n                else:\\n                    self['_pass_headers'] = [value,]\\n                if self.has_key('pass_headers'):\\n                    self['pass_headers'] += ',' + value\\n                else:\\n                    self['pass_headers'] = value\\n                return\\n\\n        value_type  = SUPPORTED_OPTIONS[key][0]\\n        if value_type == 'B':\\n            if value.lower() not in self._boolean_states:\\n                raise ValueError, 'Not a boolean: %s' % value\\n        elif value_type == 'I':\\n            _value = int(value)\\n        if key in ('keepalive_ans', 'keepalive_orig'):\\n            if _value < 0:\\n                raise ValueError, 'keepalive_ans should be non-negative'\\n        elif key == 'max_credit_time':\\n            if _value <= 0:\\n                raise ValueError, 'max_credit_time should be more than zero'\\n        elif key == 'allowed_pts':\\n            self['_allowed_pts'] = [int(x) for x in value.split(',')]\\n        elif key in ('accept_ips', 'pass_headers', 'rtp_proxy_clients'):\\n            self['_' + key] = [x.strip() for x in value.split(',')]\\n        elif key == 'sip_address':\\n            if 'my' in dir(value):\\n                self['_sip_address'] = value\\n                value = '*'\\n            elif value in ('*', '0.0.0.0', '::'):\\n                self['_sip_address'] = SipConf.my_address\\n            else:\\n                self['_sip_address'] = value\\n        elif key == 'sip_port':\\n            if _value <= 0 or _value > 65535:\\n                raise ValueError, 'sip_port should be in the range 1-65535'\\n            self['_sip_port'] = _value\\n        self[key] = value\\n\\n    def options_help(self):\\n        supported_options = SUPPORTED_OPTIONS.items()\\n        supported_options.sort()\\n        for option, (value_type, helptext) in supported_options:\\n            if value_type == 'B':\\n                value = 'on/off'\\n            elif value_type == 'I':\\n                value = 'number'\\n            else:\\n                value = '\\\"string\\\"'\\n            print '--%s=%s\\\\n\\\\t%s\\\\n' % (option, value, helptext)\"}]", "import_statement": "from sippy.Timeout import Timeout\nfrom sippy.Signal import Signal\nfrom sippy.SipFrom import SipFrom\nfrom sippy.SipTo import SipTo\nfrom sippy.SipCiscoGUID import SipCiscoGUID\nfrom sippy.UA import UA\nfrom sippy.CCEvents import CCEventRing, CCEventConnect, CCEventDisconnect, CCEventTry, CCEventUpdate, CCEventFail\nfrom sippy.UasStateTrying import UasStateTrying\nfrom sippy.UasStateRinging import UasStateRinging\nfrom sippy.UaStateDead import UaStateDead\nfrom sippy.SipConf import SipConf\nfrom sippy.SipHeader import SipHeader\nfrom sippy.RadiusAuthorisation import RadiusAuthorisation\nfrom sippy.RadiusAccounting import RadiusAccounting\nfrom sippy.FakeAccounting import FakeAccounting\nfrom sippy.SipLogger import SipLogger\nfrom sippy.Rtp_proxy_session import Rtp_proxy_session\nfrom sippy.Rtp_proxy_client import Rtp_proxy_client\nfrom signal import SIGHUP, SIGPROF, SIGUSR1, SIGUSR2\nfrom twisted.internet import reactor\nfrom urllib import unquote\nfrom sippy.Cli_server_local import Cli_server_local\nfrom sippy.SipTransactionManager import SipTransactionManager\nfrom sippy.SipCallId import SipCallId\nfrom re import sub\nfrom time import time\nfrom urllib import quote\nfrom hashlib import md5\nfrom sippy.MyConfigParser import MyConfigParser\nimport gc, getopt, os, sys", "code": "        if req.getMethod() in ('NOTIFY', 'PING'):\n            # Whynot?\n            return (req.genResponse(200, 'OK'), None, None)\n        return (req.genResponse(501, 'Not Implemented'), None, None)\n\n    def discAll(self, signum = None):\n        if signum != None:\n            print 'Signal %d received, disconnecting all calls' % signum\n        for cc in tuple(self.ccmap):\n            cc.disconnect()\n\n    def toggleDebug(self, signum):\n        if self.debug_mode:\n            print 'Signal %d received, toggling extra debug output off' % signum\n        else:\n            print 'Signal %d received, toggling extra debug output on' % signum\n        self.debug_mode = not self.debug_mode\n\n    def safeRestart(self, signum):\n        print 'Signal %d received, scheduling safe restart' % signum\n        self.safe_restart = True\n\n    def GClector(self):\n        print 'GC is invoked, %d calls in map' % len(self.ccmap)\n        if self.debug_mode:\n            print self.global_config['_sip_tm'].tclient, self.global_config['_sip_tm'].tserver\n            for cc in tuple(self.ccmap):\n                try:\n                    print cc.uaA.state, cc.uaO.state\n                except AttributeError:\n                    print None\n        else:\n            print '%d client, %d server transactions in memory' % \\\n              (len(self.global_config['_sip_tm'].tclient), len(self.global_config['_sip_tm'].tserver))\n        if self.safe_restart:\n            if len(self.ccmap) == 0:\n                self.global_config['_sip_tm'].userv.close()\n                os.chdir(self.global_config['_orig_cwd'])\n                argv = [sys.executable,]\n                argv.extend(self.global_config['_orig_argv'])\n                os.execv(sys.executable, argv)\n                # Should not reach this point!\n            self.el.ival = 1\n        #print gc.collect()\n        if len(gc.garbage) > 0:\n            print gc.garbage\n\n    def recvCommand(self, clim, cmd):\n        args = cmd.split()\n        cmd = args.pop(0).lower()\n        if cmd == 'q':\n            clim.close()\n            return False\n        if cmd == 'l':\n            res = 'In-memory calls:\\n'\n            total = 0\n            for cc in self.ccmap:\n                res += '%s: %s (' % (cc.cId, cc.state.sname)\n                if cc.uaA != None:\n                    res += '%s %s:%d %s %s -> ' % (cc.uaA.state, cc.uaA.getRAddr0()[0], \\\n                      cc.uaA.getRAddr0()[1], cc.uaA.getCLD(), cc.uaA.getCLI())\n                else:\n                    res += 'N/A -> '\n                if cc.uaO != None:\n                    res += '%s %s:%d %s %s)\\n' % (cc.uaO.state, cc.uaO.getRAddr0()[0], \\\n                      cc.uaO.getRAddr0()[1], cc.uaO.getCLI(), cc.uaO.getCLD())\n                else:\n                    res += 'N/A)\\n'\n                total += 1\n            res += 'Total: %d\\n' % total\n            clim.send(res)\n            return False\n        if cmd == 'd':\n            if len(args) != 1:\n                clim.send('ERROR: syntax error: d <call-id>\\n')\n                return False\n            if args[0] == '*':\n                self.discAll()\n                clim.send('OK\\n')\n                return False\n            dlist = [x for x in self.ccmap if str(x.cId) == args[0]]\n            if len(dlist) == 0:\n                clim.send('ERROR: no call with id of %s has been found\\n' % args[0])\n                return False\n            for cc in dlist:\n                cc.disconnect()\n            clim.send('OK\\n')\n            return False\n        if cmd == 'r':\n            if len(args) != 1:\n                clim.send('ERROR: syntax error: r [<id>]\\n')\n                return False\n            idx = int(args[0])\n            dlist = [x for x in self.ccmap if x.id == idx]\n            if len(dlist) == 0:\n                clim.send('ERROR: no call with id of %d has been found\\n' % idx)\n                return False\n            for cc in dlist:\n                if cc.state == CCStateConnected and cc.proxied:\n                    cc.disconnect(time() - 60)\n            clim.send('OK\\n')\n            return False\n        clim.send('ERROR: unknown command\\n')\n        return False\n\ndef reopen(signum, logfile):\n    print 'Signal %d received, reopening logs' % signum\n    fd = os.open(logfile, os.O_WRONLY | os.O_CREAT | os.O_APPEND)\n    os.dup2(fd, sys.__stdout__.fileno())\n    os.dup2(fd, sys.__stderr__.fileno())\n    os.close(fd)\n\ndef usage(global_config, brief = False):\n    print 'usage: b2bua.py [--option1=value1] [--option2=value2] ... [--optionN==valueN]'\n    if not brief:\n        print '\\navailable options:\\n'\n        global_config.options_help()\n    sys.exit(1)\n\nif __name__ == '__main__':\n", "next_line": "    global_config = MyConfigParser()", "gold_snippet_index": 20, "id": 4, "__internal_uuid__": "87fa929a-f5f1-4769-a25b-c7e39d42cd4a"}
{"repo_name": "mitshell/libmich", "file_path": "libmich/formats/IEEE80211.py", "context": "[{\"identifier\":\"Bit\", \"path\":\"libmich/core/element.py\", \"snippet\":\"class Bit(Element):\\n    '''\\n    class defining a standard element, managed like a bit (e.g. a flag)\\n    or bit-stream of variable bit length\\n    Values are corresponding to unsigned integer: from 0 to pow(2, bit_len)-1.\\n    It does not require to be byte-aligned.\\n    \\n    attributes:\\n    Pt: to point to another object or direct integer value;\\n    PtFunc: when defined, PtFunc(Pt) is used to generate the integer value;\\n    Val: when defined, overwrites the Pt (and PtFunc) integer value, \\n         used when mapping string to the element;\\n    BitLen: length in bits of the bit stream;\\n    BitLenFunc: to be used when mapping string with variable bit-length, \\n                BitLenFunc(BitLen) is used;\\n    Dict: dictionnary to use for a look-up when representing \\n          the element into python;\\n    Repr: representation style, binary, hexa or human: human uses Dict;\\n    Trans: to define transparent element which has empty str() and len() to 0,\\n           it \\\"nullifies\\\" its existence; can point to something for automation;\\n    TransFunc: when defined, TransFunc(Trans) is used to automate the \\n               transparency aspect: used e.g. for conditional element;\\n    '''\\n    # for object representation\\n    _reprs = ['hex', 'bin', 'hum']\\n    \\n    def __init__(self, CallName='', ReprName=None, \\n                 Pt=None, PtFunc=None, Val=None, \\n                 BitLen=1, BitLenFunc=None,\\n                 Dict=None, DictFunc=None, Repr='bin', \\n                 Trans=False, TransFunc=None):\\n        if CallName or not self.CallName:\\n            self.CallName = CallName\\n        if ReprName is None: \\n            self.ReprName = ''\\n        else: \\n            self.ReprName = ReprName\\n        self.Pt = Pt\\n        self.PtFunc = PtFunc\\n        self.Val = Val\\n        self.BitLen = BitLen\\n        self.BitLenFunc = BitLenFunc\\n        self.Dict = Dict\\n        self.DictFunc = DictFunc\\n        self.Repr = Repr\\n        self.Trans = Trans\\n        self.TransFunc = TransFunc\\n    \\n    def __setattr__(self, attr, val):\\n        # ensures no bullshit is provided into element's attributes \\n        # (however, it is not a complete test...)\\n        if self.safe:\\n            if attr == 'CallName':\\n                if type(val) is not str or len(val) == 0:\\n                    raise AttributeError('CallName must be a non-null string')\\n            elif attr == 'ReprName':\\n                if type(val) is not str:\\n                    raise AttributeError('ReprName must be a string')\\n            elif attr == 'PtFunc':\\n                if val is not None and not isinstance(val, type_funcs) :\\n                    raise AttributeError('PtFunc must be a function')\\n            elif attr == 'Val':\\n                if val is not None and not isinstance(val, (int, long)):\\n                    raise AttributeError('Val must be an int')\\n            elif attr == 'BitLenFunc':\\n                if val is not None and not isinstance(val, type_funcs) :\\n                    raise AttributeError('BitLenFunc must be a function')\\n            elif attr == 'DictFunc':\\n                if val is not None and not isinstance(val, type_funcs) :\\n                    raise AttributeError('DictFunc must be a function')\\n            elif attr == 'Repr':\\n                if val not in self._reprs:\\n                    raise AttributeError('Repr %s does not exist, use in: %s' \\\\\\n                          % (val, self._reprs))\\n            elif attr == 'TransFunc':\\n                if val is not None and not isinstance(val, type_funcs) :\\n                    raise AttributeError('TransFunc must be a function')\\n        object.__setattr__(self, attr, val)\\n    \\n    def __call__(self):\\n        if self.Val is None and self.Pt is None: return 0\\n        if self.Val is not None: return self.__confine(self.Val) \\n        elif self.PtFunc is not None:\\n            if self.safe:\\n                assert( type(self.PtFunc(self.Pt)) is int )\\n            return self.__confine(self.PtFunc(self.Pt))\\n        else: return self.__confine(int(self.Pt))\\n    \\n    def __confine(self, value):\\n        # makes sure value provided does not overflow bit length\\n        return max( 0, min( pow(2, self.bit_len())-1, value ))\\n    \\n    def __str__(self):\\n        # return the string representation of the integer value\\n        # big-endian encoded\\n        # left-aligned according to the bit length\\n        # -> last bits of the last byte are nullified\\n        # \\n        # manages Element transparency\\n        if self.is_transparent():\\n            return ''\\n        # do it the dirty way:\\n        h = self.__hex__()\\n        if not h:\\n            return ''\\n        if len(h) % 2: h = ''.join(('0', h))\\n        return str(shtr(unhexlify(h)) << (8-(self.bit_len()%8))%8)\\n    \\n    def _shar__str__(self):\\n        bitlen = self.bit_len()\\n        if bitlen == 0:\\n            return ''\\n        ret = shar()\\n        ret.set_uint(self(), bitlen)\\n        return ret.to_buf()\\n    \\n    def __len__(self):\\n        # just for fun here, \\n        # but do not use this in program...\\n        bitlen = self.bit_len()\\n        if bitlen % 8:\\n            return (bitlen//8) + 1\\n        return bitlen//8\\n    \\n    def bit_len(self):\\n        # manages Element transparency\\n        if self.is_transparent():\\n            return 0\\n        # and standard bit length processing\\n        if self.BitLenFunc is not None:\\n            if self.safe:\\n                assert( type(self.BitLenFunc(self.BitLen)) is int )\\n            return self.BitLenFunc(self.BitLen)\\n        else:\\n            if self.safe:\\n                assert( type(self.BitLen) is int )\\n            return self.BitLen\\n    \\n    def map_len(self):\\n        bitlen = self.bit_len()\\n        if bitlen % 8:\\n            return (bitlen//8)+1\\n        return bitlen//8\\n    \\n    def __hex__(self):\\n        bitlen = self.bit_len()\\n        if not bitlen:\\n            return ''\\n        hexa = hex(self())[2:]\\n        if hexa[-1] == 'L':\\n            # thx to Python to add 'L' for long on hex repr...\\n            hexa = hexa[:-1]\\n        if self.bit_len()%4: \\n            return '0'*(self.bit_len()//4 + 1 - len(hexa)) + hexa\\n        else: \\n            return '0'*(self.bit_len()//4 - len(hexa)) + hexa\\n    \\n    def __int__(self):\\n        return self()\\n        \\n    def __bin__(self):\\n        bitlen = self.bit_len()\\n        if not bitlen:\\n            return ''\\n        binary = format(self(), 'b')\\n        return (bitlen - len(binary))*'0' + binary\\n        \\n    def __repr__(self):\\n        if self.Repr == 'hex': return '0x%s' % self.__hex__()\\n        elif self.Repr == 'bin': return '0b%s' % self.__bin__()\\n        elif self.Repr == 'hum':\\n            value = self()\\n            if self.DictFunc:\\n                if self.safe:\\n                    assert(hasattr(self.DictFunc(self.Dict), '__getitem__'))\\n                try: val = '%i : %s' % (value, self.DictFunc(self.Dict)[value])\\n                except KeyError: val = value\\n            elif self.Dict:\\n                try: val = '%i : %s' % (value, self.Dict[value])\\n                except KeyError: val = value\\n            else:\\n                val = value\\n            #return repr(val)\\n            rep = repr(val)\\n            if rep[-1] == 'L':\\n                rep = rep[:-1]\\n            return rep\\n    \\n    def getattr(self):\\n        return ['CallName', 'ReprName', 'Pt', 'PtFunc', 'Val', 'BitLen',\\n                'BitLenFunc', 'Dict', 'DictFunc', 'Repr', 'Trans', 'TransFunc']\\n    \\n    def showattr(self):\\n        for a in self.getattr():\\n            if a == 'Dict' and self.Dict is not None: \\n                print('%s : %s' % ( a, self.__getattribute__(a).__class__) )\\n            else: \\n                print('%s : %s' % ( a, repr(self.__getattribute__(a))) )\\n    \\n    # cloning an element, used in set of elements\\n    def clone(self):\\n        clone = self.__class__(\\n                 self.CallName, self.ReprName,\\n                 self.Pt, self.PtFunc,\\n                 self.Val, \\n                 self.BitLen, self.BitLenFunc,\\n                 self.Dict, self.DictFunc, self.Repr,\\n                 self.Trans, self.TransFunc )\\n        return clone\\n    \\n    def map(self, string=''):\\n        # map each bit of the string from left to right\\n        # using the shtr() class to shift the string\\n        # string must be ascii-encoded (see shtr)\\n        if not self.is_transparent():\\n            self.map_bit( shtr(string).left_val(self.bit_len()) )\\n    \\n    def map_bit(self, value=0):\\n        # map an int / long value\\n        if self.safe:\\n            assert( 0 <= value <= pow(2, self.bit_len()) )\\n        self.Val = value\\n    \\n    def map_ret(self, string=''):\\n        if self.is_transparent():\\n            return string\\n        else:\\n            shtring = shtr(string)\\n            bitlen = self.bit_len()\\n            self.map_bit( shtring.left_val(bitlen) )\\n            return shtring << bitlen\\n    \\n    # shar manipulation interface\\n    def to_shar(self):\\n        ret = shar()\\n        ret.set_uint(self(), self.bit_len())\\n        return ret\\n    \\n    def map_shar(self, sh):\\n        if not self.is_transparent():\\n            self.map_bit( sh.get_uint(self.bit_len()) )\"}, {\"identifier\":\"Str\", \"path\":\"libmich/core/element.py\", \"snippet\":\"class Str(Element):\\n    '''\\n    class defining a standard Element, \\n    managed like a stream of byte(s), or string.\\n    It is always byte-aligned (in term of length, at least)\\n    \\n    attributes:\\n    Pt: to point to another stream object (can simply be a string);\\n    PtFunc: when defined, PtFunc(Pt) is used \\n            to generate the str() / len() representation;\\n    Val: when defined, overwrites the Pt (and PtFunc) string value, \\n         used when mapping a string buffer to the element;\\n    Len: can be set to a fixed int value, or to another object\\n         when called by LenFunc\\n    LenFunc: to be used when mapping string buffer with variable length\\n             (e.g. in TLV object), LenFunc(Len) is used;\\n    Repr: python representation; binary, hexa, human or ipv4;\\n    Trans: to define transparent element which has empty str() and len() to 0,\\n           it \\\"nullifies\\\" its existence; can point to something for automation;\\n    TransFunc: when defined, TransFunc(Trans) is used to automate the \\n               transparency aspect: used e.g. for conditional element;\\n    '''\\n    \\n    # this is used when printing the object representation\\n    _repr_limit = 1024\\n    _reprs = ['hex', 'bin', 'hum', 'ipv4']\\n    \\n    # padding is used when .Pt and .Val are None, \\n    # but Str instance has still a defined .Len attribute\\n    _padding_byte = '\\\\0'\\n    \\n    def __init__(self, CallName='', ReprName=None, \\n                 Pt=None, PtFunc=None, Val=None, \\n                 Len=None, LenFunc=None,\\n                 Repr=\\\"hum\\\",\\n                 Trans=False, TransFunc=None):\\n        if CallName or not self.CallName:\\n            self.CallName = CallName\\n        if ReprName is None :\\n            self.ReprName = ''\\n        else :\\n            self.ReprName = ReprName\\n        self.Pt = Pt\\n        self.PtFunc = PtFunc\\n        self.Val = Val\\n        self.Len = Len\\n        self.LenFunc = LenFunc\\n        self.Type = 'stream'\\n        self.Repr = Repr\\n        self.Trans = Trans\\n        self.TransFunc = TransFunc\\n    \\n    def __setattr__(self, attr, val):\\n        # ensures no bullshit is provided into element's attributes \\n        # (however, it is not a exhaustive test...)\\n        # managed with the class \\\"safe\\\" trigger\\n        if self.safe :\\n            if attr == 'CallName' :\\n                if type(val) is not str or len(val) == 0 :\\n                    raise AttributeError('CallName must be a non-null string')\\n            elif attr == 'ReprName' :\\n                if type(val) is not str:\\n                    raise AttributeError('ReprName must be a string')\\n            elif attr == 'PtFunc' :\\n                if val is not None and not isinstance(val, type_funcs) :\\n                    raise AttributeError('PtFunc must be a function')\\n            elif attr == 'Val' :\\n                if val is not None and not isinstance(val, \\\\\\n                (str, bytes, Element, Layer, Block, tuple, list)) :\\n                    raise AttributeError('Val must be a string or something ' \\\\\\n                    'that makes a string at the end...')\\n            elif attr == 'Len' :\\n                if val is not None and not isinstance(val, \\\\\\n                (int, tuple, Element, type_funcs)) :\\n                    raise AttributeError('Len must be an int or element')\\n            elif attr == 'LenFunc' :\\n                if val is not None and not isinstance(val, type_funcs) :\\n                    raise AttributeError('LenFunc must be a function')\\n            elif attr == 'Repr' :\\n                if val not in self._reprs :\\n                    raise AttributeError('Repr %s does not exist, only: %s' \\\\\\n                          % (val, self._reprs))\\n            elif attr == 'TransFunc' :\\n                if val is not None and not isinstance(val, type_funcs) :\\n                    raise AttributeError('TransFunc must be a function')\\n        # this is for Layer() pointed by Pt attr in Str() object\\n        #if isinstance(self.Pt, Layer) and hasattr(self.Pt, attr):\\n        #    setattr(self.Pt, attr, val)\\n        # ...does not work properly\\n        # and the final standard python behaviour\\n        object.__setattr__(self, attr, val)\\n    \\n    def __getattr__(self, attr):\\n        # this is for Layer() pointed by Pt attr in Str() object\\n        if isinstance(self.Pt, Layer) and hasattr(self.Pt, attr):\\n            return getattr(self.Pt, attr)\\n        # and the final standard python behaviour\\n        object.__getattr__(self, attr)\\n    \\n    # the libmich internal instances check\\n    # this is highly experimental...\\n    def __is_intern_inst(self, obj):\\n        return isinstance(obj, (Element, Layer, Block))\\n    \\n    # building basic methods for manipulating easily the Element \\n    # from its attributes\\n    def __call__(self, l=None):\\n        # when length has fixed value:\\n        if not l and type(self.Len) is int:\\n            l = self.Len\\n        #else:\\n        #    l = None\\n        # when no values are defined at all:\\n        if self.Val is None and self.Pt is None: \\n            if l: return l * self._padding_byte\\n            else: return ''\\n        # returning the right string:\\n        # if defined, self.Val overrides self.Pt capabilities\\n        elif self.Val is not None:\\n            # allow to pass tuple or list of libmich internal instances\\n            if isinstance(self.Val, (list, tuple)) \\\\\\n            and False not in map(self.__is_intern_inst, self.Val):\\n                return ''.join(map(str, self.Val))[:l]\\n            return str(self.Val)[:l]\\n        # else: use self.Pt capabilities to get the string\\n        elif self.PtFunc is not None: \\n            if self.safe: \\n                assert(hasattr(self.PtFunc(self.Pt), '__str__'))\\n            return str(self.PtFunc(self.Pt))[:l]\\n        else:\\n            # allow to pass tuple or list of libmich internal instances\\n            if isinstance(self.Pt, (list, tuple)) \\\\\\n            and False not in map(self.__is_intern_inst, self.Pt):\\n                return ''.join(map(str, self.Pt))[:l]\\n            # otherwise, handle simply as is\\n            if self.safe: \\n                assert(hasattr(self.Pt, '__str__'))\\n            return str(self.Pt)[:l]\\n    \\n    def __str__(self):\\n        # when Element is Transparent:\\n        if self.is_transparent():\\n            return ''\\n        #else:\\n        return self()\\n    \\n    def __len__(self):\\n        # does not take the LenFunc(Len) into account\\n        # When a Str is defined, the length is considered dependent of the Str\\n        # the Str is dependent of the LenFunc(Len) only \\n        # when mapping data into the Element\\n        return len(self.__str__())\\n    \\n    def bit_len(self):\\n        return len(self)*8\\n    \\n    def map_len(self):\\n        # need special length definition \\n        # when mapping a string to the 'Str' element \\n        # that has no fixed length\\n        #\\n        # uses LenFunc, applied to Len, when length is variable:\\n        # and TransFunc, applied to Trans, to managed potential transparency\\n        # (e.g. for optional element triggered by other element)\\n        #\\n        if self.Len is None:\\n            return None\\n            #return 0\\n        if self.LenFunc is None: \\n            return self.Len\\n        else:\\n            if self.safe:\\n                assert( type(self.LenFunc(self.Len)) in (int, long) )\\n            return self.LenFunc(self.Len)\\n    \\n    def __int__(self):\\n        # big endian integer representation of the string buffer\\n        return shtr(self).left_val(len(self)*8)\\n    \\n    def __bin__(self):\\n        # does not use the standard python 'bin' function to keep \\n        # the right number of prefixed 0 bits\\n        h = hex(self)\\n        binary = ''\\n        for i in xrange(0, len(h), 2):\\n            b = format( int(h[i:i+2], 16), 'b' )\\n            binary += ( 8-len(b) ) * '0' + b\\n        return binary\\n    \\n    def __hex__(self):\\n        return self().encode('hex')\\n    \\n    def __repr__(self):\\n        # check for simple representations\\n        if self.Pt is None and self.Val is None: \\n            return repr(None)\\n        if self.Repr == 'ipv4':\\n            #if self.safe: assert( len(self) == 4 )\\n            if len(self) != 4:\\n                return '0x%s' % hex(self)\\n            return inet_ntoa( self.__str__() )\\n        elif self.Repr == 'hex': \\n            ret = '0x%s' % hex(self)\\n        elif self.Repr == 'bin': \\n            ret = '0b%s' % self.__bin__()\\n        # check for the best human-readable representation\\n        elif self.Repr == 'hum':\\n            # standard return\\n            ret = repr( self() )\\n            # complex return:\\n            # allow to assign a full Block or Layer to a Str...\\n            if self.__is_intern_inst(self.Pt):\\n                ret = repr(self.Pt)\\n            if self.__is_intern_inst(self.Val):\\n                ret = repr(self.Val)\\n            # allow to assign a list or tuple of Block or Layer...\\n            if isinstance(self.Pt, (list, tuple)) \\\\\\n            and False not in map(self.__is_intern_inst, self.Pt):\\n                ret = '|'.join(map(repr, self.Pt))\\n            if isinstance(self.Val, (list, tuple)) \\\\\\n            and False not in map(self.__is_intern_inst, self.Val):\\n                ret = '|'.join(map(repr, self.Val))\\n            # finally, self.Val can be a raw value... still\\n            if self.Val is not None and hasattr(self.Val, '__repr__'):\\n                ret = repr(self.Val)\\n        # truncate representation if string too long:\\n        # avoid terminal panic...\\n        if len(ret) <= self._repr_limit:\\n            return ret\\n        else:\\n            return ret[:self._repr_limit-3]+'...'\\n    \\n    # some more methods for checking Element's attributes:\\n    def getattr(self):\\n        return ['CallName', 'ReprName', 'Pt', 'PtFunc', 'Val', 'Len',\\n                'LenFunc', 'Type', 'Repr', 'Trans', 'TransFunc']\\n    \\n    def showattr(self):\\n        for a in self.getattr():\\n            print('%s : %s' % (a, repr(self.__getattribute__(a))) )\\n    \\n    # cloning an Element, useful for \\\"duplicating\\\" an Element \\n    # without keeping any dependency\\n    # used in Layer with Element\\n    # However,\\n    #...\\n    # This is not that true, as an object pointed by .Pt or .Len or .Trans\\n    # will not be updated to its clone()\\n    # Conclusion:\\n    # use this with care\\n    def clone(self):\\n        clone = self.__class__(\\n                 self.CallName, self.ReprName,\\n                 self.Pt, self.PtFunc,\\n                 self.Val, \\n                 self.Len, self.LenFunc,\\n                 self.Repr,\\n                 self.Trans, self.TransFunc )\\n        return clone\\n    \\n    # standard method map() to map a string to the Element\\n    def map(self, string=''):\\n        if not self.is_transparent():\\n            l = self.map_len()\\n            if l is not None:\\n                self.Val = string[:l]\\n            else:\\n                self.Val = string\\n            if self.dbg >= DBG:\\n                log(DBG, '(Element) mapping %s on %s, %s' \\\\\\n                    % (repr(string), self.CallName, repr(self)))\\n    \\n    def map_ret(self, string=''):\\n        self.map(string)\\n        return string[len(self):]\\n    \\n    # shar manipulation interface\\n    def to_shar(self):\\n        ret = shar()\\n        ret.set_buf(self())\\n        return ret\\n    \\n    def map_shar(self, sh):\\n        if not self.is_transparent():\\n            l = self.map_len()\\n            if l is not None:\\n                self.Val = sh.get_buf(l*8)\\n            else:\\n                self.Val = sh.get_buf()\\n            if self.dbg >= DBG:\\n                log(DBG, '(Element) mapping %s on %s, %s' \\\\\\n                    % (repr(string), self.CallName, repr(self)))\"}, {\"identifier\":\"Int\", \"path\":\"libmich/core/element.py\", \"snippet\":\"class Int(Element):\\n    '''\\n    class defining a standard element, managed like an integer.\\n    It can be signed (int) or unsigned (uint),\\n    and support arbitrary byte-length:\\n        int8, int16, int24, int32, int40, int48, int56, int64, int72, ...\\n        uint8, uint16, uint24, uint32, ..., uint128, ...\\n        and why not int65536 !\\n    It is always byte-aligned (in term of length, at least).\\n    \\n    attributes:\\n    Pt: to point to another object or direct integer value;\\n    PtFunc: when defined, PtFunc(Pt) is used to generate the integer value;\\n    Val: when defined, overwrites the Pt (and PtFunc) integer value, \\n         used when mapping a string buffer to the element;\\n    Type: type of integer for encoding, 8,16,24,32,40,48,56,64 bits signed or\\n          unsigned integer;\\n    Dict: dictionnary to use for a look-up when representing \\n          the element into python;\\n    Repr: representation style, binary, hexa or human: human uses Dict \\n          if defined;\\n    Trans: to define transparent element which has empty str() and len() to 0,\\n           it \\\"nullifies\\\" its existence; can point to something for automation;\\n    TransFunc: when defined, TransFunc(Trans) is used to automate the \\n               transparency aspect: used e.g. for conditional element;\\n    '''\\n    # endianness is 'little' / 'l' or 'big' / 'b'\\n    _endian = 'big'\\n    # types format for struct library\\n    # 24 (16+8), 40 (32+8), 48 (32+16), 56 (32+16+8)\\n    _types = { 'int8':'b', 'int16':'h', 'int32':'i', 'int64':'q',\\n               #'int24':None, 'int40':None, 'int48':None, 'int56':None,\\n               'uint8':'B', 'uint16':'H', 'uint32':'I', 'uint64':'Q',\\n               #'uint24':None, 'uint40':None, 'uint48':None, 'uint56':None,\\n               }\\n    #\\n    # for object representation\\n    _reprs = ['hex', 'bin', 'hum']\\n    \\n    def __init__(self, CallName='', ReprName=None,\\n                 Pt=None, PtFunc=None, Val=None,\\n                 Type='int32', Dict=None, DictFunc=None,\\n                 Repr='hum',\\n                 Trans=False, TransFunc=None):\\n        if CallName or not self.CallName:\\n            self.CallName = CallName\\n        if ReprName is None:\\n            self.ReprName = ''\\n        else:\\n            self.ReprName = ReprName\\n        self.Pt = Pt\\n        self.PtFunc = PtFunc\\n        self.Val = Val\\n        self.Type = Type\\n        self.Dict = Dict\\n        self.DictFunc = DictFunc\\n        self.Repr = Repr\\n        self.Trans = Trans\\n        self.TransFunc = TransFunc\\n        # automated attributes:\\n        self.Len = int(self.Type.lstrip('uint'))//8\\n    \\n    def __setattr__(self, attr, val):\\n        # ensures no bullshit is provided into element's attributes \\n        # (however, it is not a complete test...)\\n        if self.safe:\\n            if attr == 'CallName':\\n                if type(val) is not str or len(val) == 0:\\n                    raise AttributeError('CallName must be a non-null string')\\n            elif attr == 'ReprName':\\n                if type(val) is not str:\\n                    raise AttributeError('ReprName must be a string')\\n            elif attr == 'PtFunc':\\n                if val is not None and not isinstance(val, type_funcs) :\\n                    raise AttributeError('PtFunc must be a function')\\n            elif attr == 'Val':\\n                if val is not None and not isinstance(val, (int, long)):\\n                    raise AttributeError('Val must be an int or long')\\n            elif attr == 'Type':\\n                cur = 0\\n                if val[cur] == 'u':\\n                    cur = 1\\n                if val[cur:cur+3] != 'int' or int(val[cur+3:]) % 8 != 0:\\n                    raise AttributeError('Type must be intX / uintX with X '\\\\\\n                    'multiple of 8 bits (e.g. int24, uint32, int264, ...)')\\n            elif attr == 'DictFunc':\\n                if val is not None and not isinstance(val, type_funcs) :\\n                    raise AttributeError('DictFunc must be a function')\\n            elif attr == 'Repr':\\n                if val not in self._reprs:\\n                    raise AttributeError('Repr %s does not exist, use in: %s' \\\\\\n                          % (val, self._reprs))\\n            elif attr == 'TransFunc':\\n                if val is not None and not isinstance(val, type_funcs) :\\n                    raise AttributeError('TransFunc must be a function')\\n        if attr == 'Type':\\n            self.Len = int(val.lstrip('uint'))//8\\n        object.__setattr__(self, attr, val)\\n    \\n    def __call__(self):\\n        # when no values are defined at all, arbitrary returns None:\\n        if self.Val is None and self.Pt is None:\\n            # instead of \\\"return None\\\" \\n            # that triggers error when calling __str__() method\\n            return 0\\n        # else, Val overrides Pt capabilities:\\n        # transparency are not taken into account in __call__, \\n        # only for the __str__ representation\\n        elif self.Val is not None: \\n            return self.__confine( self.Val )\\n        elif self.PtFunc is not None: \\n            if self.safe: \\n                assert( type(self.PtFunc(self.Pt)) in (int, long) )\\n            return self.__confine(self.PtFunc(self.Pt))\\n        else:\\n            return self.__confine(int(self.Pt))\\n    \\n    def __confine(self, value):\\n        # unsigned\\n        if self.Type[0] == 'u':\\n            return max(0, min(2**(8*self.Len)-1, value))\\n        # signed\\n        else:\\n            return max(-2**(8*self.Len-1), min(2**(8*self.Len-1)-1, value))\\n    \\n    def __str__(self):\\n        # manages Element transparency\\n        if self.is_transparent():\\n                return ''\\n        # otherwise returns standard string values\\n        return self.__pack()\\n    \\n    def __len__(self):\\n        if self.is_transparent(): \\n            return 0\\n        return self.Len\\n    \\n    def bit_len(self):\\n        return 8*self.__len__()\\n    \\n    # map_len() is a-priori not needed in \\\"Int\\\" element, \\n    # but still kept for Element API uniformity\\n    def map_len(self):\\n        return self.__len__()\\n    \\n    # define integer value\\n    def __int__(self):\\n        return self()\\n    \\n    def __bin__(self):\\n        # unsigned or positive signed:\\n        val = self()\\n        if self.Type[0] == 'u' \\\\\\n        or self.Type[0] == 'i' and val >= 0 : \\n            binstr = bin(val)[2:]\\n            if self._endian[0] == 'l':\\n                # little endian\\n                bs = '0'*(8*self.__len__() - len(binstr)) + binstr\\n                bs = [bs[i:i+8] for i in range(0, len(bs), 8)]\\n                bs.reverse()\\n                return ''.join(bs)\\n            else:\\n                # big endian\\n                return '0'*(8*self.__len__() - len(binstr)) + binstr\\n        # negative signed\\n        else : \\n            # takes 2' complement to the signed val\\n            binstr = bin(val + 2**(8*self.__len__()-1))[2:]\\n            if self._endian[0] == 'l':\\n                # little endian\\n                bs = '1' + '0'*(8*self.__len__() - len(binstr) - 1) + binstr\\n                bs = [bs[i:i+8] for i in range(0, len(bs), 8)]\\n                bs.reverse()\\n                return ''.join(bs)\\n            else:\\n                # big endian\\n                return '1' + '0'*(8*self.__len__() - len(binstr) - 1) + binstr\\n    \\n    def __hex__(self):\\n        return hexlify(self.__pack())\\n    \\n    def __repr__(self):\\n        if self.Pt is None and self.Val is None:\\n            return repr(None)\\n        if self.Repr == 'hex':\\n            return '0x%s' % self.__hex__()\\n        elif self.Repr == 'bin':\\n            return '0b%s' % self.__bin__()\\n        elif self.Repr == 'hum':\\n            value = self()\\n            if self.DictFunc:\\n                if self.safe:\\n                    assert(hasattr(self.DictFunc(self.Dict), '__getitem__'))\\n                try:\\n                    val = '%i : %s' % (value, self.DictFunc(self.Dict)[value])\\n                except KeyError:\\n                    val = value\\n            elif self.Dict:\\n                try:\\n                    val = '%i : %s' % (value, self.Dict[value])\\n                except KeyError:\\n                    val = value\\n            else:\\n                val = value\\n            rep = repr(val)\\n            if rep[-1] == 'L':\\n                return rep[:-1]\\n            return rep\\n    \\n    def getattr(self):\\n        return ['CallName', 'ReprName', 'Pt', 'PtFunc', 'Val', 'Len',\\n                'Type', 'Dict', 'DictFunc', 'Repr', 'Trans', 'TransFunc']\\n    \\n    def showattr(self):\\n        for a in self.getattr():\\n            if a == \\\"Dict\\\" and self.Dict is not None:\\n                print('%s : %s' % ( a, self.__getattribute__(a).__class__) )\\n            else:\\n                print('%s : %s' % ( a, repr(self.__getattribute__(a))) )\\n    \\n    def clone(self):\\n        clone = self.__class__(\\n                 self.CallName, self.ReprName,\\n                 self.Pt, self.PtFunc,\\n                 self.Val, self.Type,\\n                 self.Dict, self.DictFunc, self.Repr, \\n                 self.Trans, self.TransFunc )\\n        #clone._endian = self._endian\\n        return clone\\n    \\n    def map(self, string=''):\\n        if not self.is_transparent():\\n            # error log will be done by the Layer().map() method\\n            # but do this not to throw exception\\n            if len(string) < self.Len:\\n                if self.dbg >= WNG:\\n                    log(WNG, '(%s) %s map(string) : string not long enough' \\\\\\n                        % (self.__class__, self.CallName))\\n                return\\n            self.Val = self.__unpack(string[:self.Len])\\n    \\n    def map_ret(self, string=''):\\n        l = self.__len__()\\n        if 0 < l <= len(string):\\n            self.map(string)\\n            return string[l:]\\n        else:\\n            return string\\n    \\n    def __pack(self):\\n        # manage endianness (just in case...)\\n        if self._endian[0] == 'l':\\n            e = '<'\\n        else:\\n            e = '>'\\n        if self.Len == 0:\\n            return ''\\n        elif self.Len in (1, 2, 4, 8):\\n            # standard types for using Python struct\\n            return pack(e+self._types[self.Type], self())\\n        elif self.Type[0] == 'u':\\n            # non-standard unsigned int types\\n            return self.__pack_uX(e)\\n        else:\\n            # non-standard signed int types\\n            return self.__pack_iX(e)\\n    \\n    def __unpack(self, string):\\n        if self._endian[0] == 'l':\\n            e = '<'\\n        else:\\n            e = '>'\\n        if self.Len == 0:\\n            return\\n        elif self.Len in (1, 2, 4, 8):\\n            # standard types for using Python struct\\n            return unpack(e+self._types[self.Type], string[:self.Len])[0]\\n        elif self.Type[0] == 'u':\\n            # non-standard unsigned int types\\n            return self.__unpack_uX(string[:self.Len], e)\\n        else:\\n            # non-standard signed int types\\n            return self.__unpack_iX(string[:self.Len], e)\\n    \\n    def __pack_uX(self, e='>'):\\n        if e == '<':\\n            # little endian, support indefinite uint (e.g. uint3072)\\n            if self.Len <= 8:\\n                return pack('<Q', self())[:self.Len]\\n            else:\\n                #'''\\n                u8 = decompose(0x100, self())\\n                # pad with 0 until the right length\\n                if len(u8) < self.Len:\\n                    u8.extend( [0]*(self.Len-len(u8)) )\\n                return pack('<'+self.Len*'B', *u8)\\n        else:\\n            # big endian, support indefinite uint too\\n            if self.Len <= 8:\\n                return pack('>Q', self())[-self.Len:]\\n            else:\\n                u64 = decompose(0x10000000000000000, self())\\n                u64_len = 8*len(u64)\\n                u64_pad = 0\\n                if u64_len < self.Len:\\n                    rest_len = self.Len - u64_len\\n                    u64_pad = rest_len // 8\\n                    if rest_len % 8:\\n                        u64_pad += 1\\n                    u64.extend( [0]*u64_pad )\\n                u64.reverse()\\n                return pack('>'+len(u64)*'Q', *u64)[-self.Len:]\\n    \\n    def __pack_iX(self, e='>'):\\n        val = self()\\n        # positive values are encoded just like uint\\n        if val >= 0:\\n            return self.__pack_uX(e)\\n        #\\n        # negative values, 2's complement encoding\\n        if e == '<':\\n            # little endian\\n            if self.Len <= 8:\\n                return pack('<Q', 2**(self.Len*8) - abs(val))[:self.Len]\\n            else:\\n                i8 = decompose(0x100, 2**(self.Len*8) - abs(val))\\n                if len(i8) < self.Len:\\n                    i8.extend( [0]*(self.Len-len(i8)) )\\n                return pack('<'+self.Len*'B', *i8)\\n        else:\\n            # big endian\\n            if self.Len <= 8:\\n                return pack('>Q', 2**(self.Len*8) - abs(val))[-self.Len:]\\n            else:\\n                i8 = decompose(0x100, 2**(self.Len*8) - abs(val))\\n                if len(i8) < self.Len:\\n                    i8.extend( [0]*(self.Len-len(i8)) )\\n                i8.reverse()\\n                return pack('>'+self.Len*'B', *i8)\\n    \\n    def __unpack_uX(self, string, e='>'):\\n        if e == '<':\\n            # little endian, support indefinite uint (e.g. uint3072)\\n            if self.Len <= 8:\\n                return unpack('<Q', string + '\\\\0'*(8 - self.Len))[0]\\n            else:\\n                u8 = list(unpack('<'+self.Len*'B', string))\\n                u8.reverse()\\n                return reduce(lambda x,y: (x<<8)+y, u8)\\n        else:\\n            # big endian, support indefinite uint (e.g. uint3072)\\n            if self.Len <= 8:\\n                return unpack('>Q', '\\\\0'*(8 - self.Len) + string)[0]\\n            else:\\n                #'''\\n                u64_len, u64_pad = self.Len // 8, self.Len % 8\\n                if u64_pad:\\n                    u64_len += 1\\n                    string = '\\\\0'*u64_pad + string\\n                u64 = unpack('>'+u64_len*'Q', string)\\n                return reduce(lambda x,y:(x<<64)+y, u64)\\n    \\n    def __unpack_iX(self, string, e='>'):\\n        if e == '<':\\n            # little endian\\n            if ord(string[-1]) & 0x80 == 0:\\n                # check if it is a positive value\\n                return self.__unpack_uX(string, e)\\n            elif self.Len <= 8:\\n                return unpack('<Q', string + '\\\\0'*(8 - self.Len))[0] \\\\\\n                       - 2**(self.Len*8)\\n            else:\\n                i8 = list(unpack('<'+self.Len*'B', string))\\n                i8.reverse()\\n                return reduce(lambda x,y:(x<<8)+y, i8) - 2**(self.Len*8)\\n        else:\\n            # big endian\\n            if ord(string[0]) & 0x80 == 0:\\n                # check if it is a positive value\\n                return self.__unpack_uX(string, e)\\n            elif self.Len <= 8:\\n                return unpack('>Q', '\\\\0'*(8 - self.Len) + string)[0] \\\\\\n                       - 2**(self.Len*8)\\n            else:\\n                i8 = unpack('>'+self.Len*'B', string)\\n                return reduce(lambda x,y:(x<<8)+y, i8) - 2**(self.Len*8)\\n    \\n    # shar manipulation interface\\n    def to_shar(self):\\n        ret = shar()\\n        ret.set_buf(self.__str__())\\n        return ret\\n    \\n    def map_shar(self, sh):\\n        if len(sh)*8 < self.Len:\\n            if self.dbg >= WNG:\\n                log(WNG, '(%s) %s map(string) : shar buffer not long enough' \\\\\\n                    % (self.__class__, self.CallName))\\n            return\\n        # standard handling\\n        if not self.is_transparent():\\n            self.map(sh.get_buf(len(self)*8))\"}, {\"identifier\":\"Layer\", \"path\":\"libmich/core/element.py\", \"snippet\":\"class Layer(object):\\n    '''\\n    class built from stack of \\\"Str\\\", \\\"Int\\\", \\\"Bit\\\" and \\\"Layer\\\" objects\\n    got from the initial constructorList.\\n    Layer object is recursive: it can contain other Layer() instances\\n    Layer does not require to be byte-aligned. This happens depending of the\\n    presence of Bit() instances.\\n    \\n    when instantiated:\\n    clones the list of \\\"Str\\\", \\\"Int\\\", \\\"Bit\\\" elements in the constructorList\\n    to build a dynamic elementList, that can be changed afterwards (adding /\\n    removing objects);\\n    A common hierarchy level for the whole Layer is defined, it is useful \\n    when used into \\\"Block\\\" to create hierarchical relationships: \\n        self.hierarchy (int), self.inBlock (bool)\\n        when .inBlock is True, provides: .get_payload(), .get_header(), \\n             .has_next(), .get_next(), .get_previous(), and .Block\\n    It provides several methods for calling elements in the layer:\\n        by CallName / ReprName passed in attribute\\n        by index in the elementList\\n        can be iterated too\\n        and many other manipulations are defined\\n    It has also some common methods with \\\"Str\\\", \\\"Int\\\" and \\\"Bit\\\" to emulate \\n    a common handling:\\n    __str__, __len__, __int__, bit_len, getattr, showattr, show, map\\n    '''\\n    #\\n    # debugging threshold for Layer:\\n    dbg = ERR\\n    # add some sanity checks\\n    #safe = True\\n    safe = False\\n    # define the type of str() and map() method\\n    _byte_aligned = True\\n    # reserved attributes:\\n    Reservd = ['CallName', 'ReprName', 'elementList', 'Len', 'BitLen',\\n               'hierarchy', 'inBlock', 'Trans', 'ConstructorList',\\n               'dbg', 'Reservd']\\n    #\\n    # represent transparent elements in __repr__()\\n    _repr_trans = True\\n    \\n    # structure description:\\n    constructorList = []\\n    \\n    def __init__(self, CallName='', ReprName='', Trans=False, **kwargs):\\n        if type(CallName) is not str:\\n            raise AttributeError('CallName must be a string')\\n        elif len(CallName) == 0:\\n            self.CallName = self.__class__.__name__\\n        else:\\n            self.CallName = CallName\\n        if type(ReprName) is str and len(ReprName) > 0: \\n            self.ReprName = ReprName\\n        else: \\n            self.ReprName = ''\\n        self.elementList = []\\n        self.set_hierarchy(0)\\n        self.inBlock = False\\n        self.Trans = Trans\\n        \\n        CallNames = []\\n        for e in self.constructorList:\\n            # This is for little players\\n            #if isinstance(e, Element):\\n            # OK, now let's put the balls on the table and\\n            # make Layer recursive (so will have Layer() into Layer())\\n            if isinstance(e, (Element, Layer)):\\n                if e.CallName in self.Reservd:\\n                    if self.safe or self.dbg >= ERR:\\n                        log(ERR, '(Layer - %s) using a reserved '\\n                            'attribute as CallName %s: aborting...' \\\\\\n                          % (self.__class__, e.CallName))\\n                    return\\n                if e.CallName in CallNames:\\n                    if self.dbg >= WNG:\\n                        log(WNG, '(Layer - %s) different elements have ' \\\\\\n                           'the same CallName %s' % (self.__class__, e.CallName))\\n                if isinstance(e, Element):\\n                    self.append(e.clone())\\n                # do not clone Layer() as it breaks dynamic element inside\\n                # i.e. element with PtFunc, LenFunc, DictFunc, TransFunc defined\\n                # TODO: patch Layer().clone() method to solve this...\\n                # lets try with deepcopy()\\n                elif isinstance(e, Layer):\\n                    self.append(e.clone())\\n            CallNames.append(e.CallName)\\n        \\n        # check for bit alignment until we lost information on the Layer length\\n        # also check if fixed length can be deduced\\n        self.BitLen = 0\\n        for e in self.elementList:\\n            if self.dbg >= DBG:\\n                log(DBG, '(Layer - %s) length verification for %s' \\\\\\n                    % (self.__class__, e.CallName))\\n            if isinstance(e, Bit):\\n                self.BitLen += e.bit_len()\\n            elif hasattr(e, 'Len') and type(e.Len) is int:\\n                self.BitLen += (e.Len)*8\\n            else:\\n                self.BitLen, self.Len = 'var', 'var'\\n                break\\n        if type(self.BitLen) is int :\\n            if self.BitLen % 8:\\n                if self.dbg >= WNG and self._byte_aligned:\\n                    log(WNG, '(Layer - %s) Elements seem not to be '\\\\\\n                        'byte-aligned: hope you expect it!' \\\\\\n                        % self.__class__)\\n                # record length in bit (precise one) and in bytes (unprecised)\\n                self.Len = 1 + self.BitLen//8\\n            else:\\n                self.Len = self.BitLen//8\\n        #\\n        # check additional args that would correspond to contained Element\\n        args = kwargs.keys()\\n        if self.dbg >= DBG:\\n            log(DBG, '(Layer - %s) init kwargs: %s' % (self.__class__, args))\\n        for e in self:\\n            if hasattr(e, 'CallName') and e.CallName in args:\\n                if hasattr(e, 'Pt'):\\n                    e.Pt = kwargs[e.CallName]\\n                else:\\n                    kwe = kwargs[e.CallName]\\n                    if isinstance(kwe, (tuple, list)):\\n                        e.__init__(*kwe)\\n                    elif isinstance(kwe, dict):\\n                        e.__init__(**kwe)\\n    \\n    # define some basic list facilities for managing elements into the Layer, \\n    # through the \\\"elementList\\\" attribute:\\n    def __iter__(self):\\n        if 'elementList' in self.__dict__.keys():\\n            return self.__dict__['elementList'].__iter__()\\n        else: return [].__iter__()\\n    \\n    def __getitem__(self, num):\\n        return self.elementList[num]\\n    \\n    def __getslice__(self, i, j):\\n        l = Layer('_slice_')\\n        if not i or i < 0:\\n            i=0\\n        #maxj = len(self.elementList)-1\\n        maxj = len(self.elementList)\\n        if not j or j > maxj:\\n            j = maxj\\n        #\\n        for k in xrange(i, j):\\n            l.append( self[k] )\\n        return l\\n    \\n    def __setitem__(self, num, value):\\n        # special handling here: \\n        # use to override the element value \\n        # with its \\\"Val\\\" attribute (like when mapping a string)\\n        self.elementList[num].Val = value\\n    \\n    def append(self, element):\\n        #if isinstance(element, Element):\\n        # make Layer recursive:\\n        if isinstance(element, (Element, Layer)):\\n            if self.dbg >= WNG and element.CallName in self.getattr():\\n                log(WNG, '(Layer - %s) different elements have the same '\\\\\\n                         'CallName %s' % (self.__class__, element.CallName))\\n            self.elementList.append(element)\\n    \\n    def __lshift__(self, element):\\n        self.append(element)\\n        if isinstance(element, Layer):\\n            element.inc_hierarchy(self.hierarchy)\\n    \\n    def insert(self, index, element):\\n        CallNames = self.getattr()\\n        #if isinstance(element, Element):\\n        # make Layer recursive:\\n        if isinstance(element, (Element, Layer)):\\n            if self.dbg >= WNG and element.CallName in CallNames:\\n                log(WNG, '(Layer - %s) different elements have the same '\\\\\\n                         'CallName %s' % (self.__class__, element.CallName))\\n            self.elementList.insert(index, element)\\n    \\n    def __rshift__(self, element):\\n        self.insert(0, element)\\n        if isinstance(element, Layer):\\n            element.inc_hierarchy(self.hierarchy)\\n    \\n    def extend(self, newElementList):\\n        for e in newElementList:\\n            self.append(e)\\n    \\n    def remove(self, element):\\n        for e in self:\\n            if e == element:\\n                self.elementList.remove(element)\\n    \\n    def replace(self, current_element, new_element):\\n        # check index of the element ro replace\\n        index = 0\\n        for elt in self.elementList:\\n            if elt == current_element:\\n                self.remove(current_element)\\n                self.insert(index, new_element)\\n                return\\n            else:\\n                index += 1\\n    \\n    # define some attribute facilities for managing elements \\n    # by their CallName into the Layer\\n    # warning: dangerous when parsing data into Layer, \\n    # with elements which could have same CallName\\n    # \\n    # list facilities can be preferred in this case\\n    def __getattr__(self, name):\\n        names = [x.CallName for x in self.elementList]\\n        if name in names:\\n            return self.elementList[ names.index(name) ]\\n        names = [x.ReprName for x in self.elementList]\\n        if name in names:\\n            return self.elementList[ names.index(name) ]\\n        #\\n        return object.__getattribute__(self, name)\\n        #return self.__getattribute__(name)\\n        #return getattr(self, name)\\n    \\n    def __setattr__(self, name, value):\\n        # special handling here: use to override the element value \\n        # with its \\\"Val\\\" attribute (like when mapping a string)\\n        for e in self:\\n            if name == e.CallName or name == e.ReprName: \\n                e.Val = value\\n                return\\n        return object.__setattr__(self, name, value)\\n        raise AttributeError( '\\\"Layer\\\" has no \\\"%s\\\" attribute: %s' \\\\\\n              % (name, self.getattr()) )\\n    \\n    def __hasattr__(self, name):\\n        for e in self:\\n            if name == e.CallName or name == e.ReprName: \\n                return True\\n        #return object.__hasattr__(self, name): \\n        # not needed (does not work in the code... but works in python...)\\n        raise AttributeError( '\\\"Layer\\\" has no \\\"%s\\\" attribute: %s' \\\\\\n              % (name, self.getattr()) )\\n    \\n    # method for managing the Layer hierarchy (easy):\\n    def set_hierarchy(self, hier=0):\\n        self.hierarchy = hier\\n        for e in self:\\n            if isinstance(e, Layer):\\n                e.set_hierarchy(hier)\\n        \\n    def inc_hierarchy(self, ref=None):\\n        if ref is None:\\n            self.set_hierarchy(self.hierarchy+1)\\n        else: \\n            self.set_hierarchy(self.hierarchy+ref+1)\\n        #for l in self:\\n        #    if isinstance(l, Layer):\\n        #        l.hierarchy = self.hierarchy\\n    \\n    def dec_hierarchy(self, ref=None):\\n        if ref is None: \\n            self.set_hierarchy(self.hierarchy-1)\\n        else: \\n            self.set_hierarchy(self.hierarchy+ref-1)\\n        #for l in self:\\n        #    if isinstance(l, Layer):\\n        #        l.hierarchy = self.hierarchy\\n    \\n    # define same methods as \\\"Element\\\" type for being use the same way\\n    def __str__(self):\\n        if self.dbg >= DBG:\\n            log(DBG, '(Layer.__str__) entering str() for %s' % self.CallName)\\n        # First take care of transparent Layer (e.g. in L3Mobile)\\n        if hasattr(self, 'Trans') and self.Trans:\\n            return ''\\n        # dispatch to the right method depending of byte alignment\\n        if self._byte_aligned is True:\\n            return self.__str_aligned()\\n        else:\\n            return self.__str_unaligned()\\n    \\n    def __str_unaligned(self):\\n        # then init resulting string \\n        # and bit offset needed to shift unaligned strings\\n        s = []\\n        off = 0\\n        # loop on each element into the Layer\\n        # also on Layer into Layer...\\n        for e in self:\\n            shtr_e, bitlen_e = e.shtr(), e.bit_len()\\n            rest_e = bitlen_e % 8\\n            if self.dbg >= DBG:\\n                log(DBG, '(Layer.__str__) %s: %s, %i, offset: %i' \\\\\\n                    % (e.CallName, hexlify(shtr_e), bitlen_e, off))\\n            # if s is not byte-aligned and e is not transparent\\n            # update the last component of s\\n            if off and shtr_e:\\n                # 1st update last bits of s with MSB of e, \\n                # before stacking with the rest of e\\n                # (8 - off) is the room left in the LSB of s\\n                s[-1] = ''.join((s[-1][:-1],\\n                                  chr(ord(s[-1][-1]) + shtr_e.left_val(8-off)),\\n                                  (shtr_e << (8-off))\\n                                ))\\n                # take care in case the shifting of e voids its last byte\\n                if rest_e and rest_e-(8-off) <= 0:\\n                    s[-1] = s[-1][:-1]\\n                # update offset\\n                if rest_e:\\n                    off += bitlen_e\\n                    off %= 8\\n            # in case s is already aligned, append a new component to s\\n            elif shtr_e:\\n                s.append(shtr_e)\\n                # update offset\\n                if rest_e:\\n                    off += rest_e\\n            if self.dbg >= DBG:\\n                log(DBG, '(Layer.__str__) %s' % hexlify(s))\\n        # well done!\\n        return ''.join(s)\\n    \\n    def __str_aligned(self):\\n        s = []\\n        BitStream = ''\\n        # loop on each element in the Layer\\n        # also on Layer into Layer...\\n        for e in self:\\n            # need special processing for stacking \\\"Bit\\\" element: \\n            #   using \\\"BitStream\\\" variable\\n            #   works only with contiguous \\\"Bit\\\" elements \\n            #   to avoid byte-misalignment of other element in the Layer \\n            #   (and programming complexity with shifting everywhere...)\\n            if isinstance(e, Bit):\\n                # manage element transparency with (Trans, TranFunc)\\n                # and build a bitstream ('100110011...1101011') from Bit values\\n                if not e.is_transparent():\\n                    BitStream += str(e.__bin__())\\n                # when arriving on a byte boundary from bitstream, \\n                # create bytes and put it into the s variable\\n                if len(BitStream) >= 8:\\n                    while 1:\\n                        s.append( pack('!B', int(BitStream[:8], 2)) )\\n                        BitStream = BitStream[8:]\\n                        if len(BitStream) < 8:\\n                            break\\n                if self.dbg >= DBG:\\n                    log(DBG, '(Element) %s: %s, %s\\\\nBitstream: %s' \\\\\\n                        % (e.CallName, e(), e.__bin__(), BitStream))\\n            # when going to standard Str or Int element, \\n            # or directly end of __str__ function \\n            # verify the full BitStream has been consumed\\n            # and continue to build the resulting string easily...\\n            else:\\n                # possible byte mis-alignment for Str / Int is not managed...\\n                self.__is_aligned(BitStream)\\n                BitStream = ''\\n                if isinstance(e, Layer) and not e.Trans \\\\\\n                or isinstance(e, Element):\\n                    s.append( str(e) )\\n        self.__is_aligned(BitStream)\\n        return ''.join(s)\\n    \\n    def __is_aligned(self, BitStream):\\n        if BitStream and self.dbg >= ERR:\\n            log(ERR, '(Layer - %s) some of the Bit elements have not been ' \\\\\\n                'stacked in the \\\"str(Layer)\\\"\\\\nremaining bitstream: %s' \\\\\\n                % (self.__class__, BitStream))\\n            if self.safe:\\n                assert(not BitStream)\\n    \\n    def __call__(self):\\n        return self.__str__()\\n    \\n    def __len__(self):\\n        return len(self.__str__())\\n    \\n    def shtr(self):\\n        return shtr(self.__str__())\\n    \\n    def bit_len(self):\\n        # just go over all internal elements to track their own bit length\\n        # updated attributes initialized when Layer was constructed\\n        self.BitLen = 0\\n        for e in self:\\n            if hasattr(e, 'bit_len'):\\n                self.BitLen += e.bit_len()\\n            elif hasattr(e, '__len__'):\\n                self.BitLen += len(e)*8\\n        self.Len = 1 + (self.BitLen // 8) if self.BitLen % 8 \\\\\\n                   else (self.BitLen // 8)\\n        return self.BitLen\\n    \\n    def __hex__(self):\\n        bit_len = self.bit_len()\\n        hex_len = bit_len/4\\n        if bit_len%4:\\n            hex_len += 1\\n        #\\n        return self.__str__().encode('hex')[:hex_len]\\n    \\n    def __bin__(self):\\n        bits = []\\n        for e in self:\\n            bits.append( e.__bin__() )\\n        return ''.join(bits)\\n    \\n    def __int__(self):\\n        # big endian integer representation of the string buffer\\n        if self._byte_aligned:\\n            return shtr(self).left_val(len(self)*8)\\n        else:\\n            return shtr(self).left_val(self.bit_len())\\n    \\n    def __repr__(self):\\n        t = ''\\n        if self.Trans:\\n            t = ' - transparent '\\n        s = '<%s[%s]%s: ' % ( self.ReprName, self.CallName, t )\\n        for e in self:\\n            if self._repr_trans or not e.is_transparent():\\n                s += '%s(%s):%s, ' % ( e.CallName, e.ReprName, repr(e) )\\n        s = s[:-2] + '>'\\n        return s\\n    \\n    def map_len(self):\\n        return len(self)\\n    \\n    def getattr(self):\\n        return [e.CallName for e in self.elementList]\\n    \\n    def showattr(self):\\n        for a in self.getattr():\\n            print('%s : %s' % ( a, repr(self.__getattr__(a))) )\\n    \\n    def clone2(self):\\n        # TODO: deepcopy is not adapted here, can create errors...\\n        return deepcopy(self)\\n    \\n    def clone(self):\\n        #\\n        # build a new constructorList made of clones\\n        constructorList_new = []\\n        for e in self:\\n            constructorList_new.append(e.clone())\\n        #\\n        # substitute the current constructorList with the one made of clones\\n        constructorList_ori = self.__class__.constructorList\\n        self.__class__.constructorList = constructorList_new\\n        # instantiate the clone\\n        c = self.__class__()\\n        c.CallName = self.CallName\\n        if hasattr(self, 'ReprName'):\\n            c.ReprName = self.ReprName\\n        if hasattr(self, 'Trans'):\\n            c.Trans = self.Trans\\n        # restore the original constructorList\\n        self.__class__.constructorList = constructorList_ori\\n        #\\n        return c\\n    \\n    def is_transparent(self):\\n        if self.Trans:\\n            return True\\n        else:\\n            return False\\n    \\n    def show(self, with_trans=False):\\n        re, tr = '', ''\\n        if self.ReprName != '':\\n            re = '%s ' % self.ReprName\\n        if self.is_transparent():\\n            # TODO: eval the best convinience here\\n            if not with_trans:\\n                return ''\\n            tr = ' - transparent'\\n        # Layer content\\n        str_lst = [e.show().replace('\\\\n', '\\\\n ') for e in self]\\n        # insert spaces for nested layers and filter out empty content\\n        str_lst = [' %s\\\\n' % s for s in str_lst if s]\\n        # insert layer's title\\n        str_lst.insert(0, '### %s[%s]%s ###\\\\n' % (re, self.CallName, tr))\\n        # return full inline string without last CR\\n        return ''.join(str_lst)[:-1]\\n    \\n    def map(self, string=''):\\n        if self.dbg >= DBG:\\n            log(DBG, '(Layer.map) entering map() for %s' % self.CallName)\\n        # First take care of transparent Layer (e.g. in L3Mobile)\\n        if hasattr(self, 'Trans') and self.Trans:\\n            return\\n        # dispatch to the right method depending of byte alignment\\n        if self._byte_aligned is True:\\n            self.__map_aligned(string)\\n        else:\\n            self.__map_unaligned(string)\\n    \\n    def __map_unaligned(self, string=''):\\n        s = shtr(string)\\n        # otherwise go to map() over all elements\\n        for e in self:\\n            if self.dbg >= DBG:\\n                log(DBG, '(Layer.__map_unaligned) %s, bit length: %i' \\\\\\n                    % (e.CallName, e.bit_len()))\\n                log(DBG, '(Layer.__map_unaligned) string: %s' % hexlify(s))\\n            # this is beautiful\\n            s = e.map_ret(s)\\n    \\n    def __map_aligned(self, string=''):\\n        # Bit() elements are processed intermediary: \\n        # 1st placed into BitStack\\n        # and when BitStack is byte-aligned (check against BitStack_len)\\n        # string buffer is then mapped to it\\n        self.__BitStack = []\\n        self.__BitStack_len = 0\\n        # Furthermore, it manages only contiguous Bit elements \\n        # for commodity... otherwise, all other elements should be shifted\\n        #\\n        for e in self:\\n            # special processing for Bit() element:\\n            if isinstance(e, Bit):\\n                self.__add_to_bitstack(e)\\n                # if BitStack is byte aligned, map string to it:\\n                if self.__BitStack_len % 8 == 0:\\n                    string = self.__map_to_bitstack(string)\\n            # for other elements (Str(), Int(), Layer()), standard processing:   \\n            else:\\n                if self.__BitStack_len > 0 and self.dbg >= ERR:\\n                    log(WNG, '(Layer - %s) some of the Bit elements have not ' \\\\\\n                        'been mapped in the \\\"Layer\\\": not byte-aligned' \\\\\\n                        % self.__class__)\\n                if isinstance(e, (Layer, Element)) and not e.is_transparent():\\n                    if len(string) < e.map_len() and self.dbg >= WNG:\\n                        log(WNG, '(Layer - %s) String buffer not long ' \\\\\\n                            'enough for %s' % (self.__class__, e.CallName))\\n                        #if self.safe:\\n                        #    return\\n                    e.map(string)\\n                    string = string[e.map_len():]\\n        # delete .map() *internal* attributes\\n        del self.__BitStack\\n        del self.__BitStack_len\\n    \\n    def __add_to_bitstack(self, bit_elt):\\n        # check for Bit() element transparency\\n        if not bit_elt.is_transparent():\\n            self.__BitStack += [bit_elt]\\n            self.__BitStack_len += bit_elt.bit_len()\\n            \\n    def __map_to_bitstack(self, string):\\n        # 1st check if string is long enough for the prepared BitStack\\n        if len(string) < self.__BitStack_len//8 and self.dbg >= ERR:\\n            log(ERR, '(Layer - %s) String buffer not long enough for %s' \\\\\\n                % (self.__class__, self.__BitStack[-1].CallName))\\n            #if self.safe:\\n            #    return\\n        # string buffer parsing is done through intermediary\\n        # string buffer \\\"s_stack\\\"\\n        s_stack = string[:self.__BitStack_len//8]\\n        # create a bitstream \\\"s_bin\\\" for getting the full BitStack\\n        s_bin = ''\\n        for char in s_stack:\\n            # convert to bitstream thanks to python native bit repr\\n            s_bin_tmp = bin(ord(char))[2:]\\n            # prepend 0 to align on byte (python does not do it)\\n            # and append to the bitstream \\\"s_bin\\\" (string of 0 and 1)\\n            s_bin = ''.join((s_bin, (8-len(s_bin_tmp))*'0', s_bin_tmp))\\n        # map the bitstream \\\"s_bin\\\" into each BitStack element\\n        for bit_elt in self.__BitStack:\\n            bitlen = bit_elt.bit_len()\\n            if bitlen:\\n                # convert the bitstream \\\"s_bin\\\" into integer \\n                # according to the length in bit of bit_elt\\n                bit_elt.map_bit( int(s_bin[:bit_elt.bit_len()], 2) )\\n                # truncate the \\\"s_bin\\\" bitstream\\n                s_bin = s_bin[bit_elt.bit_len():]\\n        # consume the global string buffer that has been mapped \\n        # (from s_stack internal variable)\\n        # and reinitialize self.__BitStack* attributes\\n        string = string[self.__BitStack_len//8:]\\n        self.__BitStack = []\\n        self.__BitStack_len = 0\\n        # finally return string to parent method .map()\\n        return string\\n    \\n    # map_ret() maps a buffer to a Layer, the unaligned way,\\n    # and returns the rest of the buffer that was not mapped\\n    def map_ret(self, string=''):\\n        if self.dbg >= DBG:\\n            log(DBG, '(Layer.map_ret) entering map_ret() for %s' % self.CallName)\\n        # First take care of transparent Layer (e.g. in L3Mobile)\\n        if hasattr(self, 'Trans') and self.Trans:\\n            return string\\n        if self._byte_aligned is True:\\n            self.__map_aligned(string)\\n            return string[len(self):]\\n        else:\\n            # actually, map_ret() is only interesting for unaligned layers\\n            s = shtr(string)\\n            for e in self:\\n                if self.dbg >= DBG:\\n                    log(DBG, '(Layer.map_ret) %s, bit length: %i' \\\\\\n                        % (e.CallName, e.bit_len()))\\n                    log(DBG, '(Layer.map_ret) string: %s' % hexlify(s))\\n                # this is beautiful\\n                s = e.map_ret(s)\\n            return s\\n    \\n    # define methods when Layer is in a Block:\\n    # next, previous, header: return Layer object reference\\n    # payload: returns Block object reference\\n    def get_index(self):\\n        if self.inBlock is not True: \\n            return 0\\n        i = 0\\n        for l in self.Block:\\n            if l == self: return i\\n            else:  i += 1\\n        # could happen if Layer is placed in several Block()\\n        # and the last Block used is deleted\\n        return False\\n    \\n    def has_next(self):\\n        if self.inBlock is not True: \\n            return False\\n        return self.Block.has_index(self.get_index()+1)\\n    \\n    def get_next(self):\\n        if self.has_next():\\n            return self.Block[self.get_index()+1]\\n        return RawLayer()\\n    \\n    def has_previous(self):\\n        if self.inBlock is not True: \\n            return False\\n        index = self.get_index()\\n        if index <= 0:\\n            return False\\n        return self.Block.has_index(index-1)\\n        \\n    def get_previous(self):\\n        if self.has_previous():\\n            return self.Block[self.get_index()-1]\\n        return RawLayer()\\n        \\n    def get_header(self):\\n        if self.has_previous():\\n            index = self.get_index()\\n            i = index - 1\\n            while i >= 0:\\n                if self.Block[i].hierarchy == self.hierarchy-1:\\n                    return self.Block[i]\\n                else:\\n                    i -= 1\\n        return RawLayer()\\n    \\n    def get_payload(self):\\n        # return a Block, not a Layer like other methods \\n        # for management into a Block\\n        pay = Block('pay')\\n        if self.has_next():\\n            index = self.get_index()\\n            for l in self.Block[ index+1 : ]:\\n                if l.hierarchy > self.hierarchy:\\n                    #pay.append( l.clone() )\\n                    # not needed to append a clone\\n                    # better keep reference to original layer\\n                    pay.append( l )\\n                else:\\n                    break\\n            if pay.num() == 0:\\n                pay.append( RawLayer() )\\n            return pay\\n        pay.append( RawLayer() )\\n        return pay\\n    \\n    def num(self):\\n        return 1\\n    \\n    # this is to retrieve full Layer's dynamicity from a mapped layer\\n    def reautomatize(self):\\n        for e in self:\\n            if hasattr(e, 'reautomatize'):\\n                e.reautomatize()\\n    \\n    def parse(self, s=''):\\n        self.map(s)\\n    \\n    # shar manipulation interface\\n    def to_shar(self):\\n        if hasattr(self, 'Trans') and self.Trans:\\n            return shar()\\n        bits = []\\n        for e in self:\\n            bits.append( e.to_shar().get_bits() )\\n        s = shar()\\n        s.set_bits( bits )\\n        return s\\n    \\n    def map_shar(self, sh):\\n        if self.dbg >= DBG:\\n            log(DBG, '(Layer.map) entering map() for %s' % self.CallName)\\n        # First take care of transparent Layer (e.g. in L3Mobile)\\n        if hasattr(self, 'Trans') and self.Trans:\\n            return\\n        if not isinstance(sh, shar):\\n            sh = shar(sh)\\n        for e in self:\\n            e.map_shar(sh)\"}, {\"identifier\":\"Block\", \"path\":\"libmich/core/element.py\", \"snippet\":\"class Block(object):\\n    '''\\n    class to build a block composed of \\\"Layer\\\" objects\\n    define only methods, not content, which depends of the protocol or data model.\\n    \\n    when instantiated:\\n    defines a layerList;\\n    has methods to append, remove, extend, insert... layers into the block;\\n    provides several methods for calling layers in the block;\\n    to manage their hierarchy...;\\n    and use of signs such as >> << | to include layers into the block;\\n    \\n    have also same methods as \\\"Layer\\\": map, show, clone\\n    to emulate a common handling.\\n    '''\\n    # debugging thresholf for Block:\\n    dbg = 0\\n    \\n    def __init__(self, Name=''):\\n        if type(Name) is not str:\\n            raise AttributeError('CallName must be a string')\\n        self.CallName = Name\\n        self.layerList = []\\n        self.set_hierarchy(0)\\n        self.inBlock = False\\n\\n    # define some basic list facilities for managing layers into the Block:\\n    def __iter__(self):\\n        if 'layerList' in self.__dict__.keys():\\n            return self.__dict__['layerList'].__iter__()\\n        else: return [].__iter__()\\n    \\n    def __getitem__(self, num):\\n        return self.layerList[num]\\n    \\n    def __getattr__(self, attr):\\n        for l in self:\\n            if attr == l.CallName: return l\\n        return object.__getattribute__(self, attr)\\n    \\n    def num(self):\\n        return len(self.layerList)\\n    \\n    def has_index(self, index):\\n        if index < self.num(): return True\\n        return False\\n    \\n    def append(self, obj):\\n        #if isinstance(obj, Layer):\\n        # and the following will be great !!!\\n        # this lib starts really to amaze me!\\n        if isinstance(obj, (Layer, Block)):\\n            obj.inBlock = True\\n            # do not re-assign Layer.Block when building a payload\\n            # referring to Layer.get_payload()\\n            if self.CallName != 'pay':\\n                obj.Block = self\\n            # better keep original Layer / Block hierarchy\\n            #obj.hierarchy = self.hierarchy\\n            self.layerList.append(obj)\\n    \\n    def extend(self, block):\\n    # would need some more intelligence...\\n        if isinstance(block, Block):\\n            self.layerList.extend(block.layerList)\\n    \\n    def insert(self, index, layer):\\n        if isinstance(layer, Layer):\\n            layer.inBlock = True\\n            layer.Block = self\\n            self.layerList.insert(index, layer)\\n    \\n    def remove(self, start, stop=None):\\n        if stop is None:\\n            self.layerList.remove( self.layerList[start] )\\n        else:\\n            for i in xrange(start, stop):\\n                self.layerList.remove( self.layerList[start] )\\n    \\n    # method for Block hierarchy setting\\n    def set_hierarchy(self, hier):\\n        self.hierarchy = hier\\n        for l in self:\\n            l.set_hierarchy(l.hierarchy + hier)\\n    \\n    # method for managing all the Layers hierarchy in the Block (easy):\\n    def inc_hierarchy(self, ref=0):\\n        #self.hierarchy += 1+ref\\n        #for l in self: l.hierarchy += 1+ref\\n        self.set_hierarchy(self.hierarchy+1+ref)\\n        \\n    def dec_hierarchy(self, ref=0):\\n        #self.hierarchy -= 1-ref\\n        #for l in self: l.hierarchy -= 1-ref\\n        self.set_hierarchy(self.hierarchy-1+ref)\\n    \\n    # define operations to insert layers into a block:\\n    # OR: block | new_layer, append the new_layer with the same hierarchy \\n    # as last layer in the block\\n    def __or__(self, newLayer):\\n        self.append(newLayer)\\n        self[-1].set_hierarchy(self[-2].hierarchy)\\n        #for l in self[-1]:\\n        #    if isinstance(l, Layer):\\n        #        l.hierarchy = self[-1].hierarchy\\n    \\n    # LSHIFT: block << new_layer, append the new_layer with a higher hierarchy\\n    # than last layer in the block\\n    def __lshift__(self, newLayer):\\n        self.append(newLayer)\\n        if self.num() > 1:\\n            self[-1].inc_hierarchy( self[-2].hierarchy )\\n        else:\\n            self[-1].set_hierarchy(self.hierarchy)\\n    \\n    # RSHIFT: block >> new_layer, append the new_layer with a lower hierarchy\\n    # than last layer in the block\\n    def __rshift__(self, newLayer):\\n        self.append(newLayer)\\n        self[-1].dec_hierarchy( self[-2].hierarchy )\\n    \\n    # standard methods for common management with Layers\\n    def __str__(self):\\n        s = []\\n        for l in self:\\n            if not hasattr(self, 'Trans') or not l.Trans:\\n                s.append( str(l) )\\n        return ''.join(s)\\n    \\n    def shtr(self):\\n        return shtr(self.__str__())\\n    \\n    def __len__(self):\\n        return len(self.__str__())\\n    \\n    def __int__(self):\\n        # big endian integer representation of the string buffer\\n        if self._byte_aligned:\\n            return shtr(self).left_val(len(self)*8)\\n        else:\\n            return shtr(self).left_val(self.bit_len())\\n    \\n    def __repr__(self):\\n        s = '[[%s] ' % self.CallName\\n        for l in self:\\n            s += l.__repr__()\\n        s = s + ' [%s]]' % self.CallName\\n        return s\\n    \\n    def map_len(self):\\n        return len(self)\\n    \\n    def clone(self):\\n        clone = self.__class__()\\n        clone.CallName, clone.layerList = self.CallName, []\\n        for l in self:\\n            if isinstance(l, Layer): \\n                clone.append( l.clone() )\\n            elif self.dbg >= ERR:\\n                log(ERR, '(Block - %s) cloning not implemented for: %s' \\\\\\n                    % (self.__class__, l))\\n        return clone\\n    \\n    def show(self, with_trans=False):\\n        s = '%s[[[ %s ]]]\\\\n' % (self.hierarchy*'\\\\t', self.CallName)\\n        for l in self:\\n            s += '\\\\t'*l.hierarchy + l.show(with_trans).replace('\\\\n', '\\\\n'+'\\\\t'*l.hierarchy) + '\\\\n'\\n        return s[:-1]\\n    \\n    def map(self, string=''):\\n        s = string\\n        for l in self:\\n            if not hasattr(l, 'Trans') or not l.Trans:\\n                if hasattr(l, 'parse'):\\n                    l.parse(s)\\n                else:\\n                    l.map(s)\\n                s = s[l.map_len():]\\n    \\n    # this is to retrieve full Block's dynamicity from a parsed or mapped one\\n    def reautomatize(self):\\n        for l in self:\\n            if hasattr(l, 'reautomatize'):\\n                l.reautomatize()\"}, {\"identifier\":\"show\", \"path\":\"libmich/core/element.py\", \"snippet\":\"def show(self, with_trans=False):\\n    tr, re = '', ''\\n    if self.is_transparent():\\n        # TODO: eval the best convinience here\\n        if not with_trans:\\n            return ''\\n        tr = ' - transparent'\\n    else:\\n        tr = ''\\n    if self.ReprName != '':\\n        re = ''.join((self.ReprName, ' '))\\n    return '<%s[%s%s] : %s>' % ( re, self.CallName, tr, repr(self) )\"}, {\"identifier\":\"debug\", \"path\":\"libmich/core/element.py\", \"snippet\":\"def debug(thres, level, string):\\n    if level and level<=thres:\\n        print('[%s] %s' %(debug_level[level], string))\"}, {\"identifier\":\"log\", \"path\":\"libmich/core/element.py\", \"snippet\":\"def log(level=DBG, string=''):\\n    # if needed, can be changed to write somewhere else\\n    # will redirect all logs from the library\\n    print('[libmich %s] %s' % (debug_level[level], string))\"}, {\"identifier\":\"ERR\", \"path\":\"libmich/core/element.py\", \"snippet\":\"ERR = 1\"}, {\"identifier\":\"WNG\", \"path\":\"libmich/core/element.py\", \"snippet\":\"WNG = 2\"}, {\"identifier\":\"DBG\", \"path\":\"libmich/core/element.py\", \"snippet\":\"DBG = 3\"}]", "import_statement": "from libmich.core.element import Bit, Str, Int, Layer, Block, show, debug, \\\n    log, ERR, WNG, DBG\nfrom zlib import crc32\nfrom binascii import unhexlify, hexlify", "code": "# -*- coding: UTF-8 -*-\n#/**\n# * Software Name : libmich \n# * Version : 0.2.2\n# *\n# * Copyright © 2014. Benoit Michau. ANSSI.\n# *\n# * This program is free software: you can redistribute it and/or modify\n# * it under the terms of the GNU General Public License version 2 as published\n# * by the Free Software Foundation. \n# *\n# * This program is distributed in the hope that it will be useful,\n# * but WITHOUT ANY WARRANTY; without even the implied warranty of\n# * MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# * GNU General Public License for more details. \n# *\n# * You will find a copy of the terms and conditions of the GNU General Public\n# * License version 2 in the \"license.txt\" file or\n# * see http://www.gnu.org/licenses/ or write to the Free Software Foundation,\n# * Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301 USA\n# *\n# *--------------------------------------------------------\n# * File Name : formats/IEEE80211.py\n# * Created : 2014-04-29\n# * Authors : Benoit Michau \n# *--------------------------------------------------------\n#*/ \n\n#!/usr/bin/env python\n\n# generic imports\n#\n\n\nType_dict = {\n    0 : 'Management',\n    1 : 'Control',\n    2 : 'Data',\n    3 : 'Reserved'\n    }\nSubtypeMgt_dict = {\n    0 : 'Association request',\n    1 : 'Association response',\n    2 : 'Reassociation request',\n    3 : 'Reassociation response',\n    4 : 'Probe request',\n    5 : 'Probe response',\n    8 : 'Beacon',\n    9 : 'ATIM',\n    10 : 'Disassociation',\n    11 : 'Authentication',\n    12 : 'Deauthentication',\n    13 : 'Action',\n    }\nSubtypeCtrl_dict = {\n    8 : 'Block Ack Request',\n    9 : 'Block Ack',\n    10 : 'PS-Poll',\n    11 : 'RTS',\n    12 : 'CTS',\n    13 : 'ACK',\n    14 : 'CF-End',\n    15 : 'CF-End + CF-Ack'\n    }\nSubtypeData_dict = {\n    0 : 'Data',\n    1 : 'Data + CF-Ack',\n    2 : 'Data + CF-Poll',\n    3 : 'Data + CF-Ack + CF-Poll',\n    4 : 'Null',\n    5 : 'CF-Ack',\n    6 : 'CF-Poll',\n    7 : 'CF-Ack + CF-Poll',\n    8 : 'QoS Data',\n    9 : 'QoS Data + CF-Ack',\n    10 : 'QoS Data + CF-Poll',\n    11 : 'QoS Data + CF-Ack + CF-Poll',\n    12 : 'Qos Null',\n    14 : 'QoS CF-Poll',\n    15 : 'QoS CF-Ack + CF-Poll',\n    }\nPwrMgt_dict = {\n    0 : 'Active mode',\n    1 : 'PS mode',\n    }\nclass FrameCtrl(Layer):\n    constructorList = [\n", "next_line": "        Bit('ProtVers', Pt=0, BitLen=2, Repr='hum'),", "gold_snippet_index": 0, "id": 5, "__internal_uuid__": "df439e4d-2bdf-4498-bddd-8812ebc032bd"}
{"repo_name": "neuroo/equip", "file_path": "equip/analysis/defs.py", "context": "[{\"identifier\":\"logger\", \"path\":\"equip/utils/log.py\", \"snippet\":\"LOGGING_FMT = '%(asctime)s - %(levelname)3s] %(filename)s::%(funcName)s(%(lineno)d) - %(message)s'\\ndef removeOtherHandlers(to_keep=None):\\ndef enableLogger(to_file=None):\"}, {\"identifier\":\"ForwardDataflow\", \"path\":\"equip/analysis/dataflow/fixedpoint.py\", \"snippet\":\"class ForwardDataflow(Dataflow):\\n  def __init__(self, cfg, lattice, transfer, path_sensitive=False, start_states=None):\\n    Dataflow.__init__(self, cfg, lattice, transfer, path_sensitive)\\n    self.forward = True\"}, {\"identifier\":\"Lattice\", \"path\":\"equip/analysis/dataflow/lattice.py\", \"snippet\":\"class Lattice(object):\\n  \\\"\\\"\\\"\\n    Interface for a lattice element. Practically, we only use the semi-lattice\\n    with the join (V) operator.\\n  \\\"\\\"\\\"\\n  def __init__(self):\\n    pass\\n\\n  def init_state(self):\\n    \\\"\\\"\\\"\\n      Returns a new initial state.\\n    \\\"\\\"\\\"\\n    pass\\n\\n  def join_all(self, *states):\\n    result_state = None\\n    for state in states:\\n      if result_state is None:\\n        result_state = state\\n      else:\\n        result_state = self.join(result_state, state)\\n    return result_state\\n\\n  def join(self, state1, state2):\\n    \\\"\\\"\\\"\\n      Returns the result of the V (supremum) between the two states.\\n    \\\"\\\"\\\"\\n    pass\\n\\n  def meet_all(self, *states):\\n    result_state = None\\n    for state in states:\\n      if result_state is None:\\n        result_state = state\\n      else:\\n        result_state = self.meet(result_state, state)\\n    return result_state\\n\\n  def meet(self, state1, state2):\\n    \\\"\\\"\\\"\\n      Returns the result of the meet \\\\/ (infimum) between the two states.\\n    \\\"\\\"\\\"\\n    pass\\n\\n\\n  def lte(self, state1, state2):\\n    \\\"\\\"\\\"\\n      This is the <= operator between two lattice elements (states) as defined by:\\n        state1 <= state2 and state2 <= state1 <=> state1 == state2\\n    \\\"\\\"\\\"\\n    pass\\n\\n\\n  def top(self):\\n    \\\"\\\"\\\"\\n      The top of the lattice.\\n    \\\"\\\"\\\"\\n    pass\\n\\n  def bottom(self):\\n    \\\"\\\"\\\"\\n      The bottom of the lattice.\\n    \\\"\\\"\\\"\\n    pass\"}, {\"identifier\":\"State\", \"path\":\"equip/analysis/dataflow/state.py\", \"snippet\":\"class State(dict):\\n  \\\"\\\"\\\"\\n    State object to represent the current state for a given\\n    node of the CFG. This is just a wrapper around a ``dict`` until\\n    we switch to functional data structures.\\n  \\\"\\\"\\\"\\n\\n  def copy(self):\\n    return copy.deepcopy(self)\\n\\n  def __repr__(self):\\n    return 'State<%s>(%s)' % (hex(id(self))[-5:], dict.__repr__(self))\"}, {\"identifier\":\"Transfer\", \"path\":\"equip/analysis/dataflow/state.py\", \"snippet\":\"class Transfer(object):\\n  \\\"\\\"\\\"\\n    Interface for the transfer function used for dataflow analysis.\\n  \\\"\\\"\\\"\\n  def __init__(self):\\n    pass\\n\\n\\n  def run(self, node, input_state):\\n    \\\"\\\"\\\"\\n      Returns a new state from the evaluation of the node w.r.t. the\\n      input state.\\n\\n      :param node: The node being currently analyzed.\\n      :param input_state: The state at the beginning of the node.\\n    \\\"\\\"\\\"\\n    pass\"}, {\"identifier\":\"dict_union\", \"path\":\"equip/analysis/dataflow/utils.py\", \"snippet\":\"def dict_union(dct1, dct2):\\n  result = copy.deepcopy(dct1)\\n  for key in dct2:\\n    result[key] = dct2[key] if key not in result else result[key].union(dct2[key])\\n  return result\"}, {\"identifier\":\"dump_native_ast\", \"path\":\"equip/analysis/ast/utils.py\", \"snippet\":\"def dump_native_ast(node, annotate_fields=True, include_attributes=False, indent='  '):\\n  \\\"\\\"\\\"\\n    Dumps the AST node in a pretty-printed format.\\n    Taken from http://greentreesnakes.readthedocs.org\\n  \\\"\\\"\\\"\\n  if node is None:\\n    return \\\"<None>\\\"\\n\\n  def _format(node, level=0):\\n    if isinstance(node, AST):\\n      fields = [(a, _format(b, level)) for a, b in iter_fields(node)]\\n      if include_attributes and node._attributes:\\n        fields.extend([(a, _format(getattr(node, a), level))\\n                 for a in node._attributes])\\n      return ''.join([\\n        node.__class__.__name__,\\n        '(',\\n        ', '.join(('%s=%s' % field for field in fields)\\n               if annotate_fields else\\n               (b for a, b in fields)),\\n        ')'])\\n    elif isinstance(node, list):\\n      lines = ['[']\\n      lines.extend((indent * (level + 2) + _format(x, level + 2) + ','\\n             for x in node))\\n      if len(lines) > 1:\\n        lines.append(indent * (level + 1) + ']')\\n      else:\\n        lines[-1] += ']'\\n      return '\\\\n'.join(lines)\\n    return repr(node)\\n\\n  if not isinstance(node, AST):\\n    raise TypeError('expected AST, got %r' % node.__class__.__name__)\\n  return _format(node)\"}, {\"identifier\":\"split_assignment\", \"path\":\"equip/analysis/ast/utils.py\", \"snippet\":\"def split_assignment(stmt_node):\\n  \\\"\\\"\\\"\\n    Takes a ``Assign`` or ``AugAssign`` AST node, and split it into\\n    store expressions (expanded targets) and value.\\n  \\\"\\\"\\\"\\n  is_assign = isinstance(stmt_node, Assign)\\n  is_aug_assign = isinstance(stmt_node, AugAssign)\\n\\n  store_expr = list()\\n  load_expr = None\\n\\n  if is_assign or is_aug_assign:\\n    if is_assign:\\n      targets = stmt_node.targets\\n      if isinstance(targets, Tuple) or isinstance(targets, List):\\n        for elmt in targets.elts:\\n          store_expr.append(elmt)\\n      else:\\n        for elmt in targets:\\n          store_expr.append(elmt)\\n    else:\\n      store_expr.append(stmt_node.target)\\n\\n    # Process value\\n    load_expr = stmt_node.value\\n\\n  return store_expr, load_expr\"}, {\"identifier\":\"serialize_name_attr\", \"path\":\"equip/analysis/ast/utils.py\", \"snippet\":\"def serialize_name_attr(expr_node):\\n  \\\"\\\"\\\"\\n    Takes a ``Name`` or ``Attribute`` AST node, and serialize it into\\n    a string representation.\\n  \\\"\\\"\\\"\\n  if isinstance(expr_node, Name):\\n    return expr_node.id\\n  elif isinstance(expr_node, Attribute):\\n    # Walk back to the non-Attribute and '.' join the notation\\n    path = []\\n    cur = expr_node\\n    while True:\\n      if isinstance(cur, Attribute):\\n        path.insert(0, cur.attr)\\n        cur = cur.value\\n      elif isinstance(cur, Name):\\n        path.insert(0, cur.id)\\n        break\\n      elif isinstance(cur, Subscript) or isinstance(cur, Slice) or isinstance(cur, ExtSlice):\\n        cur = cur.value\\n      else:\\n        logger.error(\\\"Not handled node: %s\\\", cur)\\n    return '.'.join(path)\"}, {\"identifier\":\"named_expr_iterator\", \"path\":\"equip/analysis/ast/utils.py\", \"snippet\":\"def named_expr_iterator(node):\\n  \\\"\\\"\\\"\\n    Returns a generator over the nested named expression (``Name`` or ``Attribute``).\\n  \\\"\\\"\\\"\\n  if isinstance(node, Name) or isinstance(node, Attribute):\\n    yield node\\n  elif isinstance(node, AST):\\n    for attr_name, attr_value in iter_fields(node):\\n      for na in named_expr_iterator(attr_value):\\n        yield na\\n  elif isinstance(node, list):\\n    for n in node:\\n      for na in named_expr_iterator(n):\\n        yield na\"}, {\"identifier\":\"DiGraph\", \"path\":\"equip/analysis/graph/graphs.py\", \"snippet\":\"class DiGraph(object):\\n  \\\"\\\"\\\"\\n    A simple directed-graph structure.\\n  \\\"\\\"\\\"\\n\\n  def __init__(self, multiple_edges=True):\\n    self._multiple_edges = multiple_edges\\n    self._nodes = set()\\n    self._edges = set()\\n    self._in = {} # dest -> source : set(edges)\\n    self._out = {} # source -> dest : set(edges)\\n    self._immutable = False\\n    self._imm_in = None # src -> set(in edges)\\n    self._imm_out = None # src -> set(out edges)\\n\\n  @property\\n  def multiple_edges(self):\\n    return self._multiple_edges\\n\\n  @multiple_edges.setter\\n  def multiple_edges(self, value):\\n    self._multiple_edges = value\\n\\n  @property\\n  def immutable(self):\\n    return self._immutable\\n\\n  @immutable.setter\\n  def immutable(self, value):\\n    self._immutable = value\\n\\n  @property\\n  def nodes(self):\\n    return self._nodes\\n\\n  @property\\n  def edges(self):\\n    return self._edges\\n\\n  def add_edge(self, edge):\\n    if self.immutable:\\n      raise Exception('The graph is now immutable. Cannot add edge.')\\n    if edge in self._edges:\\n      raise Exception('Edge already present')\\n    source_node, dest_node = edge.source, edge.dest\\n\\n    if not self.multiple_edges:\\n      # If we already connected src and dest, return\\n      if source_node in self._out and dest_node in self._out[source_node]:\\n        logger.error(\\\"Already connected nodes: %s\\\", edge)\\n        return\\n      if dest_node in self._in and source_node in self._in[dest_node]:\\n        logger.error(\\\"Already connected nodes: %s\\\", edge)\\n        return\\n\\n    self._edges.add(edge)\\n    self.add_node(source_node)\\n    self.add_node(dest_node)\\n    DiGraph.__add_edge(self._out, source_node, dest_node, edge)\\n    DiGraph.__add_edge(self._in, dest_node, source_node, edge)\\n\\n  def remove_edge(self, edge):\\n    if self.immutable:\\n      raise Exception('The graph is now immutable. Cannot remove edge.')\\n    if edge not in self.edges:\\n      return\\n    source_node, dest_node = edge.source, edge.dest\\n    DiGraph.__remove_edge(self._out, source_node, dest_node, edge)\\n    DiGraph.__remove_edge(self._in, dest_node, source_node, edge)\\n    self._edges.remove(edge)\\n\\n  @staticmethod\\n  def __add_edge(in_out, source, dest, edge):\\n    if source not in in_out:\\n      in_out[source] = {}\\n    if dest not in in_out[source]:\\n      in_out[source][dest] = list()\\n    in_out[source][dest].append(edge)\\n\\n  @staticmethod\\n  def __remove_edge(in_out, source, dest, edge):\\n    if source not in in_out:\\n      return\\n    if dest not in in_out[source]:\\n      return\\n    if edge in in_out[source][dest]:\\n      in_out[source][dest].remove(edge)\\n    if not in_out[source][dest]:\\n      in_out[source].pop(dest, None)\\n    if not in_out[source]:\\n      in_out.pop(source, None)\\n\\n  def has_node(self, node):\\n    return node in self.nodes\\n\\n  def add_node(self, node):\\n    if self.immutable:\\n      raise Exception('The graph is now immutable. Cannot add node.')\\n    self._nodes.add(node)\\n\\n  def remove_node(self, node):\\n    if self.immutable:\\n      raise Exception('The graph is now immutable. Cannot remove node.')\\n    if node not in self._nodes:\\n      return\\n    # Remove all edges that touched this node\\n    self._edges = set([e for e in self._edges if e.source != node and e.dest != node])\\n    # Clean up _in/_out\\n    for src in self._in:\\n      self._in[src].pop(node, None)\\n    for dest in self._out:\\n      self._out[dest].pop(node, None)\\n    if node in self._in:\\n      self._in.pop(node, None)\\n    if node in self._out:\\n      self._out.pop(node, None)\\n    # Finally remove the node\\n    self._nodes.remove(node)\\n\\n  def in_edges(self, node):\\n    if self.immutable:\\n      return self._imm_in[node]\\n    else:\\n      if not self.has_node(node) or node not in self._in:\\n        return list()\\n      return list([e for n in self._in[node] for e in self._in[node][n]])\\n\\n  def out_edges(self, node):\\n    if self.immutable:\\n      return self._imm_out[node]\\n    else:\\n      if not self.has_node(node) or node not in self._out:\\n        return list()\\n      return list([e for n in self._out[node] for e in self._out[node][n]])\\n\\n  def in_degree(self, node):\\n    return len(self.in_edges(node))\\n\\n  def out_degree(self, node):\\n    return len(self.out_edges(node))\\n\\n  def to_dot(self):\\n    from .io import DotConverter\\n    return DotConverter.process(self)\\n\\n  def roots(self):\\n    r = set()\\n    for n in self.nodes:\\n      if self.in_degree(n) < 1:\\n        r.add(n)\\n    return r\\n\\n  def freeze(self):\\n    if self.immutable:\\n      return\\n\\n    self._imm_in = {}\\n    self._imm_out = {}\\n    for node in self.nodes:\\n      self._imm_in[node] = self.in_edges(node)\\n      self._imm_out[node] = self.out_edges(node)\\n\\n    self._in = None\\n    self._out = None\\n    self.immutable = True\\n\\n  def unfreeze(self):\\n    if not self.immutable:\\n      return\\n    self._in = {}\\n    self._out = {}\\n\\n    for edge in self.edges:\\n      source_node, dest_node = edge.source, edge.dest\\n      DiGraph.__add_edge(self._out, source_node, dest_node, edge)\\n      DiGraph.__add_edge(self._in, dest_node, source_node, edge)\\n\\n    self._imm_in = None\\n    self._imm_out = None\\n    self.immutable = False\\n\\n  def copy(self):\\n    return copy.deepcopy(self)\\n\\n  # Some helpers\\n  def make_add_node(self, kind=None, data=None):\\n    if self.immutable:\\n      raise Exception('The graph is now immutable. Cannot add node.')\\n    node = DiGraph.make_node(kind=kind, data=data)\\n    self.add_node(node)\\n    return node\\n\\n  def make_add_edge(self, source=None, dest=None, kind=None, data=None):\\n    if self.immutable:\\n      raise Exception('The graph is now immutable. Cannot add edge.')\\n    edge = DiGraph.make_edge(source=source, dest=dest, kind=kind, data=data)\\n    self.add_edge(edge)\\n    return edge\\n\\n  def inverse(self):\\n    \\\"\\\"\\\"\\n      Returns a frozen copy of this graph where all edges have been inverted.\\n    \\\"\\\"\\\"\\n    new_g = DiGraph(multiple_edges=self.multiple_edges)\\n    for edge in self.edges:\\n      new_edge = copy.deepcopy(edge)\\n      new_edge.inverse()\\n      new_g.add_edge(new_edge)\\n    new_g.freeze()\\n    return new_g\\n\\n  @staticmethod\\n  def make_node(kind=None, data=None):\\n    return Node(kind=kind, data=data)\\n\\n  @staticmethod\\n  def make_edge(source=None, dest=None, kind=None, data=None):\\n    return Edge(source=source, dest=dest, kind=kind, data=data)\"}, {\"identifier\":\"Edge\", \"path\":\"equip/analysis/graph/graphs.py\", \"snippet\":\"class Edge(object):\\n  GLOBAL_COUNTER = 0\\n\\n  def __init__(self, source=None, dest=None, kind=None, data=None):\\n    Edge.GLOBAL_COUNTER += 1\\n    self._id = Edge.GLOBAL_COUNTER\\n    self._source = source\\n    self._dest = dest\\n    self._kind = kind\\n    self._data = data\\n    self._inversed = False\\n    self._weight = 1\\n\\n  @property\\n  def gid(self):\\n    return self._id\\n\\n  @property\\n  def source(self):\\n    return self._source\\n\\n  @source.setter\\n  def source(self, value):\\n    self._source = value\\n\\n  @property\\n  def dest(self):\\n    return self._dest\\n\\n  @dest.setter\\n  def dest(self, value):\\n    self._dest = value\\n\\n  @property\\n  def kind(self):\\n    return self._kind\\n\\n  @kind.setter\\n  def kind(self, value):\\n    self._kind = value\\n\\n  @property\\n  def data(self):\\n    return self._data\\n\\n  @data.setter\\n  def data(self, value):\\n    self._data = value\\n\\n  @property\\n  def weight(self):\\n    return self._weight\\n\\n  @weight.setter\\n  def weight(self, value):\\n    self._weight = value\\n\\n  @property\\n  def inversed(self):\\n    return self._inversed\\n\\n  def inverse(self):\\n    tmp = self._source\\n    self._source = self._dest\\n    self._dest = tmp\\n    self._inversed = True\\n\\n  def __ne__(self, obj):\\n    return not self == obj\\n\\n  def __eq__(self, obj):\\n    return isinstance(obj, Edge) and obj.gid == self.gid\\n\\n  def __hash__(self):\\n    return hash('edge-' + str(self.gid))\\n\\n  def __repr__(self):\\n    return 'Edge%d(src=%s, dst=%s, kind=%s, data=%s)' \\\\\\n           % (self.gid, self.source, self.dest, repr(self.kind), repr(self.data))\"}, {\"identifier\":\"Node\", \"path\":\"equip/analysis/graph/graphs.py\", \"snippet\":\"class Node(object):\\n  GLOBAL_COUNTER = 0\\n\\n  def __init__(self, kind=None, data=None):\\n    Node.GLOBAL_COUNTER += 1\\n    self._id = Node.GLOBAL_COUNTER\\n    self._kind = kind\\n    self._data = data\\n\\n  @property\\n  def gid(self):\\n    return self._id\\n\\n  @property\\n  def kind(self):\\n    return self._kind\\n\\n  @kind.setter\\n  def kind(self, value):\\n    self._kind = value\\n\\n  @property\\n  def data(self):\\n    return self._data\\n\\n  @data.setter\\n  def data(self, value):\\n    self._data = value\\n\\n  def __ne__(self, obj):\\n    return not self == obj\\n\\n  def __eq__(self, obj):\\n    return isinstance(obj, Node) and obj.gid == self.gid\\n\\n  def __hash__(self):\\n    return hash('node-' + str(self.gid))\\n\\n  def __repr__(self):\\n    return 'Node%d(kind=%s, data=%s)' % (self.gid, repr(self.kind), repr(self.data))\"}, {\"identifier\":\"Walker\", \"path\":\"equip/analysis/graph/traversals.py\", \"snippet\":\"class Walker(object):\\n  \\\"\\\"\\\"\\n    Traverses edges in the graph in DFS.\\n  \\\"\\\"\\\"\\n  def __init__(self, graph, visitor, backwards=False):\\n    self._graph = graph\\n    self._visitor = visitor\\n    self._backwards = backwards\\n    self.worklist = None\\n\\n  @property\\n  def graph(self):\\n    return self._graph\\n\\n  @graph.setter\\n  def graph(self, value):\\n    self._graph = value\\n\\n  @property\\n  def visitor(self):\\n    return self._visitor\\n\\n  @visitor.setter\\n  def visitor(self, value):\\n    self._visitor = value\\n\\n  def traverse(self, root):\\n    self.worklist = []\\n    self.__run(root)\\n\\n  def __run(self, root=None):\\n    visited = set()\\n    if root is not None:\\n      self.__process(root)\\n    while self.worklist:\\n      current = self.worklist.pop(0)\\n      if current in visited:\\n        continue\\n      self.__process(current)\\n      visited.add(current)\\n\\n  def __process(self, current):\\n    cur_node = None\\n    if isinstance(current, Edge):\\n      cur_node = current.dest if not self._backwards else current.source\\n      self.visitor.visit(current)\\n    else:\\n      cur_node = current\\n\\n    list_edges = self.graph.out_edges(cur_node)     \\\\\\n                 if not self._backwards             \\\\\\n                 else self.graph.in_edges(cur_node)\\n    for edge in list_edges:\\n      self.worklist.insert(0, edge)\"}, {\"identifier\":\"EdgeVisitor\", \"path\":\"equip/analysis/graph/traversals.py\", \"snippet\":\"class EdgeVisitor(object):\\n  def __init__(self):\\n    pass\\n\\n  def visit(self, edge):\\n    pass\"}, {\"identifier\":\"show_bytecode\", \"path\":\"equip/bytecode/utils.py\", \"snippet\":\"def show_bytecode(bytecode, start=0, end=2**32):\\n  from ..analysis.python.effects import get_stack_effect\\n\\n  if bytecode is None:\\n    return ''\\n  buffer = []\\n  j = start\\n  end = min(end, len(bytecode) - 1)\\n  while j <= end:\\n    index, lineno, op, arg, _, co = bytecode[j]\\n    uid = hex(id(co))[-5:]\\n\\n    pop_push_str = ''\\n    try:\\n      pop, push = get_stack_effect(op, arg)\\n      pop_push_str = ' (-%d +%d) ' % (pop, push)\\n    except ValueError, ex:\\n      pop_push_str = '         '\\n\\n    if op >= opcode.HAVE_ARGUMENT:\\n      rts = repr(arg)\\n      if len(rts) > 40:\\n        rts = rts[:40] + '[...]'\\n      jump_target = ''\\n      if op in opcode.hasjrel or op in opcode.hasjabs:\\n        jump_address = arg if op in opcode.hasjabs else index + arg + 3\\n        jump_target = ' -------------> (%4d)' % jump_address\\n\\n      buffer.append(\\\"[%5s]%4d(%4d) %20s(%3d)%s (%s)%s\\\"\\n                    % (uid, lineno, index, opcode.opname[op], op, pop_push_str, rts, jump_target))\\n    else:\\n      buffer.append(\\\"[%5s]%4d(%4d) %20s(%3d)%s\\\"\\n                    % (uid, lineno, index, opcode.opname[op], op, pop_push_str))\\n    j += 1\\n  return '\\\\n'.join(buffer)\"}]", "import_statement": "import opcode\nimport _ast\nimport copy\nfrom operator import itemgetter\nfrom ..utils.log import logger\nfrom .dataflow import ForwardDataflow, \\\n                      Lattice,         \\\n                      State,           \\\n                      Transfer\nfrom .dataflow.utils import dict_union\nfrom .ast.utils import dump_native_ast,     \\\n                       split_assignment,    \\\n                       serialize_name_attr, \\\n                       named_expr_iterator\nfrom .graph import DiGraph, Edge, Node, Walker, EdgeVisitor\nfrom ..bytecode.utils import show_bytecode\nfrom .python.opcodes import *", "code": "# -*- coding: utf-8 -*-\n\"\"\"\n  equip.analysis.defs\n  ~~~~~~~~~~~~~~~~~~~\n\n  A simple implementation of the DefUse dataflow algorithm.\n\n  :copyright: (c) 2014 by Romain Gaucher (@rgaucher)\n  :license: Apache 2, see LICENSE for more details.\n\"\"\"\n\n\nGEN = 0x1\nUSE = 0x2\nKILL = 0x4\nGEN_USE = (GEN, USE, KILL)\n\n\nclass DefLattice(Lattice):\n  \"\"\"\n    The lattice being used for def-use analysis.\n  \"\"\"\n  def __init__(self):\n    Lattice.__init__(self)\n\n\n  def init_state(self):\n    return State({GEN: {}, USE: {}, KILL: {}})\n\n\n  def join(self, state1, state2):\n    result_state = state1.copy()\n    for guk in GEN_USE:\n      result_state[guk] = dict_union(state1[guk], state2[guk])\n    return result_state\n\n\n  def meet(self, state1, state2):\n    raise Exception()\n\n\nclass DefTransfer(Transfer):\n  \"\"\"\n    The transfer function for def-use analysis.\n  \"\"\"\n  def __init__(self):\n    Transfer.__init__(self)\n    self.length = -1\n\n\n  def run(self, node, input_state):\n    result_state = input_state.copy()\n    block = node.data\n\n    for stmt in block.statements:\n      native = stmt.native\n      if not native:\n        continue\n      if isinstance(native, _ast.Assign) or isinstance(native, _ast.AugAssign):\n        self.transfer_assign(result_state, native, stmt.start_bytecode_index)\n      else:\n        self.transfer_load(result_state, native, stmt.start_bytecode_index)\n\n    return result_state\n\n\n  @staticmethod\n  def update_gen_kill(stmt_state, var, index):\n    if var == 'None':\n      return\n    if var not in stmt_state[GEN]:\n      stmt_state[GEN][var] = set()\n    else:\n      # Generate a kill for the previous GENs\n      if var not in stmt_state[KILL]:\n        stmt_state[KILL][var] = set(stmt_state[GEN][var])\n      else:\n        stmt_state[KILL][var] = stmt_state[KILL][var].union(stmt_state[GEN][var])\n      stmt_state[GEN][var] = set()\n\n    stmt_state[GEN][var].add(index)\n\n\n  @staticmethod\n  def update_use(stmt_state, var, index):\n    if var not in stmt_state[USE]:\n      stmt_state[USE][var] = set()\n    stmt_state[USE][var].add(index)\n\n\n  # Assign: a <- b\n  def transfer_assign(self, stmt_state, native_stmt, stmt_index):\n    stores, _ = split_assignment(native_stmt)\n    for store_expr in stores:\n", "next_line": "      store_name = serialize_name_attr(store_expr)", "gold_snippet_index": 8, "id": 6, "__internal_uuid__": "ade6b562-44cc-4a57-8f83-52d03b3d2688"}
{"repo_name": "geodynamics/burnman", "file_path": "burnman/classes/solution.py", "context": "[{\"identifier\":\"Mineral\", \"path\":\"burnman/classes/mineral.py\", \"snippet\":\"class Mineral(Material):\\n\\n    \\\"\\\"\\\"\\n    This is the base class for all minerals. States of the mineral\\n    can only be queried after setting the pressure and temperature\\n    using set_state(). The method for computing properties of\\n    the material is set using set_method(). This is done during\\n    initialisation if the param 'equation_of_state' has been defined.\\n    The method can be overridden later by the user.\\n\\n    This class is available as ``burnman.Mineral``.\\n\\n    If deriving from this class, set the properties in self.params\\n    to the desired values. For more complicated materials you\\n    can overwrite set_state(), change the params and then call\\n    set_state() from this class.\\n\\n    All the material parameters are expected to be in plain SI units.  This\\n    means that the elastic moduli should be in Pascals and NOT Gigapascals,\\n    and the Debye temperature should be in K not C.  Additionally, the\\n    reference volume should be in m^3/(mol molecule) and not in unit cell\\n    volume and 'n' should be the number of atoms per molecule.  Frequently in\\n    the literature the reference volume is given in Angstrom^3 per unit cell.\\n    To convert this to m^3/(mol of molecule) you should multiply by 10^(-30) *\\n    N_a / Z, where N_a is Avogadro's number and Z is the number of formula units per\\n    unit cell. You can look up Z in many places, including www.mindat.org\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, params=None, property_modifiers=None):\\n        Material.__init__(self)\\n        if params is not None:\\n            self.params = params\\n        elif 'params' not in self.__dict__:\\n            self.params = {}\\n\\n        if property_modifiers is not None:\\n            self.property_modifiers = property_modifiers\\n        elif 'property_modifiers' not in self.__dict__:\\n            self.property_modifiers = []\\n\\n        self.method = None\\n        if 'equation_of_state' in self.params:\\n            self.set_method(self.params['equation_of_state'])\\n        if 'name' in self.params:\\n            self.name = self.params['name']\\n\\n    def set_method(self, equation_of_state):\\n        \\\"\\\"\\\"\\n        Set the equation of state to be used for this mineral.\\n        Takes a string corresponding to any of the predefined\\n        equations of state:  'bm2', 'bm3', 'mgd2', 'mgd3', 'slb2', 'slb3',\\n        'mt', 'hp_tmt', or 'cork'.  Alternatively, you can pass a user defined\\n        class which derives from the equation_of_state base class.\\n        After calling set_method(), any existing derived properties\\n        (e.g., elastic parameters or thermodynamic potentials) will be out\\n        of date, so set_state() will need to be called again.\\n        \\\"\\\"\\\"\\n\\n        if equation_of_state is None:\\n            self.method = None\\n            return\\n\\n        new_method = eos.create(equation_of_state)\\n        if self.method is not None and 'equation_of_state' in self.params:\\n            self.method = eos.create(self.params['equation_of_state'])\\n\\n        if type(new_method).__name__ == 'instance':\\n            raise Exception(\\n                \\\"Please derive your method from object (see python old style classes)\\\")\\n\\n        if ((self.method is not None\\n             and isinstance(new_method, type(self.method)) is False)):\\n\\n            # Warn user that they are changing the EoS\\n            warnings.warn('Warning, you are changing the method to '\\n                          f'{new_method.__class__.__name__} even though the '\\n                          'material is designed to be used with the method '\\n                          f'{self.method.__class__.__name__}. '\\n                          'This does not overwrite any mineral attributes',\\n                          stacklevel=2)\\n            self.reset()\\n\\n        self.method = new_method\\n\\n        # Validate the params object on the requested EOS.\\n        try:\\n            self.method.validate_parameters(self.params)\\n        except Exception as e:\\n            print(f'Mineral {self.to_string()} failed to validate parameters '\\n                  f'with message: \\\\\\\"{e.message}\\\\\\\"')\\n            raise\\n\\n        # Invalidate the cache upon resetting the method\\n        self.reset()\\n\\n    def to_string(self):\\n        \\\"\\\"\\\"\\n        Returns the name of the mineral class\\n        \\\"\\\"\\\"\\n        return \\\"'\\\" + self.__class__.__module__.replace(\\\".minlib_\\\", \\\".\\\") + \\\".\\\" + self.__class__.__name__ + \\\"'\\\"\\n\\n    def debug_print(self, indent=\\\"\\\"):\\n        print(\\\"%s%s\\\" % (indent, self.to_string()))\\n\\n    def unroll(self):\\n        return ([self], [1.0])\\n\\n    @copy_documentation(Material.set_state)\\n    def set_state(self, pressure, temperature):\\n        Material.set_state(self, pressure, temperature)\\n        self._property_modifiers = eos.property_modifiers.calculate_property_modifications(\\n            self)\\n\\n        if self.method is None:\\n            raise AttributeError(\\n                \\\"no method set for mineral, or equation_of_state given in mineral.params\\\")\\n\\n    \\\"\\\"\\\"\\n    Properties from equations of state\\n    We choose the P, T properties (e.g. Gibbs(P, T) rather than Helmholtz(V, T)),\\n    as it allows us to more easily apply corrections to the free energy\\n    \\\"\\\"\\\"\\n    @material_property\\n    @copy_documentation(Material.molar_gibbs)\\n    def molar_gibbs(self):\\n        return self.method.gibbs_free_energy(self.pressure, self.temperature, self._molar_volume_unmodified, self.params) \\\\\\n            + self._property_modifiers['G']\\n\\n    @material_property\\n    def _molar_volume_unmodified(self):\\n        return self.method.volume(self.pressure, self.temperature, self.params)\\n\\n    @material_property\\n    @copy_documentation(Material.molar_volume)\\n    def molar_volume(self):\\n        return self._molar_volume_unmodified \\\\\\n            + self._property_modifiers['dGdP']\\n\\n    @material_property\\n    @copy_documentation(Material.molar_entropy)\\n    def molar_entropy(self):\\n        return self.method.entropy(self.pressure, self.temperature, self._molar_volume_unmodified, self.params) \\\\\\n            - self._property_modifiers['dGdT']\\n\\n    @material_property\\n    @copy_documentation(Material.isothermal_bulk_modulus)\\n    def isothermal_bulk_modulus(self):\\n        K_T_orig = self.method.isothermal_bulk_modulus(\\n            self.pressure, self.temperature,\\n            self._molar_volume_unmodified, self.params)\\n\\n        return self.molar_volume \\\\\\n            / ((self._molar_volume_unmodified / K_T_orig) - self._property_modifiers['d2GdP2'])\\n\\n    @material_property\\n    @copy_documentation(Material.molar_heat_capacity_p)\\n    def molar_heat_capacity_p(self):\\n        return (self.method.molar_heat_capacity_p(self.pressure,\\n                                                  self.temperature,\\n                                                  self._molar_volume_unmodified,\\n                                                  self.params)\\n                - self.temperature * self._property_modifiers['d2GdT2'])\\n\\n    @material_property\\n    @copy_documentation(Material.thermal_expansivity)\\n    def thermal_expansivity(self):\\n        return (\\n            (self.method.thermal_expansivity(self.pressure, self.temperature,\\n                                             self._molar_volume_unmodified,\\n                                             self.params)\\n             * self._molar_volume_unmodified)\\n            + self._property_modifiers['d2GdPdT']) / self.molar_volume\\n\\n    @material_property\\n    @copy_documentation(Material.shear_modulus)\\n    def shear_modulus(self):\\n        G = self.method.shear_modulus(\\n            self.pressure, self.temperature, self._molar_volume_unmodified,\\n            self.params)\\n        if G < np.finfo('float').eps:\\n            warnings.formatwarning = lambda msg, * \\\\\\n                a: 'Warning from file \\\\'{0}\\\\', line {1}:\\\\n{2}\\\\n\\\\n'.format(\\n                    a[1], a[2], msg)\\n            warnings.warn('You are trying to calculate shear modulus for {0} when it is exactly zero. \\\\n'\\n                          'If {0} is a liquid, then you can safely ignore this warning, but consider \\\\n'\\n                          'calculating bulk modulus or bulk sound rather than Vp or Vs. \\\\n'\\n                          'If {0} is not a liquid, then shear modulus calculations for the \\\\n'\\n                          'underlying equation of state ({1}) have not been implemented, \\\\n'\\n                          'and Vp and Vs estimates will be incorrect.'.format(self.name, self.method.__class__.__name__), stacklevel=1)\\n        return G\\n\\n    \\\"\\\"\\\"\\n    Properties from mineral parameters,\\n    Legendre transformations\\n    or Maxwell relations\\n    \\\"\\\"\\\"\\n    @material_property\\n    def formula(self):\\n        \\\"\\\"\\\"\\n        Returns the chemical formula of the Mineral class\\n        \\\"\\\"\\\"\\n        if 'formula' in self.params:\\n            return self.params['formula']\\n        else:\\n            raise ValueError(\\n                'No formula parameter for mineral {0}.'.format(self.to_string))\\n\\n    @material_property\\n    @copy_documentation(Material.molar_mass)\\n    def molar_mass(self):\\n        if 'molar_mass' in self.params:\\n            return self.params['molar_mass']\\n        else:\\n            raise ValueError(\\n                'No molar_mass parameter for mineral {0}.'.format(self.to_string))\\n\\n    @material_property\\n    @copy_documentation(Material.density)\\n    def density(self):\\n        return self.molar_mass / self.molar_volume\\n\\n    @material_property\\n    @copy_documentation(Material.molar_internal_energy)\\n    def molar_internal_energy(self):\\n        return self.molar_gibbs - self.pressure * self.molar_volume + self.temperature * self.molar_entropy\\n\\n    @material_property\\n    @copy_documentation(Material.molar_helmholtz)\\n    def molar_helmholtz(self):\\n        return self.molar_gibbs - self.pressure * self.molar_volume\\n\\n    @material_property\\n    @copy_documentation(Material.molar_enthalpy)\\n    def molar_enthalpy(self):\\n        return self.molar_gibbs + self.temperature * self.molar_entropy\\n\\n    @material_property\\n    @copy_documentation(Material.adiabatic_bulk_modulus)\\n    def adiabatic_bulk_modulus(self):\\n        if self.temperature < 1.e-10:\\n            return self.isothermal_bulk_modulus\\n        else:\\n            return self.isothermal_bulk_modulus * self.molar_heat_capacity_p / self.molar_heat_capacity_v\\n\\n    @material_property\\n    @copy_documentation(Material.isothermal_compressibility)\\n    def isothermal_compressibility(self):\\n        return 1. / self.isothermal_bulk_modulus\\n\\n    @material_property\\n    @copy_documentation(Material.adiabatic_compressibility)\\n    def adiabatic_compressibility(self):\\n        return 1. / self.adiabatic_bulk_modulus\\n\\n    @material_property\\n    @copy_documentation(Material.p_wave_velocity)\\n    def p_wave_velocity(self):\\n        return np.sqrt((self.adiabatic_bulk_modulus\\n                        + 4. / 3. * self.shear_modulus) / self.density)\\n\\n    @material_property\\n    @copy_documentation(Material.bulk_sound_velocity)\\n    def bulk_sound_velocity(self):\\n        return np.sqrt(self.adiabatic_bulk_modulus / self.density)\\n\\n    @material_property\\n    @copy_documentation(Material.shear_wave_velocity)\\n    def shear_wave_velocity(self):\\n        return np.sqrt(self.shear_modulus / self.density)\\n\\n    @material_property\\n    @copy_documentation(Material.grueneisen_parameter)\\n    def grueneisen_parameter(self):\\n        eps = np.finfo('float').eps\\n        if np.abs(self.molar_heat_capacity_v) > eps:\\n\\n            return (self.thermal_expansivity\\n                    * self.isothermal_bulk_modulus\\n                    * self.molar_volume\\n                    / self.molar_heat_capacity_v)\\n        elif ((np.abs(self._property_modifiers['d2GdPdT']) < eps)\\n              and (np.abs(self._property_modifiers['d2GdP2']) < eps)\\n              and (np.abs(self._property_modifiers['dGdP']) < eps)\\n              and (np.abs(self._property_modifiers['d2GdT2']) < eps)):\\n\\n            return self.method.grueneisen_parameter(self.pressure, self.temperature,\\n                                                    self.molar_volume, self.params)\\n        else:\\n            raise Exception(\\n                'You are trying to calculate the grueneisen parameter at a temperature where the heat capacity is very low and where you have defined Gibbs property modifiers.')\\n\\n    @material_property\\n    @copy_documentation(Material.molar_heat_capacity_v)\\n    def molar_heat_capacity_v(self):\\n        return self.molar_heat_capacity_p - self.molar_volume * self.temperature \\\\\\n            * self.thermal_expansivity * self.thermal_expansivity \\\\\\n            * self.isothermal_bulk_modulus\"}, {\"identifier\":\"SolutionModel\", \"path\":\"burnman/classes/solutionmodel.py\", \"snippet\":\"class SolutionModel(object):\\n\\n    \\\"\\\"\\\"\\n    This is the base class for a solution model,  intended for use\\n    in defining solutions and performing thermodynamic calculations\\n    on them.  All minerals of type :class:`burnman.Solution` use\\n    a solution model for defining how the endmembers in the solution\\n    interact.\\n\\n    A user wanting a new solution model should define the functions included\\n    in the base class. All of the functions in the base class return zero,\\n    so if the user-defined solution model does not implement them,\\n    they essentially have no effect, and the Gibbs free energy and molar\\n    volume of a solution will be equal to the weighted arithmetic\\n    averages of the different endmember values.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self):\\n        \\\"\\\"\\\"\\n        Does nothing.\\n        \\\"\\\"\\\"\\n        pass\\n\\n    def excess_gibbs_free_energy(self, pressure, temperature, molar_fractions):\\n        \\\"\\\"\\\"\\n        Given a list of molar fractions of different phases,\\n        compute the excess Gibbs free energy of the solution.\\n        The base class implementation assumes that the excess gibbs\\n        free energy is zero.\\n\\n        Parameters\\n        ----------\\n        pressure : float\\n            Pressure at which to evaluate the solution model. [Pa]\\n\\n        temperature : float\\n            Temperature at which to evaluate the solution. [K]\\n\\n        molar_fractions : list of floats\\n            List of molar fractions of the different endmembers in solution\\n\\n        Returns\\n        -------\\n        G_excess : float\\n            The excess Gibbs free energy\\n        \\\"\\\"\\\"\\n        return np.dot(np.array(molar_fractions), self.excess_partial_gibbs_free_energies(pressure, temperature, molar_fractions))\\n\\n    def excess_volume(self, pressure, temperature, molar_fractions):\\n        \\\"\\\"\\\"\\n        Given a list of molar fractions of different phases,\\n        compute the excess volume of the solution.\\n        The base class implementation assumes that the excess volume is zero.\\n\\n        Parameters\\n        ----------\\n        pressure : float\\n            Pressure at which to evaluate the solution model. [Pa]\\n\\n        temperature : float\\n            Temperature at which to evaluate the solution. [K]\\n\\n        molar_fractions : list of floats\\n            List of molar fractions of the different endmembers in solution\\n\\n        Returns\\n        -------\\n        V_excess : float\\n            The excess volume of the solution\\n        \\\"\\\"\\\"\\n        return np.dot(molar_fractions,\\n                      self.excess_partial_volumes(pressure, temperature, molar_fractions))\\n\\n    def excess_entropy(self, pressure, temperature, molar_fractions):\\n        \\\"\\\"\\\"\\n        Given a list of molar fractions of different phases,\\n        compute the excess entropy of the solution.\\n        The base class implementation assumes that the excess entropy is zero.\\n\\n        Parameters\\n        ----------\\n        pressure : float\\n            Pressure at which to evaluate the solution model. [Pa]\\n\\n        temperature : float\\n            Temperature at which to evaluate the solution. [K]\\n\\n        molar_fractions : list of floats\\n            List of molar fractions of the different endmembers in solution\\n\\n        Returns\\n        -------\\n        S_excess : float\\n            The excess entropy of the solution\\n        \\\"\\\"\\\"\\n        return np.dot(molar_fractions,\\n                      self.excess_partial_entropies(pressure, temperature, molar_fractions))\\n\\n    def excess_enthalpy(self, pressure, temperature, molar_fractions):\\n        \\\"\\\"\\\"\\n        Given a list of molar fractions of different phases,\\n        compute the excess enthalpy of the solution.\\n        The base class implementation assumes that the excess enthalpy is zero.\\n\\n        Parameters\\n        ----------\\n        pressure : float\\n            Pressure at which to evaluate the solution model. [Pa]\\n\\n        temperature : float\\n            Temperature at which to evaluate the solution. [K]\\n\\n        molar_fractions : list of floats\\n            List of molar fractions of the different endmembers in solution\\n\\n        Returns\\n        -------\\n        H_excess : float\\n            The excess enthalpy of the solution\\n        \\\"\\\"\\\"\\n        return (self.excess_gibbs_free_energy(pressure, temperature, molar_fractions)\\n                + temperature*self.excess_entropy(pressure, temperature, molar_fractions))\\n\\n    def excess_partial_gibbs_free_energies(self, pressure, temperature, molar_fractions):\\n        \\\"\\\"\\\"\\n        Given a list of molar fractions of different phases,\\n        compute the excess Gibbs free energy for each endmember of the solution.\\n        The base class implementation assumes that the excess gibbs\\n        free energy is zero.\\n\\n        Parameters\\n        ----------\\n        pressure : float\\n            Pressure at which to evaluate the solution model. [Pa]\\n\\n        temperature : float\\n            Temperature at which to evaluate the solution. [K]\\n\\n        molar_fractions : list of floats\\n            List of molar fractions of the different endmembers in solution\\n\\n        Returns\\n        -------\\n        partial_G_excess : numpy array\\n            The excess Gibbs free energy of each endmember\\n        \\\"\\\"\\\"\\n        return np.zeros_like(molar_fractions)\\n\\n    def excess_partial_entropies(self, pressure, temperature, molar_fractions):\\n        \\\"\\\"\\\"\\n        Given a list of molar fractions of different phases,\\n        compute the excess entropy for each endmember of the solution.\\n        The base class implementation assumes that the excess entropy\\n        is zero (true for mechanical solutions).\\n\\n        Parameters\\n        ----------\\n        pressure : float\\n            Pressure at which to evaluate the solution model. [Pa]\\n\\n        temperature : float\\n            Temperature at which to evaluate the solution. [K]\\n\\n        molar_fractions : list of floats\\n            List of molar fractions of the different endmembers in solution\\n\\n        Returns\\n        -------\\n        partial_S_excess : numpy array\\n            The excess entropy of each endmember\\n        \\\"\\\"\\\"\\n        return np.zeros_like(molar_fractions)\\n\\n    def excess_partial_volumes(self, pressure, temperature, molar_fractions):\\n        \\\"\\\"\\\"\\n        Given a list of molar fractions of different phases,\\n        compute the excess volume for each endmember of the solution.\\n        The base class implementation assumes that the excess volume\\n        is zero.\\n\\n        Parameters\\n        ----------\\n        pressure : float\\n            Pressure at which to evaluate the solution model. [Pa]\\n\\n        temperature : float\\n            Temperature at which to evaluate the solution. [K]\\n\\n        molar_fractions : list of floats\\n            List of molar fractions of the different endmembers in solution\\n\\n        Returns\\n        -------\\n        partial_V_excess : numpy array\\n            The excess volume of each endmember\\n        \\\"\\\"\\\"\\n        return np.zeros_like(np.array(molar_fractions))\"}, {\"identifier\":\"MechanicalSolution\", \"path\":\"burnman/classes/solutionmodel.py\", \"snippet\":\"class MechanicalSolution (SolutionModel):\\n\\n    \\\"\\\"\\\"\\n    An extremely simple class representing a mechanical solution model.\\n    A mechanical solution experiences no interaction between endmembers.\\n    Therefore, unlike ideal solutions there is no entropy of mixing;\\n    the total gibbs free energy of the solution is equal to the\\n    dot product of the molar gibbs free energies and molar fractions\\n    of the constituent materials.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, endmembers):\\n        self.n_endmembers = len(endmembers)\\n        self.formulas = [e[1] for e in endmembers]\\n\\n    def excess_gibbs_free_energy(self, pressure, temperature, molar_fractions):\\n        return 0.\\n\\n    def excess_volume(self, pressure, temperature, molar_fractions):\\n        return 0.\\n\\n    def excess_entropy(self, pressure, temperature, molar_fractions):\\n        return 0.\\n\\n    def excess_enthalpy(self, pressure, temperature, molar_fractions):\\n        return 0.\\n\\n    def excess_partial_gibbs_free_energies(self, pressure, temperature, molar_fractions):\\n        return np.zeros_like(molar_fractions)\\n\\n    def excess_partial_volumes(self, pressure, temperature, molar_fractions):\\n        return np.zeros_like(molar_fractions)\\n\\n    def excess_partial_entropies(self, pressure, temperature, molar_fractions):\\n        return np.zeros_like(molar_fractions)\\n\\n    def activity_coefficients(self, pressure, temperature, molar_fractions):\\n        return np.ones_like(molar_fractions)\\n\\n    def activities(self, pressure, temperature, molar_fractions):\\n        return np.ones_like(molar_fractions)\"}, {\"identifier\":\"IdealSolution\", \"path\":\"burnman/classes/solutionmodel.py\", \"snippet\":\"class IdealSolution (SolutionModel):\\n\\n    \\\"\\\"\\\"\\n    A class representing an ideal solution model.\\n    Calculates the excess gibbs free energy and entropy due to configurational\\n    entropy. Excess internal energy and volume are equal to zero.\\n\\n    The multiplicity of each type of site in the structure is allowed to\\n    change linearly as a function of endmember proportions. This class\\n    is therefore equivalent to the entropic part of\\n    a Temkin-type model :cite:`Temkin1945`. \\n    \\\"\\\"\\\"\\n\\n    def __init__(self, endmembers):\\n        self.n_endmembers = len(endmembers)\\n        self.formulas = [e[1] for e in endmembers]\\n\\n        # Process solution chemistry\\n        process_solution_chemistry(self)\\n\\n        self._calculate_endmember_configurational_entropies()\\n\\n    def _calculate_endmember_configurational_entropies(self):\\n        S_conf = -(constants.gas_constant\\n                   * (self.endmember_noccupancies\\n                      * logish(self.endmember_occupancies)).sum(-1))\\n        self.endmember_configurational_entropies = S_conf\\n\\n    def excess_partial_gibbs_free_energies(self, pressure, temperature,\\n                                           molar_fractions):\\n        return self._ideal_excess_partial_gibbs(temperature, molar_fractions)\\n\\n    def excess_partial_entropies(self, pressure, temperature, molar_fractions):\\n        return self._ideal_excess_partial_entropies(temperature,\\n                                                    molar_fractions)\\n\\n    def excess_partial_volumes(self, pressure, temperature, molar_fractions):\\n        return np.zeros((self.n_endmembers))\\n\\n    def gibbs_hessian(self, pressure, temperature, molar_fractions):\\n        hess_S = self._ideal_entropy_hessian(temperature, molar_fractions)\\n        return -temperature*hess_S\\n\\n    def entropy_hessian(self, pressure, temperature, molar_fractions):\\n        hess_S = self._ideal_entropy_hessian(temperature, molar_fractions)\\n        return hess_S\\n\\n    def volume_hessian(self, pressure, temperature, molar_fractions):\\n        return np.zeros((len(molar_fractions), len(molar_fractions)))\\n\\n    def _configurational_entropy(self, molar_fractions):\\n        site_noccupancies = np.einsum('i, ij', molar_fractions,\\n                                      self.endmember_noccupancies)\\n        site_multiplicities = np.einsum('i, ij', molar_fractions,\\n                                        self.site_multiplicities)\\n        site_occupancies = (site_noccupancies\\n                            * inverseish(site_multiplicities))\\n        conf_entropy = -(constants.gas_constant\\n                         * (site_noccupancies\\n                            * logish(site_occupancies)).sum(-1))\\n        return conf_entropy\\n\\n    def _ideal_excess_partial_gibbs(self, temperature, molar_fractions):\\n        return -temperature*self._ideal_excess_partial_entropies(temperature, molar_fractions)\\n\\n    def _ideal_excess_partial_entropies(self, temperature, molar_fractions):\\n        return -constants.gas_constant * self._log_ideal_activities(molar_fractions)\\n\\n    def _ideal_entropy_hessian(self, temperature, molar_fractions):\\n        hessian = -constants.gas_constant * self._log_ideal_activity_derivatives(molar_fractions)\\n        return hessian\\n\\n    def _log_ideal_activities(self, molar_fractions):\\n        site_noccupancies = np.einsum('i, ij', molar_fractions,\\n                                      self.endmember_noccupancies)\\n        site_multiplicities = np.einsum('i, ij', molar_fractions,\\n                                        self.site_multiplicities)\\n\\n        lna = np.einsum('ij, j->i', self.endmember_noccupancies,\\n                        logish(site_noccupancies) - logish(site_multiplicities))\\n\\n        normalisation_constants = (self.endmember_configurational_entropies\\n                                   / constants.gas_constant)\\n        return lna + normalisation_constants\\n\\n    def _log_ideal_activity_derivatives(self, molar_fractions):\\n        site_noccupancies = np.einsum('i, ij', molar_fractions,\\n                                      self.endmember_noccupancies)\\n        site_multiplicities = np.einsum('i, ij', molar_fractions,\\n                                        self.site_multiplicities)\\n\\n        dlnadp = (np.einsum('pj, qj, j->pq', self.endmember_noccupancies,\\n                            self.endmember_noccupancies,\\n                            inverseish(site_noccupancies))\\n                  - np.einsum('pj, qj, j->pq', self.endmember_noccupancies,\\n                              self.site_multiplicities,\\n                              inverseish(site_multiplicities)))\\n\\n        return dlnadp\\n\\n    def _ideal_activities(self, molar_fractions):\\n        return _ideal_activities_fct(molar_fractions,\\n                                     self.endmember_noccupancies,\\n                                     self.n_endmembers,\\n                                     self.n_occupancies,\\n                                     self.site_multiplicities,\\n                                     self.endmember_configurational_entropies)\\n\\n    def activity_coefficients(self, pressure, temperature, molar_fractions):\\n        return np.ones_like(molar_fractions)\\n\\n    def activities(self, pressure, temperature, molar_fractions):\\n        return self._ideal_activities(molar_fractions)\"}, {\"identifier\":\"SymmetricRegularSolution\", \"path\":\"burnman/classes/solutionmodel.py\", \"snippet\":\"class SymmetricRegularSolution (AsymmetricRegularSolution):\\n\\n    \\\"\\\"\\\"\\n    Solution model implementing the symmetric regular solution model.\\n    This is a special case of the\\n    :class:`burnman.solutionmodel.AsymmetricRegularSolution` class.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, endmembers, energy_interaction, volume_interaction=None, entropy_interaction=None):\\n        alphas = np.ones(len(endmembers))\\n        AsymmetricRegularSolution.__init__(\\n            self, endmembers, alphas, energy_interaction, volume_interaction, entropy_interaction)\"}, {\"identifier\":\"AsymmetricRegularSolution\", \"path\":\"burnman/classes/solutionmodel.py\", \"snippet\":\"class AsymmetricRegularSolution (IdealSolution):\\n\\n    \\\"\\\"\\\"\\n    Solution model implementing the asymmetric regular solution model\\n    formulation as described in :cite:`HP2003`.\\n\\n    The excess nonconfigurational Gibbs energy is given by the\\n    expression:\\n\\n    .. math::\\n        \\\\\\\\mathcal{G}_{\\\\\\\\textrm{excess}} = \\\\\\\\alpha^T p (\\\\\\\\phi^T W \\\\\\\\phi)\\n\\n    :math:`\\\\\\\\alpha` is a vector of van Laar parameters governing asymmetry\\n    in the excess properties.\\n\\n    .. math::\\n        \\\\\\\\phi_i = \\\\\\\\frac{\\\\\\\\alpha_i p_i}{\\\\\\\\sum_{k=1}^{n} \\\\\\\\alpha_k p_k},\\n        W_{ij} = \\\\\\\\frac{2 w_{ij}}{\\\\\\\\alpha_i + \\\\\\\\alpha_j} \\\\\\\\textrm{for i<j}\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, endmembers, alphas, energy_interaction,\\n                 volume_interaction=None, entropy_interaction=None):\\n\\n        self.n_endmembers = len(endmembers)\\n\\n        # Create array of van Laar parameters\\n        self.alphas = np.array(alphas)\\n\\n        # Create 2D arrays of interaction parameters\\n        self.We = np.triu(2. / (self.alphas[:, np.newaxis] + self.alphas), 1)\\n        self.We[np.triu_indices(self.n_endmembers, 1)] *= np.array([i for row in energy_interaction\\n                                                                    for i in row])\\n\\n        if entropy_interaction is not None:\\n            self.Ws = np.triu(2. / (self.alphas[:, np.newaxis] + self.alphas), 1)\\n            self.Ws[np.triu_indices(self.n_endmembers, 1)] *= np.array([i for row in entropy_interaction\\n                                                                        for i in row])\\n        else:\\n            self.Ws = np.zeros((self.n_endmembers, self.n_endmembers))\\n\\n        if volume_interaction is not None:\\n            self.Wv = np.triu(2. / (self.alphas[:, np.newaxis] + self.alphas), 1)\\n            self.Wv[np.triu_indices(self.n_endmembers, 1)] *= np.array([i for row in volume_interaction\\n                                                                        for i in row])\\n        else:\\n            self.Wv = np.zeros((self.n_endmembers, self.n_endmembers))\\n\\n        # initialize ideal solution model\\n        IdealSolution.__init__(self, endmembers)\\n\\n    def _phi(self, molar_fractions):\\n        phi = self.alphas * molar_fractions\\n        phi = np.divide(phi, np.sum(phi))\\n        return phi\\n\\n    def _non_ideal_interactions(self, W, molar_fractions):\\n        # -sum(sum(qi.qj.Wij*)\\n        # equation (2) of Holland and Powell 2003\\n        phi = self._phi(molar_fractions)\\n        return _non_ideal_interactions_fct(phi, np.array(molar_fractions), self.n_endmembers, self.alphas, W)\\n\\n    def _non_ideal_excess_partial_gibbs(self, pressure, temperature, molar_fractions):\\n        Eint = self._non_ideal_interactions(self.We, molar_fractions)\\n        Sint = self._non_ideal_interactions(self.Ws, molar_fractions)\\n        Vint = self._non_ideal_interactions(self.Wv, molar_fractions)\\n        return Eint - temperature * Sint + pressure * Vint\\n\\n    def excess_partial_gibbs_free_energies(self, pressure, temperature, molar_fractions):\\n        ideal_gibbs = IdealSolution._ideal_excess_partial_gibbs(\\n            self, temperature, molar_fractions)\\n        non_ideal_gibbs = self._non_ideal_excess_partial_gibbs(\\n            pressure, temperature, molar_fractions)\\n        return ideal_gibbs + non_ideal_gibbs\\n\\n    def excess_partial_entropies(self, pressure, temperature, molar_fractions):\\n        ideal_entropies = IdealSolution._ideal_excess_partial_entropies(\\n            self, temperature, molar_fractions)\\n        non_ideal_entropies = self._non_ideal_interactions(self.Ws, molar_fractions)\\n        return ideal_entropies + non_ideal_entropies\\n\\n    def excess_partial_volumes(self, pressure, temperature, molar_fractions):\\n        return self._non_ideal_interactions(self.Wv, molar_fractions)\\n\\n    def gibbs_hessian(self, pressure, temperature, molar_fractions):\\n        ideal_entropy_hessian = IdealSolution._ideal_entropy_hessian(self, temperature, molar_fractions)\\n        phi = self._phi(molar_fractions)\\n        nonideal_gibbs_hessian = _non_ideal_hessian_fct(phi, molar_fractions,\\n                                                        self.n_endmembers, self.alphas,\\n                                                        self.We - temperature*self.Ws + pressure*self.Wv)\\n\\n        return nonideal_gibbs_hessian - temperature*ideal_entropy_hessian\\n\\n    def entropy_hessian(self, pressure, temperature, molar_fractions):\\n        ideal_entropy_hessian = IdealSolution._ideal_entropy_hessian(self, temperature, molar_fractions)\\n        phi = self._phi(molar_fractions)\\n        nonideal_entropy_hessian = _non_ideal_hessian_fct(phi, molar_fractions,\\n                                                          self.n_endmembers, self.alphas,\\n                                                          self.Ws)\\n        return ideal_entropy_hessian + nonideal_entropy_hessian\\n\\n    def volume_hessian(self, pressure, temperature, molar_fractions):\\n        phi = self._phi(molar_fractions)\\n        return _non_ideal_hessian_fct(phi, molar_fractions,\\n                                      self.n_endmembers, self.alphas,\\n                                      self.Wv)\\n\\n    def activity_coefficients(self, pressure, temperature, molar_fractions):\\n        if temperature > 1.e-10:\\n            return np.exp(self._non_ideal_excess_partial_gibbs(pressure, temperature, molar_fractions) / (constants.gas_constant * temperature))\\n        else:\\n            raise Exception(\\\"Activity coefficients not defined at 0 K.\\\")\\n\\n    def activities(self, pressure, temperature, molar_fractions):\\n        return IdealSolution.activities(self, pressure, temperature, molar_fractions) * self.activity_coefficients(pressure, temperature, molar_fractions)\"}, {\"identifier\":\"SubregularSolution\", \"path\":\"burnman/classes/solutionmodel.py\", \"snippet\":\"class SubregularSolution (IdealSolution):\\n\\n    \\\"\\\"\\\"\\n    Solution model implementing the subregular solution model formulation\\n    as described in :cite:`HW1989`. The excess conconfigurational\\n    Gibbs energy is given by the expression:\\n\\n    .. math::\\n        \\\\\\\\mathcal{G}_{\\\\\\\\textrm{excess}} = \\\\\\\\sum_i \\\\\\\\sum_{j > i} (p_i p_j^2\\n        W_{ij} + p_j p_i^2 W_{ji} + \\\\\\\\sum_{k > j > i} p_i p_j p_k W_{ijk})\\n\\n    Interaction parameters are inserted into a 3D interaction matrix during\\n    initialization to make use of numpy vector algebra.\\n\\n    Parameters\\n    ----------\\n    endmembers : list of lists\\n        A list of all the independent endmembers in the solution.\\n        The first item of each list gives the Mineral object corresponding\\n        to the endmember. The second item gives the site-species formula.\\n\\n    energy_interaction : list of list of lists\\n        The binary endmember interaction energies.\\n        Each interaction[i, j-i-1, 0] corresponds to W(i,j), while\\n        interaction[i, j-i-1, 1] corresponds to W(j,i).\\n\\n    volume_interaction : list of list of lists\\n        The binary endmember interaction volumes.\\n        Each interaction[i, j-i-1, 0] corresponds to W(i,j), while\\n        interaction[i, j-i-1, 1] corresponds to W(j,i).\\n\\n    entropy_interaction : list of list of lists\\n        The binary endmember interaction entropies.\\n        Each interaction[i, j-i-1, 0] corresponds to W(i,j), while\\n        interaction[i, j-i-1, 1] corresponds to W(j,i).\\n\\n    energy_ternary_terms : list of lists\\n        The ternary interaction energies. Each list should contain\\n        four entries: the indices i, j, k and the value of the interaction.\\n\\n    volume_ternary_terms : list of lists\\n        The ternary interaction volumes. Each list should contain\\n        four entries: the indices i, j, k and the value of the interaction.\\n\\n    entropy_ternary_terms : list of lists\\n        The ternary interaction entropies. Each list should contain\\n        four entries: the indices i, j, k and the value of the interaction.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, endmembers, energy_interaction,\\n                 volume_interaction=None, entropy_interaction=None,\\n                 energy_ternary_terms=None,\\n                 volume_ternary_terms=None, entropy_ternary_terms=None):\\n        \\\"\\\"\\\"\\n        Initialization function for the SubregularSolution class.\\n        \\\"\\\"\\\"\\n\\n        self.n_endmembers = len(endmembers)\\n\\n        # Create 3D arrays of interaction parameters\\n        self.Wijke = np.zeros(shape=(self.n_endmembers,\\n                                     self.n_endmembers,\\n                                     self.n_endmembers))\\n        self.Wijks = np.zeros_like(self.Wijke)\\n        self.Wijkv = np.zeros_like(self.Wijke)\\n\\n        # setup excess enthalpy interaction matrix\\n        for i in range(self.n_endmembers):\\n            for j in range(i + 1, self.n_endmembers):\\n                w0 = energy_interaction[i][j - i - 1][0]/2.\\n                w1 = energy_interaction[i][j - i - 1][1]/2.\\n                self.Wijke[:, i, j] += w0\\n                self.Wijke[:, j, i] += w1\\n\\n                self.Wijke[i, j, j] += w0\\n                self.Wijke[j, i, i] += w1\\n\\n                self.Wijke[i, j, i] -= w0\\n                self.Wijke[j, i, j] -= w1\\n\\n        if energy_ternary_terms is not None:\\n            for (i, j, k, v) in energy_ternary_terms:\\n                self.Wijke[i, j, k] += v\\n\\n        if entropy_interaction is not None:\\n            for i in range(self.n_endmembers):\\n                for j in range(i + 1, self.n_endmembers):\\n                    w0 = entropy_interaction[i][j - i - 1][0]/2.\\n                    w1 = entropy_interaction[i][j - i - 1][1]/2.\\n                    self.Wijks[:, i, j] += w0\\n                    self.Wijks[:, j, i] += w1\\n\\n                    self.Wijks[i, j, j] += w0\\n                    self.Wijks[j, i, i] += w1\\n\\n                    self.Wijks[i, j, i] -= w0\\n                    self.Wijks[j, i, j] -= w1\\n\\n        if entropy_ternary_terms is not None:\\n            for (i, j, k, v) in entropy_ternary_terms:\\n                self.Wijks[i, j, k] += v\\n\\n        if volume_interaction is not None:\\n            for i in range(self.n_endmembers):\\n                for j in range(i + 1, self.n_endmembers):\\n                    w0 = volume_interaction[i][j - i - 1][0]/2.\\n                    w1 = volume_interaction[i][j - i - 1][1]/2.\\n                    self.Wijkv[:, i, j] += w0\\n                    self.Wijkv[:, j, i] += w1\\n\\n                    self.Wijkv[i, j, j] += w0\\n                    self.Wijkv[j, i, i] += w1\\n\\n                    self.Wijkv[i, j, i] -= w0\\n                    self.Wijkv[j, i, j] -= w1\\n\\n        if volume_ternary_terms is not None:\\n            for (i, j, k, v) in volume_ternary_terms:\\n                self.Wijkv[i, j, k] += v\\n\\n        # initialize ideal solution model\\n        IdealSolution.__init__(self, endmembers)\\n\\n    def _non_ideal_function(self, Wijk, molar_fractions):\\n        n = len(molar_fractions)\\n        return _non_ideal_interactions_subreg(molar_fractions, n, Wijk)\\n\\n    def _non_ideal_interactions(self, molar_fractions):\\n        # equation (6') of Helffrich and Wood, 1989\\n        Eint = self._non_ideal_function(self.Wijke, molar_fractions)\\n        Sint = self._non_ideal_function(self.Wijks, molar_fractions)\\n        Vint = self._non_ideal_function(self.Wijkv, molar_fractions)\\n        return Eint, Sint, Vint\\n\\n    def _non_ideal_excess_partial_gibbs(self, pressure, temperature, molar_fractions):\\n        Eint, Sint, Vint = self._non_ideal_interactions(molar_fractions)\\n        return Eint - temperature * Sint + pressure * Vint\\n\\n    def excess_partial_gibbs_free_energies(self, pressure, temperature, molar_fractions):\\n        ideal_gibbs = IdealSolution._ideal_excess_partial_gibbs(\\n            self, temperature, molar_fractions)\\n        non_ideal_gibbs = self._non_ideal_excess_partial_gibbs(\\n            pressure, temperature, molar_fractions)\\n        return ideal_gibbs + non_ideal_gibbs\\n\\n    def excess_partial_entropies(self, pressure, temperature, molar_fractions):\\n        ideal_entropies = IdealSolution._ideal_excess_partial_entropies(\\n            self, temperature, molar_fractions)\\n        non_ideal_entropies = self._non_ideal_function(self.Wijks, molar_fractions)\\n        return ideal_entropies + non_ideal_entropies\\n\\n    def excess_partial_volumes(self, pressure, temperature, molar_fractions):\\n        non_ideal_volumes = self._non_ideal_function(self.Wijkv, molar_fractions)\\n        return non_ideal_volumes\\n\\n    def gibbs_hessian(self, pressure, temperature, molar_fractions):\\n        n = len(molar_fractions)\\n        ideal_entropy_hessian = IdealSolution._ideal_entropy_hessian(self, temperature, molar_fractions)\\n        nonideal_gibbs_hessian = _non_ideal_hessian_subreg(molar_fractions, n,\\n                                                           self.Wijke - temperature*self.Wijks\\n                                                           + pressure*self.Wijkv)\\n\\n        return nonideal_gibbs_hessian - temperature*ideal_entropy_hessian\\n\\n    def entropy_hessian(self, pressure, temperature, molar_fractions):\\n        n = len(molar_fractions)\\n        ideal_entropy_hessian = IdealSolution._ideal_entropy_hessian(self, temperature, molar_fractions)\\n        nonideal_entropy_hessian = _non_ideal_hessian_subreg(molar_fractions, n,\\n                                                             self.Wijks)\\n        return ideal_entropy_hessian + nonideal_entropy_hessian\\n\\n    def volume_hessian(self, pressure, temperature, molar_fractions):\\n        n = len(molar_fractions)\\n        return _non_ideal_hessian_subreg(molar_fractions, n, self.Wijkv)\\n\\n    def activity_coefficients(self, pressure, temperature, molar_fractions):\\n        if temperature > 1.e-10:\\n            return np.exp(self._non_ideal_excess_partial_gibbs(pressure, temperature, molar_fractions) / (constants.gas_constant * temperature))\\n        else:\\n            raise Exception(\\\"Activity coefficients not defined at 0 K.\\\")\\n\\n    def activities(self, pressure, temperature, molar_fractions):\\n        return IdealSolution.activities(self, pressure, temperature, molar_fractions) * self.activity_coefficients(pressure, temperature, molar_fractions)\"}, {\"identifier\":\"independent_row_indices\", \"path\":\"burnman/utils/reductions.py\", \"snippet\":\"def independent_row_indices(m, iszerofunc=lambda x: x.is_zero,\\n                            simpfunc=lambda x: Rational(x).limit_denominator(1000)):\\n\\n    _, pivots, swaps = row_reduce(m, iszerofunc, simpfunc)\\n    indices = np.array(range(len(m)))\\n    for swap in np.array(swaps):\\n        indices[swap] = indices[swap[::-1]]\\n    return indices[:len(pivots)]\"}, {\"identifier\":\"sum_formulae\", \"path\":\"burnman/utils/chemistry.py\", \"snippet\":\"def sum_formulae(formulae, amounts=None):\\n    \\\"\\\"\\\"\\n    Adds together a set of formulae.\\n\\n    Parameters\\n    ----------\\n    formulae : list of dictionary or counter objects\\n        List of chemical formulae\\n    amounts : list of floats\\n        List of amounts of each formula\\n\\n    Returns\\n    -------\\n    summed_formula : Counter object\\n        The sum of the user-provided formulae\\n    \\\"\\\"\\\"\\n    if amounts is None:\\n        amounts = [1. for formula in formulae]\\n    else:\\n        assert (len(formulae) == len(amounts))\\n\\n    summed_formula = Counter()\\n    for i, formula in enumerate(formulae):\\n        summed_formula.update(Counter({element: amounts[i] * n_atoms\\n                                       for (element, n_atoms)\\n                                       in formula.items()}))\\n    return summed_formula\"}, {\"identifier\":\"sort_element_list_to_IUPAC_order\", \"path\":\"burnman/utils/chemistry.py\", \"snippet\":\"def sort_element_list_to_IUPAC_order(element_list):\\n    \\\"\\\"\\\"\\n    Parameters\\n    ----------\\n    element_list : list\\n        List of elements\\n\\n    Returns\\n    -------\\n    sorted_list : list\\n        List of elements sorted into IUPAC order\\n    \\\"\\\"\\\"\\n    sorted_list = [e for e in IUPAC_element_order if e in element_list]\\n    assert len(sorted_list) == len(element_list)\\n    return sorted_list\"}]", "import_statement": "import numpy as np\nfrom sympy import Matrix, nsimplify\nfrom .material import material_property, cached_property\nfrom .mineral import Mineral\nfrom .solutionmodel import SolutionModel\nfrom .solutionmodel import MechanicalSolution, IdealSolution\nfrom .solutionmodel import SymmetricRegularSolution, AsymmetricRegularSolution\nfrom .solutionmodel import SubregularSolution\nfrom .averaging_schemes import reuss_average_function\nfrom ..utils.reductions import independent_row_indices\nfrom ..utils.chemistry import sum_formulae, sort_element_list_to_IUPAC_order", "code": "# This file is part of BurnMan - a thermoelastic and thermodynamic toolkit for\n# the Earth and Planetary Sciences\n# Copyright (C) 2012 - 2022 by the BurnMan team, released under the GNU\n# GPL v2 or later.\n\n\nfrom __future__ import absolute_import\n\n\n\n\nclass Solution(Mineral):\n    \"\"\"\n    This is the base class for all solutions.\n    Site occupancies, endmember activities and the constant\n    and pressure and temperature dependencies of the excess\n    properties can be queried after using set_composition().\n    States of the solution can only be queried after setting\n    the pressure, temperature and composition using set_state().\n\n    This class is available as :class:`burnman.Solution`.\n    It uses an instance of :class:`burnman.SolutionModel` to\n    calculate interaction terms between endmembers.\n\n    All the solution parameters are expected to be in SI units.  This\n    means that the interaction parameters should be in J/mol, with the T\n    and P derivatives in J/K/mol and m^3/mol.\n\n    The parameters are relevant to all solution models. Please\n    see the documentation for individual models for details about\n    other parameters.\n\n    Parameters\n    ----------\n    name : string\n        Name of the solution\n    solution_type : string\n        String determining which SolutionModel to use. One of 'mechanical',\n        'ideal', 'symmetric', 'asymmetric' or 'subregular'.\n    endmembers : list of lists\n        List of endmembers in this solution. The first item of each\n        list should be a :class:`burnman.Mineral` object. The second item\n        should be a string with the site formula of the endmember.\n    molar_fractions : numpy array (optional)\n        The molar fractions of each endmember in the solution.\n        Can be reset using the set_composition() method.\n    \"\"\"\n\n    def __init__(self,\n                 name=None,\n                 solution_type=None,\n                 endmembers=None,\n                 energy_interaction=None,\n                 volume_interaction=None,\n                 entropy_interaction=None,\n                 energy_ternary_terms=None,\n                 volume_ternary_terms=None,\n                 entropy_ternary_terms=None,\n                 alphas=None,\n                 molar_fractions=None):\n        \"\"\"\n        Set up matrices to speed up calculations for when P, T, X is defined.\n        \"\"\"\n        Mineral.__init__(self)\n\n        # Solution needs a method attribute to call Mineral.set_state().\n        # Note that set_method() below will not change self.method\n        self.method = 'SolutionMethod'\n\n        if name is not None:\n            self.name = name\n        if solution_type is not None:\n            self.solution_type = solution_type\n        if endmembers is not None:\n            self.endmembers = endmembers\n        if energy_interaction is not None:\n            self.energy_interaction = energy_interaction\n        if volume_interaction is not None:\n            self.volume_interaction = volume_interaction\n        if entropy_interaction is not None:\n            self.entropy_interaction = entropy_interaction\n        if energy_ternary_terms is not None:\n            self.energy_ternary_terms = energy_ternary_terms\n        if volume_ternary_terms is not None:\n            self.volume_ternary_terms = volume_ternary_terms\n        if entropy_ternary_terms is not None:\n            self.entropy_ternary_terms = entropy_ternary_terms\n        if alphas is not None:\n            self.alphas = alphas\n        if endmembers is not None:\n            self.endmembers = endmembers\n\n        if hasattr(self, 'endmembers') is False:\n            raise Exception(\"'endmembers' attribute missing \"\n                            \"from solution\")\n\n        # Set default solution model type\n        if hasattr(self, 'solution_type'):\n            if self.solution_type == 'mechanical':\n                self.solution_model = MechanicalSolution(self.endmembers)\n            elif self.solution_type == 'ideal':\n                self.solution_model = IdealSolution(self.endmembers)\n            else:\n                if hasattr(self, 'energy_interaction') is False:\n                    self.energy_interaction = None\n                if hasattr(self, 'volume_interaction') is False:\n                    self.volume_interaction = None\n                if hasattr(self, 'entropy_interaction') is False:\n                    self.entropy_interaction = None\n\n                if self.solution_type == 'symmetric':\n", "next_line": "                    self.solution_model = SymmetricRegularSolution(", "gold_snippet_index": 4, "id": 7, "__internal_uuid__": "4a2d1987-b4f3-41f4-9192-48a7fc3895fd"}
{"repo_name": "TrainingB/Clembot", "file_path": "clembot/exts/config/cogs/config_cog.py", "context": "[{\"identifier\":\"Icons\", \"path\":\"clembot/config/constants.py\", \"snippet\":\"class Icons:\\n\\n    raid_report = \\\"https://i.imgur.com/uRhgISs.png\\\"\\n    configure = \\\"https://i.imgur.com/nPyXbkD.png\\\"\\n    configure_success = \\\"https://i.imgur.com/OBlddqw.png\\\"\\n    configure_failure = \\\"https://i.imgur.com/30rAjXD.png\\\"\\n    uptime = \\\"https://i.imgur.com/82Cqf1x.png\\\"\\n    wild_report = \\\"https://i.imgur.com/eW8sCSo.png\\\"\\n    field_research = \\\"https://raw.githubusercontent.com/TrainingB/Clembot/v1-rewrite/images/field-research.png?cache=13\\\"\\n    research_report = \\\"https://i.imgur.com/O1XNv5z.png\\\"\\n    BOT_ERROR= \\\"https://i.imgur.com/C3qZaeo.png\\\"\\n    trash = \\\"https://i.imgur.com/K6iLiPP.png\\\"\\n    error = \\\"https://i.imgur.com/dfyevnZ.png\\\"\\n\\n    INVALID_INPUT = \\\"https://i.imgur.com/Sl9Nr3g.png\\\"\\n    BOT_ERROR_2 = \\\"https://i.imgur.com/P8UEhkD.png\\\"\\n    CONFIGURATION = \\\"https://i.imgur.com/Brzu64u.png\\\"\\n    INVALID_ACCESS = \\\"https://i.imgur.com/8LOsBQS.png\\\"\\n    TIMEZONE=\\\"https://i.imgur.com/j5L85oY.png\\\"\\n\\n\\n    @staticmethod\\n    def avatar(user: discord.Member):\\n        icon_url = f\\\"https://cdn.discordapp.com/avatars/{user.id}/{user.avatar}.jpg?size=32\\\"\\n        return icon_url\"}, {\"identifier\":\"MyEmojis\", \"path\":\"clembot/config/constants.py\", \"snippet\":\"class MyEmojis:\\n\\n    DESPAWNED = '💨'\\n    ON_MY_WAY = '🏎️'\\n    TRASH = '🗑️'\\n\\n    REMOTE = f\\\"{parse_emoji(None, config_template.misc_emoji.get('remote_raid'))}\\\"\\n    INVITE = f\\\"{parse_emoji(None, config_template.misc_emoji.get('add_friend'))}\\\"\\n    HERE = f\\\"{parse_emoji(None, config_template.misc_emoji.get('here'))}\\\"\\n    COMING = f\\\"{parse_emoji(None, config_template.misc_emoji.get('coming'))}\\\"\\n    INTERESTED = f\\\"{parse_emoji(None, config_template.misc_emoji.get('interested'))}\\\"\\n    INFO = f\\\"{parse_emoji(None, config_template.misc_emoji.get('info'))}\\\"\\n    ERROR = f\\\"{parse_emoji(None, config_template.misc_emoji.get('error'))}\\\"\\n    POKE_BATTLER = f\\\"{parse_emoji(None, config_template.misc_emoji.get('pb'))}\\\"\"}, {\"identifier\":\"GUILD_METADATA_KEY\", \"path\":\"clembot/config/constants.py\", \"snippet\":\"GUILD_METADATA_KEY = [\\\"hide-nest-preview\\\", \\\"bingo-card-repo\\\", \\\"bingo-event-title\\\", \\\"bingo-event-pokemon\\\", \\\"notifications\\\"]\"}, {\"identifier\":\"GUILD_CONFIG_KEY\", \"path\":\"clembot/config/constants.py\", \"snippet\":\"GUILD_CONFIG_KEY = ['prefix', 'city', 'timezone', 'welcome', 'teams', 'config']\"}, {\"identifier\":\"GLOBAL_CONFIG_KEY\", \"path\":\"clembot/config/constants.py\", \"snippet\":\"GLOBAL_CONFIG_KEY = [\\\"bingo-event-title\\\", \\\"cache-version\\\", \\\"timezone\\\", \\\"bingo-event-pokemon\\\", \\\"next-badge-id\\\", \\\"game-master-version\\\"]\"}, {\"identifier\":\"CHANNEL_METADATA_KEY\", \"path\":\"clembot/config/constants.py\", \"snippet\":\"CHANNEL_METADATA_KEY = [\\\"city\\\"]\"}, {\"identifier\":\"checks\", \"path\":\"clembot/core/checks.py\", \"snippet\":\"class AccessDenied(CommandError):\\nasync def _check_is_owner(ctx):\\nasync def _check_is_trusted(ctx):\\nasync def _check_is_guild_owner(ctx):\\nasync def _check_is_guild_admin(ctx):\\nasync def _check_is_moderator(ctx):\\nasync def check_is_owner(ctx):\\nasync def check_is_trusted(ctx):\\nasync def check_is_guild_owner(ctx):\\nasync def check_is_guild_admin(ctx):\\nasync def check_is_moderator(ctx):\\ndef go_thru_sometimes():\\ndef is_bot_owner():\\ndef is_trusted():\\ndef is_guild_owner():\\ndef is_guild_admin():\\ndef is_guild_mod():\"}, {\"identifier\":\"group\", \"path\":\"clembot/core/bot.py\", \"snippet\":\"def group(*args, **kwargs):\\n    def decorator(func):\\n        category = kwargs.get(\\\"category\\\")\\n        func.command_category = category\\n        examples = kwargs.get(\\\"examples\\\")\\n        func.examples = examples\\n        error_wrapped_func = wrap_error(func)\\n        result = commands.group(*args, **kwargs)(error_wrapped_func)\\n        return result\\n    return decorator\"}, {\"identifier\":\"is_guild_admin\", \"path\":\"clembot/core/checks.py\", \"snippet\":\"def is_guild_admin():\\n    return commands.check(check_is_guild_admin)\"}, {\"identifier\":\"is_guild_mod\", \"path\":\"clembot/core/checks.py\", \"snippet\":\"def is_guild_mod():\\n    return commands.check(check_is_moderator)\"}, {\"identifier\":\"wrap_error\", \"path\":\"clembot/core/errors.py\", \"snippet\":\"def wrap_error(func):\\n    \\\"\\\"\\\"\\n    Decorator to handle logging of exception and sending an error message to the user.\\n    Need ctx object to send an error message to user. if ctx is not the second argument, specify which argument is ctx.\\n    \\\"\\\"\\\"\\n    @wraps(func)\\n    async def decorator(*args, **kwargs):\\n        try:\\n            return await func(*args, **kwargs)\\n\\n        except AccessDenied as aErr:\\n            ctx = next(filter(lambda arg: isinstance(arg, Context), args))\\n            header = check_failure_header[int(str(TH.current_epoch()).split(\\\".\\\")[-1]) % len(check_failure_header)]\\n\\n            error_message = await ctx.send(embed=Embeds.make_embed(\\n                header_icon=Icons.INVALID_ACCESS, msg_color=discord.Color.dark_red(),\\n                header=f\\\"{header}\\\",\\n                content=f'**Error details:** {aErr}'))\\n\\n            await error_message.add_reaction('🗑️')\\n        except BadArgument as bErr:\\n            ctx = next(filter(lambda arg: isinstance(arg, Context), args))\\n\\n            header = bad_arguments_header[int(str(TH.current_epoch()).split(\\\".\\\")[-1]) % len(bad_arguments_header)]\\n\\n            error_message = await ctx.send(embed=Embeds.make_embed(\\n                header_icon=Icons.error, msg_color=discord.Color.dark_red(),\\n                header=f\\\"{header}\\\",\\n                content=f'**Error details:** {bErr}'))\\n\\n            await error_message.add_reaction('🗑️')\\n            return\\n\\n        except ShowErrorMessage as sErr:\\n            ctx = next(filter(lambda arg: isinstance(arg, Context), args))\\n\\n            header = user_info_header[int(str(TH.current_epoch()).split(\\\".\\\")[-1]) % len(user_info_header)]\\n\\n            error_message = await ctx.send(embed=Embeds.make_embed(\\n                header_icon=Icons.BOT_ERROR_2, msg_color=discord.Color.dark_red(),\\n                header=f\\\"{header}\\\",\\n                content=f'{sErr}'))\\n\\n            await error_message.add_reaction('🗑️')\\n\\n            pass\\n        except Exception as error:\\n            print(\\\"--------------------\\\")\\n            import traceback\\n            ref_id = f\\\"E-{CUIDGenerator.cuid(int(TH.current_epoch()))}\\\"\\n            Logger.error(f\\\"{ref_id} - {traceback.format_exc()}\\\")\\n\\n            ctx = next(filter(lambda arg: isinstance(arg, Context), args))\\n\\n            if ctx:\\n                await ctx.send(embed=Embeds.make_embed(\\n                    header_icon=Icons.BOT_ERROR, msg_color=discord.Color.dark_red(),\\n                    header=error_header[int(str(TH.current_epoch()).split(\\\".\\\")[-1]) % len(error_header)],\\n                    content=f'{error_message_description[int(str(TH.current_epoch()).split(\\\".\\\")[-1]) % len(error_message_description)]}\\\\n\\\\n||**Command:** `{ctx.message.content}`\\\\n**Error:** `{error}`\\\\n**Where:** `{func.__name__}()`||',\\n                    footer=f\\\"Reference Id: {ref_id}\\\"))\\n            return None\\n\\n    return decorator\"}, {\"identifier\":\"Logger\", \"path\":\"clembot/core/logs.py\", \"snippet\":\"def init_loggers():\"}, {\"identifier\":\"ChannelMetadata\", \"path\":\"clembot/exts/config/channel_metadata.py\", \"snippet\":\"class ChannelMetadata:\\n\\n    by_channel = dict()\\n    _in_progress_config_channels = []\\n\\n    def __init__(self, bot, channel):\\n        self.bot = bot\\n        self.channel = channel\\n\\n    def __eq__(self, other):\\n        return self.channel.id == other.channel.id\\n\\n    @classmethod\\n    def begin_configuration(cls, channel_id):\\n        cls._in_progress_config_channels.append(channel_id)\\n\\n\\n    @classmethod\\n    def end_configuration(cls, channel_id):\\n        if cls.config_in_progress(channel_id):\\n            cls._in_progress_config_channels.remove(channel_id)\\n\\n    @classmethod\\n    def config_in_progress(cls, channel_id):\\n        return channel_id in cls._in_progress_config_channels\\n\\n\\n    @property\\n    def _data(self):\\n        report_channel_query = self.bot.dbi.table('channel_metadata').query()\\n        _data = report_channel_query.where(channel_id=self.channel.id)\\n        return _data\\n\\n    @classmethod\\n    def cache(cls, channel_dict):\\n        ChannelMetadata.by_channel[channel_dict.get('channel_id')] = channel_dict\\n\\n\\n    @classmethod\\n    def evict(cls, channel_id):\\n        ChannelMetadata.by_channel.pop(channel_id, None)\\n\\n\\n    @classmethod\\n    async def find(cls, bot, channel_id, guild_id = None):\\n\\n        channel_metadata = ChannelMetadata.by_channel.get(channel_id)\\n        if channel_metadata:\\n            return channel_metadata\\n\\n        report_channel_query = bot.dbi.table('channel_metadata').query()\\n        _data = report_channel_query.where(channel_id=channel_id)\\n        db_record = await _data.get()\\n\\n        if db_record:\\n            channel_metadata = ChannelMetadata.deserialize(dict(db_record[0]))\\n            ChannelMetadata.cache(channel_metadata)\\n            return channel_metadata\\n\\n        if guild_id:\\n            await ChannelMetadata.insert(bot, {'channel_id': channel_id, 'guild_id': guild_id})\\n        return ChannelMetadata.by_channel.get(channel_id)\\n\\n\\n    @classmethod\\n    async def city(cls, bot, channel_id):\\n        channel_dict = await ChannelMetadata.find(bot, channel_id)\\n        if channel_dict:\\n            return channel_dict.get('city')\\n        raise Exception(\\\"City has not been set for this channel.\\\")\\n\\n    @staticmethod\\n    def serialize(data_dict):\\n        return _.map_values(data_dict, lambda val: json.dumps(val) if isinstance(val, dict) else val)\\n\\n    @staticmethod\\n    def deserialize(data_dict):\\n        return { k :json.loads(v) if k == 'config' and v is not None else v for (k, v) in data_dict.items()}\\n\\n\\n    @classmethod\\n    async def update(cls, bot, channel_dict):\\n        channel_metadata_table = bot.dbi.table('channel_metadata')\\n        update_dict=ChannelMetadata.serialize(channel_dict)\\n        channel_metadata_table_update = channel_metadata_table.update(**update_dict).where(channel_id=channel_dict.get('channel_id'))\\n        await channel_metadata_table_update.commit()\\n        ChannelMetadata.cache(channel_dict)\\n\\n    @classmethod\\n    async def insert(cls, bot, channel_dict):\\n        channel_metadata_table = bot.dbi.table('channel_metadata')\\n        insert_dict = ChannelMetadata.serialize(channel_dict)\\n        channel_metadata_table_insert = channel_metadata_table.insert(**insert_dict)\\n        await channel_metadata_table_insert.commit()\\n        ChannelMetadata.cache(channel_dict)\\n\\n    @staticmethod\\n    def embed(ctx, channel_dict):\\n        return (ChannelConfigEmbed.from_channel_metadata(ctx, channel_dict)).embed\\n\\n    @staticmethod\\n    def success_embed(ctx, channel_dict):\\n        embed = (ChannelConfigEmbed.from_channel_metadata(ctx, channel_dict, \\\"Configuration has been updated successfully.\\\", Icons.configure_success)).embed\\n        return embed\\n\\n    @staticmethod\\n    def failure_embed(ctx, channel_dict):\\n        embed = (ChannelConfigEmbed.from_channel_metadata(ctx, channel_dict, \\\"No configuration changes done.\\\", Icons.configure_failure)).embed\\n        return embed\\n\\n    @staticmethod\\n    def profile_embed(ctx, channel_dict, title=None):\\n        return (ChannelConfigEmbed.from_channel_profile(ctx, channel_dict, title)).embed\"}, {\"identifier\":\"GlobalConfigCache\", \"path\":\"clembot/exts/config/globalconfigmanager.py\", \"snippet\":\"class GlobalConfigCache:\\n\\n\\n    by_config_name = dict()\\n\\n    def __init__(self, dbi, bot = None):\\n        self.dbi = dbi\\n        self.utilities = Utilities()\\n        self._cache = {}\\n        if bot:\\n            self.bot = bot\\n            self.bot.loop.create_task(self.load_config())\\n\\n    def get_all_config(self):\\n        return self._cache\\n\\n\\n    async def get_clembot_config(self, config_name, reload=False):\\n\\n        if reload or not self._cache:\\n            await self.load_config()\\n\\n        if config_name in self._cache.keys():\\n            config_value = self._cache.get(config_name, None)\\n            return config_value\\n\\n        return None\\n\\n    async def load_config(self):\\n\\n        Logger.info(f'load_config()')\\n\\n        cache = {}\\n        try:\\n            clembot_config_tbl = self.dbi.table('clembot_config')\\n            clembot_query = clembot_config_tbl.query().select()\\n\\n            config_records = await clembot_query.get()\\n\\n            for cr in config_records:\\n                cache[cr['config_name']] = cr['config_value']\\n                GlobalConfigCache.by_config_name[cr['config_name']] = cr['config_value']\\n\\n            self._cache.clear()\\n            self._cache.update(cache)\\n        except Exception as error:\\n            Logger.error(error)\\n        return None\\n\\n    async def save_clembot_config(self, config_name, config_value):\\n        try:\\n            print(\\\"save_clembot_config ({0}, {1})\\\".format(config_name, config_value))\\n\\n            clembot_config_record = {\\n                \\\"config_name\\\": config_name,\\n                \\\"config_value\\\": config_value\\n            }\\n            table = self.dbi.table('clembot_config')\\n\\n            existing_config_record = await table.query().select().where(config_name=config_name).get_first()\\n\\n            if existing_config_record:\\n                update_query = table.update(config_value=config_value).where(config_name=config_name)\\n                await update_query.commit()\\n            else:\\n                insert_query = table.insert(**clembot_config_record)\\n                await insert_query.commit()\\n\\n            await self.load_config()\\n\\n        except Exception as error:\\n            Logger.error(f\\\"{traceback.format_exc()}\\\")\\n\\n    @classmethod\\n    async def loadconfig(cls, bot):\\n\\n        Logger.info(f'load_config()')\\n\\n        cache = {}\\n        try:\\n            clembot_config_tbl = bot.dbi.table('clembot_config')\\n            clembot_query = clembot_config_tbl.query().select()\\n\\n            config_records = await clembot_query.get()\\n\\n            for cr in config_records:\\n                cache[cr['config_name']] = cr['config_value']\\n                GlobalConfigCache.by_config_name[cr['config_name']] = cr['config_value']\\n\\n            cls.by_config_name.clear()\\n            cls.by_config_name.update(cache)\\n        except Exception as error:\\n            Logger.error(error)\\n        return None\\n\\n\\n    @classmethod\\n    async def saveclembotconfig(cls, bot , config_name, config_value):\\n        try:\\n            print(\\\"save_clembot_config ({0}, {1})\\\".format(config_name, config_value))\\n\\n            clembot_config_record = {\\n                \\\"config_name\\\": config_name,\\n                \\\"config_value\\\": config_value\\n            }\\n            table = bot.dbi.table('clembot_config')\\n\\n            existing_config_record = await table.query().select().where(config_name=config_name).get_first()\\n\\n            if existing_config_record:\\n                update_query = table.update(config_value=config_value).where(config_name=config_name)\\n                await update_query.commit()\\n            else:\\n                insert_query = table.insert(**clembot_config_record)\\n                await insert_query.commit()\\n\\n            await cls.load_config(bot)\\n\\n        except Exception as error:\\n            Logger.error(f\\\"{traceback.format_exc()}\\\")\"}, {\"identifier\":\"SpellHelper\", \"path\":\"clembot/exts/pkmn/spelling.py\", \"snippet\":\"class SpellHelper:\\n\\n    words = []\\n    n = None\\n\\n\\n    @classmethod\\n    def set_dictionary(cls, word_list):\\n        cls.words = Counter(word_list)\\n        cls.n = sum(cls.words.values())\\n\\n\\n    @classmethod\\n    def all_words(cls, text):\\n        return re.findall(r'\\\\w+', text.upper())\\n\\n\\n    @classmethod\\n    def P(cls, word):\\n        \\\"Probability of `word`.\\\"\\n        if not cls.n:\\n            return 0\\n\\n        return cls, cls.words[word] / cls.n\\n\\n\\n    @classmethod\\n    def correction(cls, word):\\n        \\\"Most probable spelling correction for word.\\\"\\n        return max(cls.candidates(word), key=cls.P)\\n\\n\\n    @classmethod\\n    def candidates(cls, word):\\n        \\\"Generate possible spelling corrections for word.\\\"\\n        return cls.known([word]) or cls.known(cls.edits1(word)) or cls.known(cls.edits2(word)) or [word]\\n\\n\\n    @classmethod\\n    def known(cls, words):\\n        \\\"The subset of `words` that appear in the dictionary of WORDS.\\\"\\n        if not cls.words:\\n            return None\\n\\n        return set(w.upper() for w in words if w.upper() in cls.words)\\n\\n    @classmethod\\n    def edits1(cls, word):\\n        \\\"All edits that are one edit away from `word`.\\\"\\n        letters    = 'abcdefghijklmnopqrstuvwxyz'\\n        splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\\n        deletes    = [L + R[1:]               for L, R in splits if R]\\n        transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\\n        replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\\n        inserts    = [L + c + R               for L, R in splits for c in letters]\\n        return set(deletes + transposes + replaces + inserts)\\n\\n\\n    @classmethod\\n    def edits2(cls, word):\\n        \\\"All edits that are two edits away from `word`.\\\"\\n        return (e2 for e1 in cls.edits1(word) for e2 in cls.edits1(e1))\"}, {\"identifier\":\"Embeds\", \"path\":\"clembot/utilities/utils/embeds.py\", \"snippet\":\"class Embeds:\\n\\n    def __init__(self):\\n        return\\n\\n    @staticmethod\\n    async def ask_for_input(ctx, prompt_embed, validate_response=None):\\n        \\\"\\\"\\\"\\n        returns cancel if configuration is cancelled.\\n        \\\"\\\"\\\"\\n        def check(msg):\\n            return msg.author == ctx.message.author and msg.channel == ctx.channel\\n\\n        await ctx.send(embed=prompt_embed)\\n\\n        while True:\\n            try:\\n                response = await ctx.bot.wait_for('message', check=check, timeout=60)\\n            except asyncio.TimeoutError:\\n                await Embeds.error(ctx.channel, 'Too late!')\\n                return \\\"cancel\\\"\\n            except Exception as error:\\n                break\\n            response_content = response.content.strip()\\n            if response_content == \\\"cancel\\\" or (validate_response and validate_response(response_content)):\\n                return response_content\\n            else:\\n                await Embeds.error(ctx.channel, 'I could not interpret your response. Try again!')\\n                continue\\n        return \\\"cancel\\\"\\n\\n\\n    @staticmethod\\n    def google_location_preview_url(lat_long):\\n        key = config_template.api_keys[\\\"google-api-key\\\"]\\n        gmap_base_url = f\\\"https://maps.googleapis.com/maps/api/staticmap?center={lat_long}&markers=color:blue%7C{lat_long}&maptype=roadmap&size=250x125&zoom=15&key={key}\\\"\\n\\n        return gmap_base_url\\n\\n    @staticmethod\\n    async def message(channel, description, title=None, footer=None, user=None, icon=MyEmojis.INFO):\\n        try:\\n            error_message = \\\"The output contains more than 2000 characters.\\\"\\n\\n            user_mention = f\\\"{icon} \\\"\\n            if user:\\n                user_mention = f\\\"{user_mention}**{user.display_name}** \\\"\\n\\n            if len(description) >= 2000:\\n                discord.Embed(description=\\\"{0}\\\".format(error_message), colour=discord.Color.red())\\n\\n            message_embed = discord.Embed(description=f\\\"{user_mention}{description}\\\", colour=discord.Colour.green(), title=title)\\n            if footer:\\n                message_embed.set_footer(text=footer)\\n            return await channel.send(embed=message_embed)\\n        except Exception as error:\\n            Logger.error(error)\\n\\n\\n    @staticmethod\\n    async def error(channel, description, user=None):\\n\\n        color = discord.Colour.red()\\n        user_mention = \\\"\\\"\\n        if user:\\n            user_mention = f\\\"**{user.display_name}** \\\"\\n        error_embed = discord.Embed(description=f\\\"{MyEmojis.ERROR} {user_mention}{description}\\\", colour=color)\\n        return await channel.send(embed=error_embed)\\n\\n    @staticmethod\\n    async def error_notification(ctx, header, error, disappear_in=30):\\n        \\\"\\\"\\\"The message stays for 30 seconds and disappears!\\\"\\\"\\\"\\n        try:\\n            err_msg = await ctx.send(embed=Embeds.make_embed(msg_type='error', header=header, content=error.__str__()))\\n            if disappear_in:\\n                await asyncio.sleep(disappear_in)\\n                await err_msg.delete()\\n        except Exception as err:\\n            Logger.error(err)\\n\\n\\n    @staticmethod\\n    def trim_to(text, length, delimiter=\\\",\\\"):\\n        if len(text) == 0:\\n            return \\\"None\\\"\\n        if text and delimiter:\\n            return text[:text.rfind(delimiter, 0, length)] + \\\" ** and more1.**\\\" if len(text) > length else text\\n        return text\\n\\n    @staticmethod\\n    def make_embed(msg_type='', header=None, header_icon=None, title=None, title_url=None, content=None, thumbnail='',\\n                   image='', fields=None, footer=None, footer_icon=None, inline=True, guild=None, msg_color=None):\\n        \\\"\\\"\\\"Returns a formatted discord embed object.\\n\\n        Define either a type or a colour.\\n        Types are:\\n        error, warning, info, success, help.\\n        \\\"\\\"\\\"\\n        embed_types = {\\n            'error':{\\n                'icon':'https://i.imgur.com/juhq2uJ.png',\\n                'colour':'red'\\n            },\\n            'warning':{\\n                'icon':'https://i.imgur.com/4JuaNt9.png',\\n                'colour':'gold'\\n            },\\n            'info':{\\n                'icon':'https://i.imgur.com/wzryVaS.png',\\n                'colour':'blue'\\n            },\\n            'success':{\\n                'icon':'https://i.imgur.com/ZTKc3mr.png',\\n                'colour':'green'\\n            },\\n            'help':{\\n                'icon':'https://i.imgur.com/kTTIZzR.png',\\n                'colour':'blue'\\n            }\\n        }\\n        if msg_type in embed_types.keys():\\n            msg_color = embed_types[msg_type]['colour']\\n            header_icon = embed_types[msg_type]['icon']\\n        if guild and not msg_color:\\n            msg_color = color(guild)\\n        else:\\n            if not isinstance(msg_color, discord.Colour):\\n                msg_color = color(msg_color)\\n\\n        embed = discord.Embed(title=title or discord.Embed.Empty, url=title_url or discord.Embed.Empty,\\n            description=content, colour=msg_color)\\n\\n        if header:\\n            embed.set_author(name=header, icon_url=header_icon or discord.Embed.Empty, url=discord.Embed.Empty)\\n\\n        if thumbnail:\\n            embed.set_thumbnail(url=thumbnail)\\n        if image:\\n            embed.set_image(url=image)\\n        if fields:\\n            for key, value in fields.items():\\n                ilf = inline\\n                if not isinstance(value, str):\\n                    if not value:\\n                        continue\\n                    else:\\n                        ilf = value[0]\\n                        value = value[1]\\n                if value:\\n                    embed.add_field(name=f\\\"**{key}**\\\", value=Embeds.trim_to(value, 995), inline=ilf)\\n        if footer:\\n            footer = {'text':footer}\\n            if footer_icon:\\n                footer['icon_url'] = footer_icon\\n            embed.set_footer(**footer)\\n        return embed\"}, {\"identifier\":\"Utilities\", \"path\":\"clembot/utilities/utils/utilities.py\", \"snippet\":\"class Utilities:\\n\\n    numbers = {\\\"0\\\": \\\":zero:\\\", \\\"1\\\": \\\":one:\\\", \\\"2\\\": \\\":two:\\\", \\\"3\\\": \\\":three:\\\", \\\"4\\\": \\\":four:\\\", \\\"5\\\": \\\":five:\\\", \\\"6\\\": \\\":six:\\\", \\\"7\\\": \\\":seven:\\\", \\\"8\\\": \\\":eight:\\\", \\\"9\\\": \\\":nine:\\\"}\\n\\n    @staticmethod\\n    def trim_to(text, length, delimiter=\\\",\\\"):\\n        if len(text) == 0:\\n            return \\\"None\\\"\\n        if text and delimiter:\\n            return text[:text.rfind(delimiter, 0, length)] + \\\" ** and more.**\\\" if len(text) > length else text\\n        return text\\n\\n    @staticmethod\\n    def emojify_numbers(number):\\n        number_emoji = \\\"\\\"\\n\\n        reverse = \\\"\\\".join(reversed(str(number)))\\n\\n        for digit in reverse[::-1]:\\n\\n            emoji = Utilities.numbers.get(digit)\\n            if not emoji:\\n                emoji = \\\":regional_indicator_\\\" + digit.lower() + \\\":\\\"\\n\\n            number_emoji = number_emoji + emoji\\n\\n        return number_emoji\\n\\n    @staticmethod\\n    def _normalize(emoji):\\n        initial_emoji = emoji\\n        if isinstance(emoji, discord.Reaction):\\n            emoji = emoji.emoji\\n\\n        if isinstance(emoji, discord.Emoji):\\n            emoji = ':%s:%s' % (emoji.name, emoji.id)\\n\\n        elif isinstance(emoji, discord.PartialEmoji):\\n            emoji = emoji._as_reaction()\\n        elif isinstance(emoji, str):\\n            pass\\n\\n        if emoji.count(':') == 1 and not emoji.startswith(':'):\\n            emoji = f\\\":{emoji}\\\"\\n\\n        if emoji.__contains__(\\\">\\\") and emoji.__contains__(\\\"<\\\"):\\n            emoji = emoji.replace('<','').replace('>','')\\n        return emoji\\n\\n\\n    @staticmethod\\n    def _demojify(emoji):\\n        # convert emoji to id\\n        if isinstance(emoji, discord.Reaction):\\n            emoji = emoji.emoji.id\\n\\n        if isinstance(emoji, discord.Emoji):\\n            emoji = emoji.id\\n        elif isinstance(emoji, discord.PartialEmoji):\\n            emoji = emoji.id if emoji.id else emoji.name\\n        elif isinstance(emoji, str):\\n            pass\\n\\n        return emoji\\n\\n    @staticmethod\\n    def _emojify(emoji):\\n        if emoji.__contains__(\\\">\\\") and emoji.__contains__(\\\"<\\\"):\\n            emoji = emoji.replace('<', '').replace('>', '')\\n        return emoji\\n\\n    @staticmethod\\n    def _uuid(cls, id):\\n        try:\\n            return '%x' % (hash(id) % 10 ** 8)\\n        except Exception as error:\\n            Logger.error(error)\\n            return id\\n\\n    @staticmethod\\n    async def _send_error_message(channel, description, user=None):\\n\\n        color = discord.Colour.red()\\n        user_mention = \\\"\\\"\\n        if user:\\n            user_mention = f\\\"Beep Beep! **{user.display_name}** \\\"\\n        error_embed = discord.Embed(description=f\\\"{user_mention}{description}\\\", colour=color)\\n        return await channel.send(embed=error_embed)\\n\\n    @staticmethod\\n    async def message(destination, description, user=None):\\n\\n        color = discord.Colour.green()\\n        user_mention = \\\"\\\"\\n        if user:\\n            user_mention = f\\\"Beep Beep! **{user.display_name}** \\\"\\n        error_embed = discord.Embed(description=f\\\"{user_mention}{description}\\\", colour=color)\\n        return await destination.send(embed=error_embed)\\n\\n    @staticmethod\\n    async def message_as_text(channel, description):\\n        return await channel.send(description)\\n\\n    @staticmethod\\n    async def error(channel, description, user=None):\\n\\n        color = discord.Colour.red()\\n        user_mention = \\\"\\\"\\n        if user:\\n            user_mention = f\\\"Beep Beep! **{user.display_name}** \\\"\\n        error_message = f\\\"{user_mention}{description}\\\"\\n        error_embed = discord.Embed(description=f\\\"{error_message}\\\", colour=color)\\n        Logger.error(error_message)\\n        return await channel.send(embed=error_embed)\\n\\n    @staticmethod\\n    async def _send_message(channel, description, title=None, footer=None, user=None):\\n        try:\\n\\n            error_message = \\\"The output contains more than 2000 characters.\\\"\\n\\n            user_mention = \\\"\\\"\\n            if user:\\n                user_mention = f\\\"Beep Beep! **{user.display_name}** \\\"\\n\\n            if len(description) >= 2000:\\n                discord.Embed(description=\\\"{0}\\\".format(error_message))\\n\\n            color = discord.Colour.green()\\n            message_embed = discord.Embed(description=f\\\"{user_mention}{description}\\\", colour=color, title=title)\\n            if footer:\\n                message_embed.set_footer(text=footer)\\n            return await channel.send(embed=message_embed)\\n        except Exception as error:\\n            Logger.error(error)\\n\\n    @staticmethod\\n    async def _send_embed(channel, description=None, title=None, additional_fields={}, footer=None):\\n\\n        embed = discord.Embed(description=description, colour=discord.Colour.gold(), title=title)\\n\\n        for label, value in additional_fields.items():\\n            if value:\\n                embed.add_field(name=\\\"**{0}**\\\".format(label), value=value, inline=False)\\n\\n        if footer:\\n            embed.set_footer(text=footer)\\n\\n        try:\\n            return await channel.send(embed=embed)\\n        except Exception as error:\\n            return await channel.send(error)\\n\\n    @command(name=\\\"export\\\")\\n    async def _export(self, ctx):\\n\\n        return await self._send_message(ctx.channel, \\\"Beep Beep! **{}**, This feature is under-development!\\\".format(ctx.message.author.display_name))\\n\\n        Logger.error(\\\"_export() called!\\\")\\n\\n        raid_dict = ctx.bot.guild_dict[ctx.guild.id]['raidchannel_dict'][ctx.channel.id]\\n\\n        channel_mentions = ctx.message.raw_channel_mentions\\n\\n        if len(channel_mentions) < 1:\\n            await self._send_error_message(ctx.channel, \\\"Beep Beep! **{}**, Please provide the channel reference to export the details!\\\".format(ctx.message.author.display_name))\\n\\n    @command(name=\\\"clean_content\\\")\\n    async def _clean_content(self, message):\\n\\n        message_content = {}\\n        content_without_mentions = message.content\\n\\n        for mention in message.mentions:\\n            mention_text = mention.mention.replace(\\\"!\\\", \\\"\\\")\\n            content_without_mentions = content_without_mentions.replace(\\\"<@!\\\", \\\"<@\\\").replace(mention_text, '')\\n\\n        # remove extra spaces\\n        message_content['content_without_mentions'] = re.sub(' +', ' ', content_without_mentions)\\n\\n        return message_content\\n\\n    @staticmethod\\n    def get_help_embed(description, usage, available_value_title, available_values, mode=\\\"message\\\"):\\n\\n        if mode == \\\"message\\\":\\n            color = discord.Colour.green()\\n        else:\\n            color = discord.Colour.red()\\n\\n        help_embed = discord.Embed( description=\\\"**{0}**\\\".format(description), colour=color)\\n\\n        help_embed.add_field(name=\\\"**Usage :**\\\", value = \\\"**{0}**\\\".format(usage))\\n        help_embed.add_field(name=\\\"**{0} :**\\\".format(available_value_title), value=_(\\\"**{0}**\\\".format(\\\", \\\".join(available_values))), inline=False)\\n\\n        return help_embed\\n\\n    @staticmethod\\n    async def _send_error_message_and_cleanup(channel, message, user):\\n        log_message = await Embeds.error(channel, message, user=user)\\n        await asyncio.sleep(8)\\n        await log_message.delete()\\n\\n\\n    @staticmethod\\n    async def get_image_embed(channel, image_url):\\n        embed = discord.Embed(colour=channel.guild.me.colour)\\n        embed.set_thumbnail(url=image_url)\\n        return await channel.send(embed=embed)\\n\\n    @staticmethod\\n    async def ask(message, destination, user_list=None, *, react_list=['✅', '❎']):\\n        if user_list and type(user_list) != __builtins__.list:\\n            user_list = [user_list]\\n\\n\\n        def check(reaction, user):\\n            if user_list and type(user_list) is __builtins__.list:\\n                return (user.id in user_list) and (reaction.message.id == message.id) and (reaction.emoji in react_list)\\n            elif not user_list:\\n                return (user.id != message.guild.me.id) and (reaction.message.id == message.id) and (reaction.emoji in react_list)\\n\\n\\n        for r in react_list:\\n            await asyncio.sleep(0.25)\\n            await message.add_reaction(r)\\n        try:\\n            reaction, user = await Clembot.wait_for('reaction_add', check=check, timeout=60)\\n            return reaction, user\\n        except asyncio.TimeoutError:\\n            await message.clear_reactions()\\n            return\\n\\n\\n    @staticmethod\\n    @wrap_error\\n    async def ask_confirmation(ctx, message, rusure_message, yes_message, no_message, timed_out_message):\\n        author = message.author\\n        channel = message.channel\\n\\n        reaction_list = ['✅', '❎']\\n        # reaction_list = ['❔', '✅', '❎']\\n\\n        rusure = await ctx.channel.send(f\\\"Beep Beep! {rusure_message}\\\")\\n        await rusure.add_reaction( \\\"✅\\\")  # checkmark\\n        await rusure.add_reaction( \\\"❎\\\")  # cross\\n\\n        def check(react, user):\\n            if user.id != author.id:\\n                return False\\n            return True\\n\\n        # res = await Clembot.wait_for_reaction(reaction_list, message=rusure, check=check, timeout=60)\\n        try:\\n            reaction, user = await ctx.bot.wait_for('reaction_add', check=check, timeout=10)\\n        except asyncio.TimeoutError:\\n            await rusure.delete()\\n            confirmation = await channel.send(f\\\"Beep Beep! {timed_out_message}\\\")\\n            await asyncio.sleep(1)\\n            await confirmation.delete()\\n            return False\\n\\n        if reaction.emoji == \\\"❎\\\":\\n            await rusure.delete()\\n            confirmation = await channel.send(f\\\"Beep Beep! {no_message}\\\")\\n            await asyncio.sleep(1)\\n            await confirmation.delete()\\n            return False\\n        elif reaction.emoji == \\\"✅\\\":\\n            await rusure.delete()\\n            confirmation = await channel.send(f\\\"Beep Beep! {yes_message}\\\")\\n            await asyncio.sleep(1)\\n            await confirmation.delete()\\n            return True\\n\\n\\n    @staticmethod\\n    @wrap_error\\n    async def ask_via_reactions(ctx, message, question_message, accepted_message, rejected_message, timed_out_message, reaction_dict, error_response):\\n        author = message.author\\n        channel = message.channel\\n\\n        rusure = await ctx.channel.send(f\\\"{question_message}\\\")\\n\\n        for r in reaction_dict.keys():\\n            await rusure.add_reaction(r)\\n\\n        def check(react, user):\\n            if user.id != author.id:\\n                return False\\n            return True\\n\\n        try:\\n            reaction, user = await ctx.bot.wait_for('reaction_add', check=check, timeout=10)\\n        except asyncio.TimeoutError:\\n            await rusure.delete()\\n            confirmation = await channel.send(f\\\"{timed_out_message}\\\")\\n            await asyncio.sleep(1)\\n            await confirmation.delete()\\n            return error_response\\n\\n        if reaction.emoji not in reaction_dict.keys():\\n            await rusure.delete()\\n            confirmation = await channel.send(f\\\"{rejected_message}\\\")\\n            await asyncio.sleep(1)\\n            await confirmation.delete()\\n            return error_response\\n\\n        elif reaction.emoji in reaction_dict.keys():\\n            await rusure.delete()\\n            confirmation = await channel.send(f\\\"{accepted_message}\\\")\\n            await asyncio.sleep(1)\\n            await confirmation.delete()\\n            return reaction_dict.get(reaction.emoji, error_response)\\n\\n\\n    @staticmethod\\n    async def send_to_hastebin(destination, whatever):\\n        whatever = whatever.encode('ascii', errors='replace').decode('utf-8')\\n        print(whatever)\"}]", "import_statement": "import json\nimport discord\nimport pytz\nfrom discord.ext import commands\nfrom discord.ext.commands import BadArgument\nfrom clembot.config.constants import Icons, MyEmojis, GUILD_METADATA_KEY, GUILD_CONFIG_KEY, GLOBAL_CONFIG_KEY, \\\n    CHANNEL_METADATA_KEY\nfrom clembot.core import checks\nfrom clembot.core.bot import group\nfrom clembot.core.checks import is_guild_admin, is_guild_mod\nfrom clembot.core.errors import wrap_error\nfrom clembot.core.logs import Logger\nfrom clembot.exts.config.channel_metadata import ChannelMetadata\nfrom clembot.exts.config.globalconfigmanager import GlobalConfigCache\nfrom clembot.exts.pkmn.spelling import SpellHelper\nfrom clembot.utilities.utils.embeds import Embeds\nfrom clembot.utilities.utils.utilities import Utilities", "code": "            country_name = f\" ({pytz.country_names[country_code]})\"\n            timezones = '\\n'.join([tz for tz in timezone_list if tz in pytz.common_timezones])\n        except KeyError:\n            timezone_list = []\n            country_name = \"\"\n            timezones = 'No timezone found. Are you sure you provided a valid ISO 3166 country code?'\n\n        response_message = await ctx.send(embed=Embeds.make_embed(header=f\"Available Timezones for {country_code}{country_name}\",\n                                               header_icon=Icons.TIMEZONE,\n                                               msg_color=discord.Color.blue(),\n                                               content=f\"{timezones}\",\n                                                footer=\"Timezone info from Olson database\"))\n\n        await response_message.add_reaction(MyEmojis.TRASH)\n\n\n    @group(pass_context=True, category='Bot Info', aliases=[\"config\"])\n    @is_guild_admin()\n    async def cmd_config(self, ctx):\n        \"\"\"\n        Command to read / update the configuration for global, guild or channel. A few shortcuts are provided for city & timezone as well.\n        `!config guild`\n        `!config channel`\n        `!config timezone America/Los_Angeles` - set timezone for guild\n        `!config city  BURBANKCA` - set city for guild\n        \"\"\"\n        if ctx.invoked_subcommand is None:\n            if ctx.subcommand_passed is None:\n                return await self.cmd_config_guild(ctx)\n\n            raise BadArgument(\"`!config` can be used with `guild, channel, timezone, city`\")\n\n\n    @cmd_config.command(pass_context=True, category='Bot Info', aliases=[\"timezone\"])\n    @wrap_error\n    @is_guild_admin()\n    async def cmd_config_timezone(self, ctx, timezone):\n\n        if timezone in pytz.all_timezones:\n            await ctx.guild_profile(key='timezone', value=timezone)\n            config = await ctx.guild_profile(key='timezone')\n            return await Embeds.message(ctx.message.channel, f\"**timezone** is set to **{config}**\")\n\n        raise BadArgument(f\"**{timezone}** didn't resolve to a valid timezone. \\nYou can see a list for a valid timezone for a country ISO 3166 country code using `!timezone country_code`\")\n\n\n    @cmd_config.command(pass_context=True, category='Bot Info', aliases=[\"city\"])\n    @wrap_error\n    @is_guild_admin()\n    async def cmd_config_city(self, ctx, *city):\n        if city:\n            city_state = ''.join(city).upper()\n            await ctx.guild_profile(key='city', value=city_state)\n            config = await ctx.guild_profile(key='city')\n            return await Embeds.message(ctx.message.channel, f\"**city** is set to **{config}**\")\n        else:\n            raise BadArgument(\"Please specify city and state, in all caps, without spaces.\")\n\n\n    @cmd_config.command(pass_context=True, category='Bot Info', aliases=[\"guild\"])\n    @wrap_error\n    @is_guild_admin()\n    async def cmd_config_guild(self, ctx, config_name=None, config_value=None):\n        \"\"\"\n        View/Change guild configuration.\n        **Example**\n        `!config guild city BURBANKCA`\n        \"\"\"\n        if config_name and config_name not in GUILD_CONFIG_KEY and config_name not in GUILD_METADATA_KEY:\n            return await Embeds.error(ctx.message.channel, \"No such configuration exists.\")\n\n        config = await ctx.guild_profile(key=config_name, value=config_value)\n        if config_name:\n            if config_value:\n                config = await ctx.guild_profile(key=config_name)\n            else:\n                config = await ctx.guild_profile(key=config_name, delete=True)\n            await Embeds.message(ctx.message.channel, f\"**{config_name}** is set to **{config}**\")\n        else:\n            await ConfigCog.send_guild_config_embed(ctx, config)\n\n\n    @cmd_config.command(pass_context=True, category='Bot Info', aliases=[\"channel\"])\n    @wrap_error\n    @is_guild_mod()\n    async def cmd_config_channel(self, ctx, config_name=None, config_value=None):\n        \"\"\"\n        View/Change channel configuration.\n        **Example**\n        `!config channel city BURBANKCA`\n        \"\"\"\n        if config_name and config_name not in CHANNEL_METADATA_KEY:\n            return await Embeds.error(ctx.message.channel, \"No such configuration exists.\")\n\n        if config_name:\n            if config_value:\n                await ctx.channel_profile(channel_id=ctx.message.channel.id, key=config_name)\n            else:\n                await ctx.channel_profile(channel_id=ctx.message.channel.id, key=config_name, delete=True)\n\n        await ctx.channel_profile(channel_id=ctx.message.channel.id, key=config_name, value=config_value)\n\n        config = await ctx.channel_profile(channel_id=ctx.message.channel.id)\n        await ctx.send(embed = ChannelMetadata.profile_embed(ctx, config))\n\n\n    @staticmethod\n    async def send_guild_config_embed(ctx, config):\n\n        embed = Embeds.make_embed(header=\"Guild Configuration\", header_icon=Icons.CONFIGURATION,\n                        fields={ key.capitalize():value for key, value in config.items()}, msg_color=discord.Color.blue(),\n                        inline=True)\n\n        return await ctx.send(embed=embed)\n\n\n    @staticmethod\n    async def send_global_config_embed(ctx, config):\n\n        embed = Embeds.make_embed(header=\"Clembot Configuration\", header_icon=Icons.CONFIGURATION,\n", "next_line": "                        fields={k[0]:k[1] for k in config.items() if k[0] in GLOBAL_CONFIG_KEY and k[1] is not None or ''},", "gold_snippet_index": 4, "id": 8, "__internal_uuid__": "204ba1a8-96bd-4048-b970-f85ed8c274e6"}
{"repo_name": "charityscience/csh-sms", "file_path": "live_tests/modules/test_texter.py", "context": "[{\"identifier\":\"TEXTLOCAL_API\", \"path\":\"cshsms/settings.py\", \"snippet\":\"BASE_DIR = os.path.dirname(os.path.dirname(os.path.abspath(__file__)))\\nDEBUG = False\\nALLOWED_HOSTS = [REMOTE['host']]\\nINSTALLED_APPS = [\\n    'management.apps.ManagementConfig',\\n    'django.contrib.admin',\\n    'django.contrib.auth',\\n    'django.contrib.contenttypes',\\n    'django.contrib.sessions',\\n    'django.contrib.messages',\\n    'django.contrib.staticfiles',\\n    'django_crontab'\\n]\\nMIDDLEWARE = [\\n    'django.middleware.security.SecurityMiddleware',\\n    'django.contrib.sessions.middleware.SessionMiddleware',\\n    'django.middleware.common.CommonMiddleware',\\n    'django.middleware.csrf.CsrfViewMiddleware',\\n    'django.contrib.auth.middleware.AuthenticationMiddleware',\\n    'django.contrib.messages.middleware.MessageMiddleware',\\n    'django.middleware.clickjacking.XFrameOptionsMiddleware',\\n]\\nROOT_URLCONF = 'cshsms.urls'\\nTEMPLATES = [\\n    {\\n        'BACKEND': 'django.template.backends.django.DjangoTemplates',\\n        'DIRS': [],\\n        'APP_DIRS': True,\\n        'OPTIONS': {\\n            'context_processors': [\\n                'django.template.context_processors.debug',\\n                'django.template.context_processors.request',\\n                'django.contrib.auth.context_processors.auth',\\n                'django.contrib.messages.context_processors.messages',\\n            ],\\n        },\\n    },\\n]\\nWSGI_APPLICATION = 'cshsms.wsgi.application'\\nAUTH_PASSWORD_VALIDATORS = [\\n    {\\n        'NAME': 'django.contrib.auth.password_validation.UserAttributeSimilarityValidator',\\n    },\\n    {\\n        'NAME': 'django.contrib.auth.password_validation.MinimumLengthValidator',\\n    },\\n    {\\n        'NAME': 'django.contrib.auth.password_validation.CommonPasswordValidator',\\n    },\\n    {\\n        'NAME': 'django.contrib.auth.password_validation.NumericPasswordValidator',\\n    },\\n]\\nLANGUAGE_CODE = 'en-us'\\nTIME_ZONE = 'Asia/Kolkata'\\nUSE_I18N = True\\nUSE_L10N = True\\nUSE_TZ = True\\nSTATIC_URL = '/static/'\\nTEST_RUNNER = 'rainbowtests.test.runner.RainbowDiscoverRunner'\\nCRONJOBS = [\\n    ('*/10 * * * *', 'jobs.text_processor_job.check_and_process_registrations'),  # Check for new registrations every 10 min\\n    ('0 16 * * *', 'jobs.text_reminder_job.remind_all')                           # Remind people daily at 4pm\\n]\"}, {\"identifier\":\"Contact\", \"path\":\"management/models.py\", \"snippet\":\"class Contact(models.Model):\\n    # Vitals\\n    name = models.CharField(max_length=50)\\n    phone_regex = RegexValidator(regex=r'^\\\\+?91?\\\\d{9,15}$',\\n        message=\\\"Phone number must be entered in the format: '+9199999999'. Up to 15 digits allowed.\\\",\\n        code=\\\"Invalid phone_number\\\")\\n    phone_number = models.CharField(validators=[phone_regex], blank=False,\\n        max_length=20, default=\\\"012345\\\") # validators should be a list\\n    alt_phone_number = models.CharField(validators=[phone_regex], blank=False,\\n        max_length=20, default=\\\"012345\\\")\\n    date_of_birth = models.DateField(auto_now=False, auto_now_add=False,\\n        default=datetime.date.today)\\n    date_of_sign_up = models.DateField(auto_now=False, auto_now_add=False,\\n        default=datetime.date.today)\\n    delay_in_days = models.SmallIntegerField(default=0, blank=True)\\n    functional_date_of_birth = models.DateField(blank=True,auto_now=False,\\n        auto_now_add=False, default=datetime.date.today)\\n    cancelled = models.BooleanField(default=False, blank=False)\\n\\n    # Personal Info\\n    gender = models.CharField(max_length=6, blank=True)\\n    mother_tongue = models.CharField(max_length=50, blank=True)\\n    state = models.CharField(max_length=50, blank=True)\\n    division = models.CharField(max_length=50, blank=True)\\n    district = models.CharField(max_length=50, blank=True)\\n    city = models.CharField(max_length=50, blank=True)\\n    monthly_income_rupees = models.IntegerField(blank=True, default=999999)\\n    religion = models.CharField(max_length=50, blank=True)\\n    children_previously_vaccinated = models.NullBooleanField()\\n    not_vaccinated_why = models.CharField(max_length=500, blank=True)\\n    mother_first_name = models.CharField(max_length=50, blank=True)\\n    mother_last_name = models.CharField(max_length=50, blank=True)\\n    \\n    # Type of Sign Up\\n    method_of_sign_up = models.CharField(max_length=50, blank=True)\\n    org_sign_up = models.CharField(max_length=40, blank=True)\\n    hospital_name = models.CharField(max_length=50, blank=True)\\n    doctor_name = models.CharField(max_length=30, blank=True)\\n    preg_signup = models.NullBooleanField(default=False)\\n    preg_update = models.NullBooleanField(default=False)\\n\\n    def has_been_born(self):\\n        today = datetime.date.today()\\n        diff = today - self.date_of_birth\\n        return  diff >= datetime.timedelta(0)\\n\\n\\n    # System Identification\\n    telerivet_contact_id = models.CharField(max_length=50, blank=True)\\n    trial_id = models.CharField(max_length=40, blank=True)\\n    trial_group = models.CharField(max_length=40, blank=True)    \\n\\n\\n    language_preference = models.CharField(max_length=20,\\n        default=\\\"English\\\", blank=False, null=False)\\n\\n    # Message References\\n    preferred_time = models.CharField(max_length=50, blank=True)\\n    script_selection = models.CharField(max_length=20, blank=True)\\n    telerivet_sender_phone = models.CharField(max_length=100, blank=True)\\n    time_created = models.DateField(auto_now=False, auto_now_add=False,\\n        default=datetime.date.today)\\n    last_heard_from = models.DateTimeField(auto_now=False, auto_now_add=False, blank=True,\\n        null=True)\\n    last_contacted = models.DateTimeField(auto_now=False, auto_now_add=False, blank=True,\\n        null=True)\\n\\n    def __str__(self):\\n        return \\\"%s, %s, %s\\\" % (self.name, self.phone_number, self.date_of_birth)\\n\\n    class Meta:\\n        ordering = ('name',)\"}, {\"identifier\":\"Group\", \"path\":\"management/models.py\", \"snippet\":\"class Group(models.Model):\\n    \\\"\\\"\\\"\\n    List of Pre-existing groups in Telerivet\\n    -----------------------\\n    Telerivet Blocked\\n    Cancelled Contacts\\n    Cancelled Contacts - English\\n    Cancelled Contacts - Hindi\\n    Cancelled Contacts - Text Sign Ups\\n    Cancelled Contacts - Text Sign Ups - English\\n    Cancelled Contacts - Text Sign Ups - Hindi\\n    Cancelled Contacts - Text Sign Ups - undefined\\n    Cancelled Contacts - undefined\\n    Everyone - English\\n    Everyone - Gujarati    Everyone - Hindi\\n    Everyone - Online Form\\n    Everyone - Text Default Time\\n    One Time Sign Up Message 06-02-17\\n    Online Form\\n    Online Form - English\\n    Online Form - English - ENG\\n    Online Form - English - ENG - Text Default Time\\n    Online Form - Gujarati\\n    Online Form - Gujarati - GUJ\\n    Online Form - Gujarati - GUJ - Text Default Time\\n    Online Form - Hindi\\n    Online Form - Hindi - HND\\n    Online Form - Hindi - HND - Text Default Time\\n    Sample Contacts\\n    Text Sign Ups\\n    Text Sign Ups - English\\n    Text Sign Ups - English - ENG\\n    Text Sign Ups - English - ENG - Text Default Time\\n    Text Sign Ups - Hindi\\n    \\\"\\\"\\\"\\n    name = models.CharField(max_length=100, unique=True)\\n    contacts = models.ManyToManyField(Contact)\\n\\n    def __str__(self):\\n        return self.name\\n\\n    class Meta:\\n        ordering = ('name',)\"}, {\"identifier\":\"Message\", \"path\":\"management/models.py\", \"snippet\":\"class Message(models.Model):\\n    contact = models.ForeignKey(Contact, on_delete=models.CASCADE, null=True)\\n    body = models.CharField(max_length=300)\\n\\n    # Message direction is Incoming or Outgoing\\n    direction = models.CharField(max_length=10)\\n    \\n    is_processed = models.BooleanField(default=False, blank=False)\\n    created_at = models.DateTimeField(auto_now_add=True)\\n    received_at = models.DateTimeField(auto_now=False,\\n                                        auto_now_add=False,\\n                                        blank=True,\\n                                        null=True)\\n    sent_at = models.DateTimeField(auto_now=False,\\n                                    auto_now_add=False,\\n                                    blank=True,\\n                                    null=True)\\n\\n    def __str__(self):\\n        return self.body\"}, {\"identifier\":\"Texter\", \"path\":\"modules/texter.py\", \"snippet\":\"class Texter(object):\\n    def read_inbox(self):\\n        textlocal = TextLocal(apikey=TEXTLOCAL_API, primary_id=TEXTLOCAL_PRIMARY_ID, sendername=TEXTLOCAL_SENDERNAME)\\n        num_message_dict = textlocal.new_messages_by_number()\\n        return num_message_dict\\n\\n    def send(self, message, phone_number):\\n        textlocal = TextLocal(apikey=TEXTLOCAL_API,\\n                        primary_id=TEXTLOCAL_PRIMARY_ID,\\n                        sendername=TEXTLOCAL_SENDERNAME)\\n        send_status = textlocal.send_message(message=message,\\n                                            phone_numbers=phone_number)\\n        return send_status\\n\\n    def read_api_outbox(self):\\n        textlocal = TextLocal(apikey=TEXTLOCAL_API, primary_id=TEXTLOCAL_PRIMARY_ID, sendername=TEXTLOCAL_SENDERNAME)\\n        num_message_dict = textlocal.new_api_send_messages_by_number()\\n        return num_message_dict\"}, {\"identifier\":\"TextLocal\", \"path\":\"modules/textlocalwrapper.py\", \"snippet\":\"class TextLocal(object):\\n    def __init__(self, apikey, primary_id, sendername):\\n        self.apikey = apikey\\n        self.primary_id = primary_id\\n        self.sendername = sendername\\n\\n\\n    def get_all_inboxes(self):\\n        params = {'apikey': self.apikey}\\n        inboxes_url = 'https://api.textlocal.in/get_inboxes/?'\\n        return self.get_url_response(request_url=inboxes_url, params=params)\\n\\n\\n    def get_primary_inbox(self):\\n        params = {'apikey': self.apikey, 'inbox_id': self.primary_id}\\n        messages_url = 'https://api.textlocal.in/get_messages/?'\\n        return self.get_url_response(request_url=messages_url, params=params)\\n\\n    def get_api_send_history(self):\\n        params = {'apikey': self.apikey}\\n        api_send_history_url = 'https://api.textlocal.in/get_history_api/?'\\n        return self.get_url_response(request_url=api_send_history_url, params=params)\\n\\n\\n    def get_url_response(self, request_url, params):\\n        f = request.urlopen(request_url + parse.urlencode(params))\\n        return json.loads(f.read().decode('latin1'))\\n\\n\\n    def get_primary_inbox_messages(self):\\n        return self.get_primary_inbox()['messages']\\n\\n    def get_api_send_history_messages(self):\\n        return self.get_api_send_history()['messages']\\n\\n    def correct_unicode(self, messages, key_name):\\n        for message in messages:\\n            message[key_name] = self.correct_corrupted_unicode_matches(message[key_name])\\n        return messages\\n\\n    def correct_corrupted_unicode_matches(self, message):\\n        fixed_message = []\\n        for message_part in message.split(' '):\\n            if all(c in string.hexdigits for c in message_part) and '09' in message_part and len(message_part) >= 4:\\n                fixed_message.append(''.join([unichr(int(message_part[i:i + 4], 16)) for i in range(0, len(message_part), 4)]))\\n            else:\\n                fixed_message.append(message_part)\\n        return ' '.join(fixed_message)\\n\\n    def response_unicode_encoder(self, message):\\n        removed_at = message.replace(\\\"@U\\\", \\\"\\\\\\\\u\\\")\\n        front_substring = removed_at[:6] + \\\"\\\\\\\\u\\\"\\n        back_substring = removed_at[6:]\\n        back_substring = \\\"\\\\\\\\u\\\".join(re.findall(\\\"....\\\", back_substring))\\n        return (front_substring + back_substring).encode(\\\"utf-8\\\").decode(\\\"unicode-escape\\\")\\n         \\n\\n    def is_message_new(self, message, date_key_name):\\n        date_of_message = datetime_string_ymd_to_datetime(message[date_key_name])\\n        margin = timedelta(hours=24)\\n        return True if datetime.now().replace(tzinfo=timezone.get_default_timezone()) - margin <= date_of_message else False\\n\\n    def add_to_num_message_dict(self, num_message_dict, message, message_key_name, date_key_name):\\n        date_of_message = datetime_string_ymd_to_datetime(message[date_key_name])\\n        num_message_dict.setdefault(str(message['number']), []).append((message[message_key_name], date_of_message))\\n        return num_message_dict\\n\\n    def new_messages_by_number(self):\\n        all_messages = self.get_primary_inbox_messages()\\n        corrected_messages = self.correct_unicode(messages=all_messages, key_name=\\\"message\\\")\\n        num_message_dict = {}\\n        for message in corrected_messages:\\n            if self.is_message_new(message=message, date_key_name=\\\"date\\\"):\\n                num_message_dict = self.add_to_num_message_dict(num_message_dict=num_message_dict,\\n                                                                message=message,\\n                                                                message_key_name=\\\"message\\\",\\n                                                                date_key_name=\\\"date\\\")\\n        return num_message_dict\\n\\n    def new_api_send_messages_by_number(self):\\n        all_messages = self.get_api_send_history_messages()\\n        corrected_messages = self.correct_unicode(messages=all_messages, key_name=\\\"content\\\")\\n        num_message_dict = {}\\n        for message in corrected_messages:\\n            if self.is_message_new(message=message, date_key_name=\\\"datetime\\\"):\\n                num_message_dict = self.add_to_num_message_dict(num_message_dict=num_message_dict,\\n                                                                message=message,\\n                                                                message_key_name=\\\"content\\\",\\n                                                                date_key_name=\\\"datetime\\\")\\n        return num_message_dict\\n\\n    def send_message(self, message, phone_numbers):\\n        send_url = \\\"https://api.textlocal.in/send/?\\\"\\n        unicode_used = 'false'\\n        if not isinstance(message, str):\\n            message = message.encode('utf-8')\\n        if is_not_ascii(message):\\n            unicode_used = 'true'\\n        data = parse.urlencode({'numbers': phone_numbers,\\n                                'message': message,\\n                                'sender': self.sendername,\\n                                'apikey': self.apikey,\\n                                'unicode': unicode_used})\\n        data = data.encode('utf-8')\\n        # Avoid triggering bot errors by setting a user agent\\n        user_agent = {'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_2) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/55.0.2883.95 Safari/537.36'}\\n        requester = request.Request(send_url, headers=user_agent)\\n        f = request.urlopen(requester, data)\\n        return json.loads(f.read().decode('latin1'))\"}, {\"identifier\":\"TextReminder\", \"path\":\"modules/text_reminder.py\", \"snippet\":\"class TextReminder(object):\\n    def __init__(self, contact):\\n        self.contact = contact\\n        self.child_name = contact.name\\n        self.date_of_birth = contact.date_of_birth\\n        self.phone_number = contact.phone_number\\n        self.language = contact.language_preference\\n        self.preg_signup = contact.preg_signup\\n        self.preg_update = contact.preg_update\\n\\n    # self.get_contact() is preferred to self.contact due to triggering a Django DB refresh.\\n    def get_contact(self):\\n        if self.contact:\\n            self.contact.refresh_from_db()\\n        return self.contact\\n\\n    def correct_date_for_reminder(self, years_after_birth=0, months_after_birth=0,\\n                                  weeks_after_birth=0, days_before_appointment=0):\\n        time_after_dob = relativedelta(years=years_after_birth,\\n                                       months=months_after_birth,\\n                                       weeks=weeks_after_birth)\\n        time_before_appointment = relativedelta(days=days_before_appointment)\\n        target_date = (datetime.now() - time_after_dob + time_before_appointment).date()\\n        return self.date_of_birth == target_date\\n\\n    def get_reminder_msg(self):\\n        if self.correct_date_for_reminder(weeks_after_birth=6, days_before_appointment=7):\\n            reminder = six_week_reminder_seven_days\\n        elif self.correct_date_for_reminder(weeks_after_birth=6, days_before_appointment=1):\\n            reminder = six_week_reminder_one_day\\n        elif self.correct_date_for_reminder(weeks_after_birth=10, days_before_appointment=7):\\n            reminder = ten_week_reminder_seven_days\\n        elif self.correct_date_for_reminder(weeks_after_birth=10, days_before_appointment=1):\\n            reminder = ten_week_reminder_one_day\\n        elif self.correct_date_for_reminder(weeks_after_birth=14, days_before_appointment=7):\\n            reminder = fourteen_week_reminder_seven_days\\n        elif self.correct_date_for_reminder(weeks_after_birth=14, days_before_appointment=1):\\n            reminder = fourteen_week_reminder_one_day\\n        elif self.correct_date_for_reminder(months_after_birth=9, days_before_appointment=7):\\n            reminder = nine_month_reminder_seven_days\\n        elif self.correct_date_for_reminder(months_after_birth=9, days_before_appointment=1):\\n            reminder = nine_month_reminder_one_day\\n        elif self.correct_date_for_reminder(months_after_birth=16, days_before_appointment=7):\\n            reminder = sixteen_month_reminder_seven_days\\n        elif self.correct_date_for_reminder(months_after_birth=16, days_before_appointment=1):\\n            reminder = sixteen_month_reminder_one_day\\n        elif self.correct_date_for_reminder(years_after_birth=5, days_before_appointment=7):\\n            reminder = five_year_reminder_seven_days\\n        elif self.correct_date_for_reminder(years_after_birth=5, days_before_appointment=1):\\n            reminder = five_year_reminder_one_day\\n        elif self.preg_signup_check():\\n            if self.correct_date_for_reminder(weeks_after_birth=2, days_before_appointment=0):\\n                reminder = verify_pregnant_signup_birthdate\\n            elif self.correct_date_for_reminder(weeks_after_birth=4, days_before_appointment=0):\\n                reminder = verify_pregnant_signup_birthdate\\n            else:\\n                reminder = None\\n        else:\\n            reminder = None\\n        return reminder(self.language).format(name=self.child_name) if reminder else None\\n\\n    def why_not_remind_reasons(self):\\n        reasons = []\\n        if self.get_contact().cancelled:\\n            reasons.append(\\\"Contact is cancelled.\\\")\\n        if self.get_reminder_msg() is None:\\n            reasons.append(\\\"Contact has no reminders for today's date.\\\")\\n        return reasons\\n        \\n    def should_remind_today(self):\\n        return len(self.why_not_remind_reasons()) == 0\\n\\n    def remind(self):\\n        if self.should_remind_today():\\n            logging.info(\\\"Sent reminder to \\\" + quote(self.phone_number))\\n            contact = self.get_contact()\\n            outgoing_message = Message.objects.create(contact=contact, direction=\\\"Outgoing\\\",\\n                body=self.get_reminder_msg())\\n            contact.last_contacted = outgoing_message.created_at\\n            contact.save()\\n            Texter().send(message=self.get_reminder_msg(),\\n                          phone_number=self.phone_number)\\n            outgoing_message.sent_at = datetime.now().replace(tzinfo=timezone.get_default_timezone())\\n            outgoing_message.save()\\n            return True\\n        else:\\n            return False\\n\\n    def preg_signup_check(self):\\n        return True if self.preg_signup and not self.preg_update else False\"}, {\"identifier\":\"TextProcessor\", \"path\":\"modules/text_processor.py\", \"snippet\":\"class TextProcessor(object):\\n    def __init__(self, phone_number):\\n        self.phone_number = phone_number\\n        self.set_language(default=None)\\n\\n    def set_language(self, default):\\n        if self.get_contacts().exists():\\n            self.language = self.contacts.first().language_preference or default\\n        else:\\n            self.language = None\\n\\n    def get_language(self, language, inferred_language, keyword):\\n        # Returns inferred language if the keyword is in the subscribe keywords of the inferred langauge,\\n        #  ignoring \\\"born\\\"\\n        subscribe_keys_without_born = keywords_without_word(language=inferred_language, word=\\\"born\\\")\\n        if keyword in subscribe_keys_without_born:\\n            return inferred_language\\n        return language\\n\\n    # self.get_contacts() is preferred to self.contact due to triggering a Django DB reload.\\n    def get_contacts(self):\\n        self.contacts = Contact.objects.filter(phone_number=self.phone_number)\\n        return self.contacts\\n\\n\\n    def create_contact(self, child_name, phone_number, date_of_birth, language, preg_update=False):\\n        contact = Contact.objects.filter(name=child_name, phone_number=self.phone_number).first()\\n        if contact:\\n            if contact.cancelled or preg_update:\\n                # Update and resubscribe\\n                contact.cancelled = False\\n                contact.language_preference = language\\n                contact.date_of_birth = date_of_birth\\n                contact.functional_date_of_birth = date_of_birth\\n                contact.preg_update = preg_update\\n                contact.save()\\n                return True\\n            elif Message.objects.filter(contact=contact,\\n                                        direction=\\\"Outgoing\\\",\\n                                        body=msg_subscribe(language).format(name=contact.name)).exists():\\n                # Already exists (error)\\n                logging.error(\\\"Contact for {name} at {phone} was subscribed but already exists!\\\".format(name=child_name, phone=self.phone_number))\\n                return False\\n\\n        # Otherwise, create\\n        update_dict = {\\\"delay_in_days\\\": 0,\\n                       \\\"language_preference\\\": self.language,\\n                       \\\"date_of_birth\\\": date_of_birth,\\n                       \\\"functional_date_of_birth\\\": date_of_birth,\\n                       \\\"method_of_sign_up\\\": \\\"Text\\\"}\\n        contact, _ = Contact.objects.update_or_create(name=child_name,\\n                                                      phone_number=phone_number,\\n                                                      defaults=update_dict)\\n        for group_name in [\\\"Text Sign Ups\\\",\\n                           \\\"Text Sign Ups - \\\" + self.language.title(),\\n                           \\\"Everyone - \\\" + self.language.title()]:\\n            add_contact_to_group(contact, group_name)\\n        self.get_contacts()\\n        return True\\n\\n\\n    def cancel_contacts(self):\\n        for contact in self.contacts:\\n            contact.cancelled = True\\n            contact.save()\\n        return True\\n\\n\\n    def process_subscribe(self, child_name, date_of_birth, preg_update):        \\n        if self.create_contact(child_name=child_name,\\n                               phone_number=self.phone_number,\\n                               date_of_birth=date_of_birth,\\n                               language=self.language,\\n                               preg_update=preg_update):\\n            return msg_subscribe(self.language).format(name=child_name)\\n        else:\\n            return msg_already_sub(self.language)\\n\\n\\n    def process_unsubscribe(self, child_name, date_of_birth, preg_update=False):\\n        if self.contacts.exists():\\n            contact = self.get_contacts().first()\\n            if contact.name is None or contact.date_of_birth is None or contact.language_preference is None:\\n                logging.error(quote(self.phone_number) + \\\" asked to be unsubscribed but some data is missing on the existing contact object.\\\")\\n            self.cancel_contacts()\\n        else:\\n            logging.error(quote(self.phone_number) + \\\" asked to be unsubscribed but does not exist.\\\")\\n        return msg_unsubscribe(self.language or \\\"English\\\")\\n\\n\\n    def process_failure(self, child_name, date_of_birth, preg_update=False):\\n        return msg_failure(self.language)\\n\\n\\n    def process_failed_date(self, child_name, date_of_birth, preg_update=False):\\n        return msg_failed_date(self.language)\\n\\n\\n    def get_data_from_message(self, message):\\n        \\\"\\\"\\\"Get the keyword, child name, and the date from the message.\\n            A text will look like `<KEYWORD> <CHILD> <DATE OF BIRTH>`, like\\n            `REMIND NATHAN 25/11/2015`. Sometimes the child name is omitted.\\\"\\\"\\\"\\n        message = message.lower().split(\\\" \\\")\\n        if len(message) == 1:\\n            keyword = message[0]\\n            date = None\\n            child_name = None\\n        elif len(message) == 2:\\n            keyword, date = message\\n            child_name = None\\n        else:\\n            keyword, child_name, date = message[0:3]\\n        date = date_string_to_date(date) if date and date_is_valid(date) else None\\n        return (keyword, child_name, date)\\n\\n    def write_to_database(self, message, date):\\n        keyword, child_name, date_entered = self.get_data_from_message(message)\\n        inferred_language = \\\"Hindi\\\" if keyword and keyword[0] not in string.ascii_lowercase else \\\"English\\\"\\n        language = self.language or inferred_language\\n\\n        if language != inferred_language:\\n            language = self.get_language(language=language,\\n                                            inferred_language=inferred_language,\\n                                            keyword=keyword)\\n\\n        if not child_name and self.get_contacts():\\n            contact = self.get_contacts().first()\\n            child_name = contact.name\\n\\n        if child_name:\\n            child_name = child_name.title()\\n\\n        incoming = self.create_message_object(child_name=child_name,\\n                                              phone_number=self.phone_number,\\n                                              language=language,\\n                                              body=message,\\n                                              direction=\\\"Incoming\\\")\\n\\n        contact = Contact.objects.get(pk=incoming.contact.id)\\n        contact.last_heard_from = incoming.created_at\\n        incoming.received_at = date\\n        incoming.save()\\n        contact.save()\\n        self.get_contacts()\\n        return incoming\\n\\n    def process(self, message):\\n        \\\"\\\"\\\"This is the main function that is run on an message to process it.\\\"\\\"\\\"\\n        contact = Contact.objects.get(pk=message.contact.id)\\n        keyword, child_name, date = self.get_data_from_message(message.body)\\n        preg_update = False\\n        if keyword in subscribe_keywords(\\\"English\\\"):\\n            self.set_language(default=\\\"English\\\")\\n            if keyword == \\\"born\\\":\\n                preg_update = True\\n            action = self.process_subscribe\\n        elif keyword in subscribe_keywords(\\\"Hindi\\\"):\\n            self.set_language(default=\\\"Hindi\\\")\\n            if keyword == hindi_born():\\n                preg_update = True\\n            action = self.process_subscribe\\n        elif keyword == \\\"end\\\":\\n            self.set_language(default=\\\"English\\\")\\n            action = self.process_unsubscribe\\n        else:\\n            self.set_language(default=\\\"English\\\")\\n            logging.error(\\\"Keyword \\\" + quote(keyword) + \\\" in message \\\" + quote(message.body) +\\n                          \\\" was not understood by the system.\\\")\\n            action = self.process_failure\\n\\n        if action == self.process_subscribe:\\n            if child_name is None:\\n                # If a child name is not found, we call them \\\"your child\\\".\\n                child_name = msg_placeholder_child(self.language)\\n            else:\\n                child_name = child_name.title()\\n\\n            if len(child_name) > 50:\\n                action = self.process_failure\\n\\n            if date is None:\\n                logging.error(\\\"Date in message \\\" + quote(message.body) + \\\" is invalid.\\\")\\n                action = self.process_failed_date\\n\\n        if action == self.process_subscribe:\\n            logging.info(\\\"Subscribing \\\" + quote(message.body) + \\\"...\\\")\\n        elif action == self.process_unsubscribe:\\n            logging.info(\\\"Unsubscribing \\\" + quote(contact.phone_number) + \\\"...\\\")\\n\\n        response_text_message = action(child_name=child_name,\\n                                       date_of_birth=date,\\n                                       preg_update=preg_update)\\n\\n        outgoing = self.create_message_object(child_name=contact.name,\\n                                              phone_number=contact.phone_number,\\n                                              language=self.language,\\n                                              body=response_text_message,\\n                                              direction=\\\"Outgoing\\\")\\n        contact = Contact.objects.get(pk=outgoing.contact.id)\\n        contact.last_contacted = outgoing.created_at\\n        contact.save()\\n        self.get_contacts()\\n        Texter().send(message=response_text_message,\\n                      phone_number=self.phone_number)\\n        outgoing.is_processed = True\\n        outgoing.sent_at = datetime.now().replace(tzinfo=timezone.get_default_timezone())\\n        outgoing.save()\\n        message.is_processed = True\\n        message.save()\\n        return response_text_message\\n\\n    def create_message_object(self, child_name, phone_number, language, body, direction):\\n        if not child_name or len(child_name) > 50:\\n            if not language:\\n                language = \\\"English\\\"\\n            child_name = msg_placeholder_child(language)\\n        try:\\n            contact, _ = Contact.objects.get_or_create(name=child_name,\\n                                                       phone_number=phone_number)\\n        except MultipleObjectsReturned:\\n            contact = Contact.objects.filter(name=child_name,\\n                                             phone_number=phone_number).first()\\n\\n        contact.language_preference = language\\n        contact.save()\\n        return Message.objects.create(contact=contact, direction=direction, body=body)\"}, {\"identifier\":\"date_to_date_string\", \"path\":\"modules/date_helper.py\", \"snippet\":\"def date_to_date_string(date):\\n    return '{d}/{m}/{y}'.format(d = date.day,\\n                                m = date.month,\\n                                y = date.year)\"}, {\"identifier\":\"msg_subscribe\", \"path\":\"modules/i18n.py\", \"snippet\":\"def msg_subscribe(language):\\n    if language == \\\"English\\\":\\n        return \\\"{name} has been subscribed to CSH health reminders. Text END to \\\" + TEXTLOCAL_PHONENUMBER + \\\" to unsubscribe.\\\"\\n    elif language == \\\"Hindi\\\":\\n        return u'{name} \\\\u0938\\\\u0940 \\\\u090f\\\\u0938 \\\\u090f\\\\u091a \\\\u0939\\\\u0947\\\\u0932\\\\u094d\\\\u0925 \\\\u0905\\\\u0928\\\\u0941\\\\u0938\\\\u094d\\\\u092e\\\\u0930\\\\u0928 \\\\u0915\\\\u0947 \\\\u0938\\\\u0926\\\\u0938\\\\u094d\\\\u092f \\\\u0939\\\\u0948\\\\u0902. \\\\u0938\\\\u0926\\\\u0938\\\\u094d\\\\u092f\\\\u0924\\\\u093e \\\\u0930\\\\u0926\\\\u094d\\\\u0926 \\\\u0915\\\\u0930\\\\u0928\\\\u0947 \\\\u0915\\\\u0947 \\\\u0932\\\\u093f\\\\u090f \\\\u0932\\\\u093f\\\\u0916\\\\u0947\\\\u0902 \\\"END\\\" \\\\u0914\\\\u0930 \\\\u092d\\\\u0947\\\\u0902\\\\u091c \\\\u0926\\\\u0947 ' + TEXTLOCAL_PHONENUMBER + u' \\\\u092a\\\\u0930.'\"}, {\"identifier\":\"msg_unsubscribe\", \"path\":\"modules/i18n.py\", \"snippet\":\"def msg_unsubscribe(language):\\n    if language == \\\"English\\\":\\n        return \\\"You have been unsubscribed from CSH health reminders.\\\"\\n    elif language == \\\"Hindi\\\":\\n        return u'\\\\u0906\\\\u092a\\\\u0915\\\\u0940 \\\\u0938\\\\u0926\\\\u0938\\\\u094d\\\\u092f\\\\u0924\\\\u093e \\\\u0938\\\\u092e\\\\u093e\\\\u092a\\\\u094d\\\\u0924 \\\\u0915\\\\u0930 \\\\u0926\\\\u0940 \\\\u0917\\\\u092f\\\\u0940 \\\\u0939\\\\u0948.'\"}, {\"identifier\":\"hindi_remind\", \"path\":\"modules/i18n.py\", \"snippet\":\"def hindi_remind():\\n    return u'\\\\u0938\\\\u094d\\\\u092e\\\\u0930\\\\u0923'\"}, {\"identifier\":\"six_week_reminder_one_day\", \"path\":\"modules/i18n.py\", \"snippet\":\"def six_week_reminder_one_day(language):\\n    if language == \\\"English\\\":\\n        return \\\"{name} is due for their important vaccinations tomorrow. Please do so then.\\\"\\n    elif language == \\\"Hindi\\\":\\n        return u'\\\\u0905\\\\u0917\\\\u0932\\\\u0947 1 \\\\u0926\\\\u093f\\\\u0928 \\\\u092e\\\\u0947\\\\u0902 {name} \\\\u0915\\\\u0940 \\\\u091c\\\\u093c\\\\u0930\\\\u0942\\\\u0930\\\\u0940 \\\\u091f\\\\u0940\\\\u0915\\\\u093e\\\\u0915\\\\u0930\\\\u0923 \\\\u0905\\\\u0935\\\\u0936\\\\u094d\\\\u092f \\\\u0915\\\\u0930\\\\u0935\\\\u093e\\\\u090f\\\\u0901.'\\n    elif language == \\\"Gujarati\\\":\\n        return u'{name} \\\\u0aa8 \\\\u0ac1\\\\u0a82\\\\u0ae7 \\\\u0aa6\\\\u0abf\\\\u0ab5\\\\u0ab8\\\\u0aae \\\\u0ac1\\\\u0a82\\\\u0aae\\\\u0ab9\\\\u0aa4\\\\u0acd\\\\u0ab5\\\\u0aaa\\\\u0ac2\\\\u0aa3\\\\u0aa3\\\\u0ab0\\\\u0ab8\\\\u0ac0\\\\u0a95\\\\u0ab0\\\\u0aa3 \\\\u0aa6\\\\u0aa8\\\\u0aaf\\\\u0aa4 \\\\u0a9b\\\\u0ac7. \\\\u0aa4\\\\u0acb \\\\u0a95\\\\u0ac3\\\\u0aaa \\\\u0a95\\\\u0ab0\\\\u0ac0\\\\u0ac1\\\\u0a82\\\\u0aa8\\\\u0ac7\\\\u0aaa\\\\u0a9b\\\\u0ac0 \\\\u0a8f \\\\u0a95\\\\u0ab0\\\\u0a9c\\\\u0acb.'\"}, {\"identifier\":\"six_week_reminder_seven_days\", \"path\":\"modules/i18n.py\", \"snippet\":\"def six_week_reminder_seven_days(language):\\n    if language == \\\"English\\\":\\n        return \\\"{name} has their scheduled vaccination in 7 days. Without this vaccination your child will be vulnerable to deadly diseases.\\\"\\n    elif language == \\\"Hindi\\\":\\n        return u'7 \\\\u0926\\\\u093f\\\\u0928\\\\u094b\\\\u0902 \\\\u092e\\\\u0947\\\\u0902 {name} \\\\u0915\\\\u093e \\\\u091f\\\\u0940\\\\u0915\\\\u093e\\\\u0915\\\\u0930\\\\u0923 \\\\u0915\\\\u0930\\\\u0935\\\\u093e\\\\u090f\\\\u0901 \\\\u090f\\\\u0935\\\\u0902 \\\\u0916\\\\u093c\\\\u0924\\\\u0930\\\\u0928\\\\u093e\\\\u0915 \\\\u092c\\\\u0940\\\\u092e\\\\u093e\\\\u0930\\\\u093f\\\\u0913\\\\u0902 \\\\u0938\\\\u0947 \\\\u092c\\\\u091a\\\\u093e\\\\u090f\\\\u0901.'\\n    elif language == \\\"Gujarati\\\":\\n        return u'{name} \\\\u0aa8 \\\\u0ac1\\\\u0a82\\\\u0aed \\\\u0aa6\\\\u0abf\\\\u0ab5\\\\u0ab8 \\\\u0aae \\\\u0ac1\\\\u0aa8 \\\\u0ac1\\\\u0a82\\\\u0aed \\\\u0aa6\\\\u0abf\\\\u0ab5\\\\u0ab8 \\\\u0aae \\\\u0ac1\\\\u0a82\\\\u0ab0\\\\u0ab8\\\\u0ac0\\\\u0a95\\\\u0ab0\\\\u0aa3 \\\\u0ab8 \\\\u0aa6\\\\u0aa8\\\\u0aa6\\\\u0abf\\\\u0aa4 \\\\u0a9b\\\\u0ac7. \\\\u0a86 \\\\u0ab0\\\\u0ab8\\\\u0ac0\\\\u0a95\\\\u0ab0\\\\u0aa3 \\\\u0ab5\\\\u0a97\\\\u0ab0 \\\\u0aa4\\\\u0aae \\\\u0ab0\\\\u0ac1\\\\u0a82 \\\\u0aac \\\\u0ab3\\\\u0a95 \\\\u0a9c\\\\u0ac0\\\\u0ab5\\\\u0ab2\\\\u0ac7\\\\u0aa3 \\\\u0ab0\\\\u0acb\\\\u0a97\\\\u0acb \\\\u0aae \\\\u0a9f\\\\u0ac7\\\\u0ab8\\\\u0ac1\\\\u0a82\\\\u0ab5\\\\u0ac7\\\\u0abf\\\\u0aa8\\\\u0ab6\\\\u0ac0\\\\u0ab2 \\\\u0ab0\\\\u0ab9\\\\u0ac7\\\\u0ab6\\\\u0ac7.'\"}, {\"identifier\":\"ten_week_reminder_seven_days\", \"path\":\"modules/i18n.py\", \"snippet\":\"def ten_week_reminder_seven_days(language):\\n    if language == \\\"English\\\":\\n        return \\\"{name} is eligible for a free vaccination in 7 days. Without this vaccination your child will be vulnerable to deadly diseases.\\\"\\n    elif language == \\\"Hindi\\\":\\n        return u'7 \\\\u0926\\\\u093f\\\\u0928\\\\u094b\\\\u0902 \\\\u092e\\\\u0947\\\\u0902 {name} \\\\u0915\\\\u0940 \\\\u0928\\\\u093f\\\\u0903\\\\u0936\\\\u0941\\\\u0932\\\\u094d\\\\u0915 \\\\u091f\\\\u0940\\\\u0915\\\\u093e\\\\u0915\\\\u0930\\\\u0923 \\\\u0915\\\\u0930\\\\u0935\\\\u093e\\\\u090f\\\\u0901 \\\\u090f\\\\u0935\\\\u0902 \\\\u0916\\\\u093c\\\\u0924\\\\u0930\\\\u0928\\\\u093e\\\\u0915 \\\\u092c\\\\u0940\\\\u092e\\\\u093e\\\\u0930\\\\u093f\\\\u0913\\\\u0902 \\\\u0938\\\\u0947 \\\\u092c\\\\u091a\\\\u093e\\\\u090f\\\\u0901.'\\n    elif language == \\\"Gujarati\\\":\\n        return u'{name} \\\\u0aed \\\\u0aa6\\\\u0abf\\\\u0ab5\\\\u0ab8\\\\u0aae \\\\u0ac1\\\\u0a82\\\\u0aae\\\\u0aab\\\\u0aa4 \\\\u0ab0\\\\u0ab8\\\\u0ac0\\\\u0a95\\\\u0ab0\\\\u0aa3 \\\\u0aae \\\\u0a9f\\\\u0ac7\\\\u0aaa \\\\u0aa4\\\\u0acd\\\\u0ab0 \\\\u0a9b\\\\u0ac7. \\\\u0a86 \\\\u0ab0\\\\u0ab8\\\\u0ac0\\\\u0a95\\\\u0ab0\\\\u0aa3 \\\\u0ab5\\\\u0a97\\\\u0ab0 \\\\u0aa4\\\\u0aae \\\\u0ab0\\\\u0ac2 \\\\u0aac \\\\u0ab3\\\\u0a95 \\\\u0a9c\\\\u0ac0\\\\u0ab5\\\\u0ab2\\\\u0ac7\\\\u0aa3 \\\\u0ab0\\\\u0acb\\\\u0a97\\\\u0acb \\\\u0aae \\\\u0a9f\\\\u0ac7\\\\u0ab8\\\\u0ac1\\\\u0a82\\\\u0ab5\\\\u0ac7\\\\u0abf\\\\u0aa8\\\\u0ab6\\\\u0ac0\\\\u0ab2 \\\\u0ab0\\\\u0ab9\\\\u0ac7\\\\u0ab6\\\\u0ac7.'\"}, {\"identifier\":\"ten_week_reminder_one_day\", \"path\":\"modules/i18n.py\", \"snippet\":\"def ten_week_reminder_one_day(language):\\n    if language == \\\"English\\\":\\n        return \\\"{name} is due for their important vaccinations tomorrow. Please do so then.\\\"\\n    elif language == \\\"Hindi\\\":\\n        return u'\\\\u091c\\\\u093c\\\\u093f\\\\u092e\\\\u094d\\\\u092e\\\\u0947\\\\u0926\\\\u093e\\\\u0930 \\\\u092e\\\\u093e\\\\u0924\\\\u093e \\\\u0939\\\\u094b\\\\u0928\\\\u0947 \\\\u0915\\\\u0947 \\\\u0932\\\\u093f\\\\u090f \\\\u0906\\\\u092a\\\\u0915\\\\u094b \\\\u092c\\\\u0927\\\\u093e\\\\u0908. 1 \\\\u0926\\\\u093f\\\\u0928 \\\\u092e\\\\u0947\\\\u0902 {name} \\\\u0915\\\\u0947 \\\\u091c\\\\u093c\\\\u0930\\\\u0942\\\\u0930\\\\u0940 \\\\u091f\\\\u0940\\\\u0915\\\\u093e\\\\u0915\\\\u0930\\\\u0923 \\\\u0915\\\\u0947 \\\\u0932\\\\u093f\\\\u090f \\\\u0906\\\\u092a \\\\u0938\\\\u0947 \\\\u092e\\\\u0941\\\\u0932\\\\u093e\\\\u0915\\\\u093c\\\\u093e\\\\u0924 \\\\u0939\\\\u094b\\\\u0917\\\\u0940.'\\n    elif language == \\\"Gujarati\\\":\\n        return u'{name} \\\\u0aa8 \\\\u0ac1\\\\u0a82\\\\u0ae7 \\\\u0aa6\\\\u0abf\\\\u0ab5\\\\u0ab8\\\\u0aae \\\\u0ac1\\\\u0a82\\\\u0aae\\\\u0ab9\\\\u0aa4\\\\u0acd\\\\u0ab5\\\\u0aaa\\\\u0ac2\\\\u0aa3\\\\u0aa3\\\\u0ab0\\\\u0ab8\\\\u0ac0\\\\u0a95\\\\u0ab0\\\\u0aa3 \\\\u0aa6\\\\u0aa8\\\\u0aaf\\\\u0aa4 \\\\u0a9b\\\\u0ac7. \\\\u0aa4\\\\u0acb \\\\u0a95\\\\u0ac3\\\\u0aaa \\\\u0a95\\\\u0ab0\\\\u0ac0\\\\u0ac1\\\\u0a82\\\\u0aa8\\\\u0ac7\\\\u0aaa\\\\u0a9b\\\\u0ac0 \\\\u0a8f \\\\u0a95\\\\u0ab0\\\\u0a9c\\\\u0acb.'\"}, {\"identifier\":\"fourteen_week_reminder_seven_days\", \"path\":\"modules/i18n.py\", \"snippet\":\"def fourteen_week_reminder_seven_days(language):\\n    if language == \\\"English\\\":\\n        return \\\"Thank you for being a responsible mother. {name} is due for their important vaccinations in 7 days. Please do so then.\\\"\\n    elif language == \\\"Hindi\\\":\\n        return u'{name} \\\\u0915\\\\u0947 \\\\u091f\\\\u0940\\\\u0915\\\\u093e\\\\u0915\\\\u0930\\\\u0923 7 \\\\u0926\\\\u093f\\\\u0928\\\\u094b\\\\u0902 \\\\u092e\\\\u0947\\\\u0902 \\\\u0905\\\\u0935\\\\u0936\\\\u094d\\\\u092f \\\\u0915\\\\u0930\\\\u0935\\\\u093e\\\\u090f\\\\u0901.'\\n    elif language == \\\"Gujarati\\\":\\n        return u'\\\\u0a8f\\\\u0a95 \\\\u0a9c\\\\u0ab5 \\\\u0aac\\\\u0abf \\\\u0ab0 \\\\u0aae \\\\u0aa4 \\\\u0ab9\\\\u0acb\\\\u0ab5 \\\\u0aac\\\\u0abf\\\\u0ab2 \\\\u0aa7\\\\u0aa8\\\\u0acd\\\\u0aaf\\\\u0ab5 \\\\u0abf. {name} \\\\u0aa8 \\\\u0ac1\\\\u0a82\\\\u0aed \\\\u0aa6\\\\u0abf\\\\u0ab5\\\\u0ab8\\\\u0aae \\\\u0ac1\\\\u0a82\\\\u0aae\\\\u0ab9\\\\u0aa4\\\\u0acd\\\\u0ab5\\\\u0aaa\\\\u0ac2\\\\u0aa3\\\\u0aa3\\\\u0ab0\\\\u0ab8\\\\u0ac0\\\\u0a95\\\\u0ab0\\\\u0aa3 \\\\u0aa6\\\\u0aa8\\\\u0aaf\\\\u0aa4 \\\\u0a9b\\\\u0ac7. \\\\u0aa4\\\\u0acb \\\\u0a95\\\\u0ac3\\\\u0aaa \\\\u0a95\\\\u0ab0\\\\u0ac0\\\\u0ac1\\\\u0a82\\\\u0aa8\\\\u0ac7\\\\u0aaa\\\\u0a9b\\\\u0ac0 \\\\u0a8f \\\\u0a95\\\\u0ab0\\\\u0a9c\\\\u0acb.'\"}, {\"identifier\":\"fourteen_week_reminder_one_day\", \"path\":\"modules/i18n.py\", \"snippet\":\"def fourteen_week_reminder_one_day(language):\\n    if language == \\\"English\\\":\\n        return \\\"Your child is eligible to receive a free course of vaccines. {name} has their scheduled vaccination tomorrow.\\\"\\n    elif language == \\\"Hindi\\\":\\n        return u'1 \\\\u0926\\\\u093f\\\\u0928 \\\\u092e\\\\u0947\\\\u0902 {name} \\\\u0915\\\\u0940 \\\\u091f\\\\u0940\\\\u0915\\\\u093e\\\\u0915\\\\u0930\\\\u0923 \\\\u0915\\\\u0930\\\\u0935\\\\u093e\\\\u090f\\\\u0901 \\\\u090f\\\\u0935\\\\u0902 \\\\u0916\\\\u093c\\\\u0924\\\\u0930\\\\u0928\\\\u093e\\\\u0915 \\\\u092c\\\\u0940\\\\u092e\\\\u093e\\\\u0930\\\\u093f\\\\u0913\\\\u0902 \\\\u0938\\\\u0947 \\\\u092c\\\\u091a\\\\u093e\\\\u090f\\\\u0901.'\\n    elif language == \\\"Gujarati\\\":\\n        return u'\\\\u0aa1\\\\u0ac9\\\\u0a95\\\\u0acd\\\\u0a9f\\\\u0ab0\\\\u0ac9 \\\\u0a8f \\\\u0ae7 \\\\u0aa6\\\\u0abf\\\\u0ab5\\\\u0ab8\\\\u0aae\\\\u0abe\\\\u0a82  {name} \\\\u0aa8\\\\u0ac7 \\\\u0ab0\\\\u0ab8\\\\u0ac0\\\\u0a95\\\\u0ab0\\\\u0aa3 \\\\u0aae\\\\u0abe\\\\u0a9f\\\\u0ac7 \\\\u0ab8\\\\u0ac2\\\\u0a9a\\\\u0ab5\\\\u0acd\\\\u0aaf\\\\u0ac1 \\\\u0a9b\\\\u0ac7.'\"}, {\"identifier\":\"msg_placeholder_child\", \"path\":\"modules/i18n.py\", \"snippet\":\"def msg_placeholder_child(language):\\n    if language == \\\"English\\\":\\n        return \\\"Your child\\\"\\n    elif language == \\\"Hindi\\\":\\n        return hindi_placeholder_name()\\n    elif language == \\\"Gujarati\\\":\\n        return gujarati_placeholder_name()\"}, {\"identifier\":\"hindi_born\", \"path\":\"modules/i18n.py\", \"snippet\":\"def hindi_born():\\n    return u'\\\\u091c\\\\u0928\\\\u094d\\\\u092e'\"}, {\"identifier\":\"verify_pregnant_signup_birthdate\", \"path\":\"modules/i18n.py\", \"snippet\":\"def verify_pregnant_signup_birthdate(language):\\n    if language == \\\"English\\\":\\n        return \\\"You signed up for health reminders while pregnant. If your child has been born, text 'BORN Name DD-MM-YY' to \\\"+ TEXTLOCAL_PHONENUMBER + \\\" to register for  English. An example message is 'BORN Aarav 14-01-17' where 'Aarav' is your child's first name and '14-01-17' is your child's birthday.\\\"\\n    elif language == \\\"Hindi\\\":\\n        return u'\\\\u0905\\\\u092a\\\\u0928\\\\u0940 \\\\u0917\\\\u0930\\\\u094d\\\\u092d\\\\u093e\\\\u0935\\\\u0938\\\\u094d\\\\u0920\\\\u093e \\\\u0915\\\\u0947 \\\\u0926\\\\u094c\\\\u0930\\\\u093e\\\\u0928 \\\\u0906\\\\u092a\\\\u0928\\\\u0947 \\\\u0938\\\\u094d\\\\u0935\\\\u093e\\\\u0938\\\\u094d\\\\u0925\\\\u094d\\\\u092f \\\\u0938\\\\u094d\\\\u092e\\\\u0930\\\\u0923 \\\\u0915\\\\u0940 \\\\u0938\\\\u0926\\\\u0938\\\\u094d\\\\u092f\\\\u0924\\\\u093e \\\\u0932\\\\u0940 \\\\u0925\\\\u0940. \\\\u092f\\\\u0926\\\\u093f \\\\u0906\\\\u092a\\\\u0915\\\\u093e \\\\u092c\\\\u091a\\\\u094d\\\\u091a\\\\u093e \\\\u091c\\\\u0928\\\\u094d\\\\u092e \\\\u0932\\\\u0947 \\\\u091a\\\\u0942\\\\u0915\\\\u093e \\\\u0939\\\\u0948 \\\\u0924\\\\u094b \\\\u0932\\\\u093f\\\\u0916\\\\u0947 \\\\u201c \\\\u091c\\\\u0928\\\\u094d\\\\u092e \\\\u0928\\\\u093e\\\\u092e DD-MM-YY \\\\u201d \\\\u0914\\\\u0930 \\\\u092d\\\\u0947\\\\u0902\\\\u091c \\\\u0926\\\\u0947 \\\\u201c ' + TEXTLOCAL_PHONENUMBER + u' \\\\u201d \\\\u092a\\\\u0930 \\\\u0939\\\\u093f\\\\u0902\\\\u0926\\\\u0940 \\\\u092e\\\\u0947\\\\u0902 \\\\u092a\\\\u0902\\\\u091c\\\\u0940\\\\u0915\\\\u0930\\\\u0923 \\\\u0915\\\\u0947 \\\\u0932\\\\u093f\\\\u090f.'\"}]", "import_statement": "import logging\nimport time\nfrom datetime import datetime\nfrom dateutil.relativedelta import relativedelta\nfrom django.test import TestCase\nfrom django.utils import timezone\nfrom freezegun import freeze_time\nfrom cshsms.settings import TEXTLOCAL_API, TEXTLOCAL_PRIMARY_ID, TEXTLOCAL_SENDERNAME, \\\n                            TEXTLOCAL_PHONENUMBER\nfrom management.models import Contact, Group, Message\nfrom modules.texter import Texter\nfrom modules.textlocalwrapper import TextLocal\nfrom modules.text_reminder import TextReminder\nfrom modules.text_processor import TextProcessor\nfrom modules.date_helper import date_to_date_string\nfrom modules.i18n import msg_subscribe, msg_unsubscribe, hindi_remind, \\\n                         six_week_reminder_one_day, six_week_reminder_seven_days, \\\n                         ten_week_reminder_seven_days, ten_week_reminder_one_day, \\\n                         fourteen_week_reminder_seven_days, fourteen_week_reminder_one_day, \\\n                         msg_placeholder_child, hindi_born, verify_pregnant_signup_birthdate", "code": "\n    def test_full_e2e_english_signup_and_cancel_flow(self):\n        logging.info(\"running e2e full flow for sign up + cancel in English...\")\n        self.run_e2e_flow_for_language(language=\"English\",\n                                       person_name=\"Testjohnson\",\n                                       join_keyword=\"JOIN\")\n\n    def test_full_e2e_hindi_signup_and_cancel_flow(self):\n        logging.info(\"running e2e full flow for sign up + cancel in Hindi...\")\n        self.run_e2e_flow_for_language(language=\"Hindi\",\n                                       person_name=u'\\u0906\\u0930\\u0935',\n                                       join_keyword=hindi_remind())\n\n    def test_preg_update_english_flow(self):\n        logging.info(\"running preg update flow in English...\")\n        self.run_preg_update_flow_for_language(language=\"English\",\n                                               person_name=\"Testjohnson\",\n                                               join_keyword=\"JOIN\",\n                                               born_keyword=\"BORN\")\n\n    def test_preg_update_hindi_flow(self):\n        logging.info(\"running preg update flow in Hindi...\")\n        self.run_preg_update_flow_for_language(language=\"Hindi\",\n                                               person_name=u'\\u0906\\u0930\\u0935',\n                                               join_keyword=hindi_remind(),\n                                               born_keyword=hindi_born())\n\n    def decode_nonenglish_messages(self, language, messages, textlocal_object):\n        if language is not \"English\":\n            for index, text_tuple in enumerate(messages):\n                list_message = list(text_tuple)\n                if list_message[0].startswith(\"@U\"):\n                    list_message[0] = textlocal_object.response_unicode_encoder(text_tuple[0])\n                    messages[index] = tuple(list_message)\n\n        return messages\n\n    def messages_within_timeframe(self, messages, timeframe_in_minutes):\n        datetime_in_past = datetime.now().replace(tzinfo=timezone.get_default_timezone()) - relativedelta(minutes=timeframe_in_minutes)\n        return [(m[0], m[1]) for m in messages if m[1] > datetime_in_past]\n\n\n    def run_e2e_flow_for_language(self, language, person_name, join_keyword):\n        logging.info(\"sending text\")\n        subscribe_date = datetime(2017, 1, 30).date()\n        join_text = join_keyword + \" \" + person_name + \" \" + date_to_date_string(subscribe_date)\n        t = Texter()\n        send_status = t.send(message=join_text,\n                             phone_number=TEXTLOCAL_PHONENUMBER)\n        self.assertTrue('success' in send_status['status'])\n        logging.info(\"sleeping three minutes before reading the text\")\n        time.sleep(180)\n        logging.info(\"reading text\")\n        new_messages = t.read_api_outbox()[TEXTLOCAL_PHONENUMBER]  # TODO: Match to job\n        tp = TextProcessor(TEXTLOCAL_PHONENUMBER)\n        processed = False\n        logging.info(\"checking text can be processed\")\n        self.assertFalse(Contact.objects.filter(name=person_name, phone_number=TEXTLOCAL_PHONENUMBER))\n        textlocal = TextLocal(apikey=TEXTLOCAL_API, primary_id=TEXTLOCAL_PRIMARY_ID, sendername=TEXTLOCAL_SENDERNAME)\n        decoded_messages = self.decode_nonenglish_messages(language=language, messages=new_messages, textlocal_object=textlocal)\n        selected_texts = self.messages_within_timeframe(messages=decoded_messages, timeframe_in_minutes=10)\n        for text in selected_texts:\n            body_of_text = text[0]\n            if body_of_text == join_text:\n                processed = True\n                message_object = tp.write_to_database(message=body_of_text, date=text[1])\n                tp.process(message_object)\n        self.assertTrue(processed)\n        logging.info(\"checking message objects are created\")\n        self.assertEqual(1, Message.objects.filter(contact=Contact.objects.get(name=person_name, phone_number=TEXTLOCAL_PHONENUMBER),\n        \t\t\t\t\t\t\t\t\t\t\tdirection=\"Incoming\",\n        \t\t\t\t\t\t\t\t\t\t\tbody=join_text,\n                                                    is_processed=True).count())\n        self.assertEqual(1, Message.objects.filter(contact=Contact.objects.get(name=person_name, phone_number=TEXTLOCAL_PHONENUMBER),\n        \t\t\t\t\t\t\t\t\t\t\tdirection=\"Outgoing\",\n        \t\t\t\t\t\t\t\t\t\t\tbody=msg_subscribe(language).format(name=person_name),\n                                                    is_processed=True).count())\n\n        logging.info(\"checking contact object is created\")\n        self.assertTrue(Contact.objects.filter(name=person_name,\n                                               phone_number=TEXTLOCAL_PHONENUMBER).exists())\n        self.assertTrue(tp.get_contacts().exists())\n\n        logging.info(\"checking groups are created\")\n        contact = tp.get_contacts().first()\n        actual_groups = [str(g) for g in contact.group_set.all()]\n        expected_groups = ['Everyone - {}'.format(language.title()),\n                           'Text Sign Ups',\n                           'Text Sign Ups - {}'.format(language.title())]\n        self.assertEqual(actual_groups, expected_groups)\n\n        logging.info(\"sleeping three minutes before checking for subscription text\")\n        time.sleep(180)\n        logging.info(\"checking subscription text\")\n        messages = t.read_api_outbox()[TEXTLOCAL_PHONENUMBER]\n        self.assertTrue(len(messages) >= 1)\n        decoded_messages = self.decode_nonenglish_messages(language=language, messages=messages, textlocal_object=textlocal)\n        selected_texts = self.messages_within_timeframe(messages=decoded_messages, timeframe_in_minutes=30)\n        self.assertTrue(len(selected_texts) >= 1)\n        list_of_text_bodies = [m[0] for m in selected_texts]\n        self.assertTrue(msg_subscribe(language).format(name=person_name) in list_of_text_bodies)\n        logging.info(\"checking person can be reminded at the correct times\")\n        tr = TextReminder(contact)\n\n        logging.info(\"Two days after...\")\n        with freeze_time(subscribe_date + relativedelta(days = 2)):\n            self.assertEqual(0, Message.objects.filter(contact=tr.contact, direction=\"Outgoing\").exclude(\n                                                    body=msg_subscribe(language).format(name=person_name)).count())\n            self.assertFalse(tr.remind())\n            self.assertEqual(0, Message.objects.filter(contact=tr.contact, direction=\"Outgoing\").exclude(\n                                                    body=msg_subscribe(language).format(name=person_name)).count())\n            self.assertEqual(tr.why_not_remind_reasons(),\n                             [\"Contact has no reminders for today's date.\"])\n        logging.info(\"sleeping three minutes before checking for lack of a reminder text\")\n        time.sleep(180)\n        messages = t.read_api_outbox()[TEXTLOCAL_PHONENUMBER]\n        decoded_messages = self.decode_nonenglish_messages(language=language, messages=messages, textlocal_object=textlocal)\n        selected_texts = self.messages_within_timeframe(messages=decoded_messages, timeframe_in_minutes=30)\n        list_of_text_bodies = [m[0] for m in selected_texts]\n        self.assertFalse(six_week_reminder_seven_days(language).format(name=person_name) in list_of_text_bodies)\n", "next_line": "        self.assertFalse(six_week_reminder_one_day(language).format(name=person_name) in list_of_text_bodies)", "gold_snippet_index": 12, "id": 9, "__internal_uuid__": "93423c7c-f71c-48d6-95c9-2f564d91603c"}
{"repo_name": "Philip-Bachman/Sequential-Generation", "file_path": "TestImpVAE.py", "context": "[{\"identifier\":\"relu_actfun\", \"path\":\"NetLayers.py\", \"snippet\":\"def relu_actfun(x):\\n    \\\"\\\"\\\"Compute rectified linear activation for x.\\\"\\\"\\\"\\n    x_relu = T.maximum(0., x)\\n    return x_relu\"}, {\"identifier\":\"softplus_actfun\", \"path\":\"NetLayers.py\", \"snippet\":\"def softplus_actfun(x, scale=None):\\n    \\\"\\\"\\\"Compute rescaled softplus activation for x.\\\"\\\"\\\"\\n    if scale is None:\\n        x_softplus = (1.0 / 2.0) * T.nnet.softplus(2.0*x)\\n    else:\\n        x_softplus = (1.0 / scale) * T.nnet.softplus(scale*x)\\n    return x_softplus\"}, {\"identifier\":\"tanh_actfun\", \"path\":\"NetLayers.py\", \"snippet\":\"def tanh_actfun(x, scale=None):\\n    \\\"\\\"\\\"Compute  rescaled tanh activation for x.\\\"\\\"\\\"\\n    if scale is None:\\n        x_tanh = T.tanh(x)\\n    else:\\n        x_tanh = scale * T.tanh(constFX(1/scale) * x)\\n    return x_tanh\"}, {\"identifier\":\"HydraNet\", \"path\":\"HydraNet.py\", \"snippet\":\"class HydraNet(object):\\n    \\\"\\\"\\\"\\n    A net that turns one input into one or more outputs.\\n\\n    Some shared hidden layers feed into one or more output layers.\\n\\n    Parameters:\\n        rng: a numpy.random RandomState object\\n        Xd: symbolic input matrix for inputs\\n        params: a dict of parameters describing the desired network:\\n            shared_config: list of \\\"layer descriptions\\\" for the shared layers\\n            output_config: list of \\\"layer descriptions\\\" for the output layers\\n        shared_param_dicts: parameters for this HydraNet\\n    \\\"\\\"\\\"\\n    def __init__(self,\\n            rng=None,\\n            Xd=None,\\n            params=None,\\n            shared_param_dicts=None):\\n        # Setup a shared random generator for this network\\n        self.rng = RandStream(rng.randint(1000000))\\n        # Grab the symbolic input matrix\\n        self.Xd = Xd\\n        #####################################################\\n        # Process user-supplied parameters for this network #\\n        #####################################################\\n        self.params = params\\n        if 'build_theano_funcs' in params:\\n            self.build_theano_funcs = params['build_theano_funcs']\\n        else:\\n            self.build_theano_funcs = True\\n        if 'init_scale' in params:\\n            self.init_scale = params['init_scale']\\n        else:\\n            self.init_scale = 1.0\\n        # Check if the params for this net were given a priori. This option\\n        # will be used for creating \\\"clones\\\" of an inference network, with all\\n        # of the network parameters shared between clones.\\n        if shared_param_dicts is None:\\n            # This is not a clone, and we will need to make a dict for\\n            # referring to the parameters of each network layer\\n            self.shared_param_dicts = {'shared': [], 'output': []}\\n            self.is_clone = False\\n        else:\\n            # This is a clone, and its layer parameters can be found by\\n            # referring to the given param dict (i.e. shared_param_dicts).\\n            self.shared_param_dicts = shared_param_dicts\\n            self.is_clone = True\\n        # Get the configuration/prototype for this network.\\n        self.shared_config = params['shared_config']\\n        self.output_config = params['output_config']\\n\\n        ###\\n        self.shared_layers = []\\n        #########################################\\n        # Initialize the shared part of network #\\n        #########################################\\n        for sl_num, sl_desc in enumerate(self.shared_config):\\n            l_name = \\\"shared_layer_{0:d}\\\".format(sl_num)\\n            if not self.is_clone:\\n                ##########################################\\n                # Initialize a layer with new parameters #\\n                ##########################################\\n                new_layer = HiddenLayer(rng=rng,\\n                        layer_description=sl_desc,\\n                        name=l_name, W_scale=self.init_scale)\\n                self.shared_layers.append(new_layer)\\n                self.shared_param_dicts['shared'].append(\\n                        new_layer.shared_param_dicts)\\n            else:\\n                ##################################################\\n                # Initialize a layer with some shared parameters #\\n                ##################################################\\n                init_params = self.shared_param_dicts['shared'][sl_num]\\n                new_layer = HiddenLayer(rng=rng,\\n                        layer_description=sl_desc,\\n                        W=init_params['W'], b=init_params['b'],\\n                        b_in=init_params['b_in'], s_in=init_params['s_in'],\\n                        name=l_name, W_scale=self.init_scale)\\n                self.shared_layers.append(new_layer)\\n        ################################\\n        # Initialize the output layers #\\n        ################################\\n        self.output_layers = []\\n        # take input from the output of the shared network\\n        for ol_num, ol_desc in enumerate(self.output_config):\\n            ol_name = \\\"output_layer_{0:d}\\\".format(ol_num)\\n            if not self.is_clone:\\n                ##########################################\\n                # Initialize a layer with new parameters #\\n                ##########################################\\n                new_layer = HiddenLayer(rng=rng,\\n                        layer_description=ol_desc,\\n                        name=ol_name, W_scale=self.init_scale)\\n                self.output_layers.append(new_layer)\\n                self.shared_param_dicts['output'].append(\\n                        new_layer.shared_param_dicts)\\n            else:\\n                ##################################################\\n                # Initialize a layer with some shared parameters #\\n                ##################################################\\n                init_params = self.shared_param_dicts['output'][ol_num]\\n                new_layer = HiddenLayer(rng=rng,\\n                        layer_description=ol_desc,\\n                        W=init_params['W'], b=init_params['b'],\\n                        b_in=init_params['b_in'], s_in=init_params['s_in'],\\n                        name=ol_name, W_scale=self.init_scale)\\n                self.output_layers.append(new_layer)\\n\\n        # mash all the parameters together, into a list.\\n        self.mlp_params = []\\n        for layer in self.shared_layers:\\n            self.mlp_params.extend(layer.params)\\n        for layer in self.output_layers:\\n            self.mlp_params.extend(layer.params)\\n        return\\n\\n    def apply(self, X, use_drop=False):\\n        \\\"\\\"\\\"\\n        Pass input X through this HydraNet and get the resulting outputs.\\n        \\\"\\\"\\\"\\n        # pass activations through the shared layers\\n        shared_acts = [X]\\n        for layer in self.shared_layers:\\n            layer_acts, _ = layer.apply(shared_acts[-1], use_drop=use_drop)\\n            shared_acts.append(layer_acts)\\n        shared_output = shared_acts[-1]\\n        # compute outputs of the output layers\\n        outputs = []\\n        for layer in self.output_layers:\\n            _, layer_acts = layer.apply(shared_output, use_drop=use_drop)\\n            outputs.append(layer_acts)\\n        return outputs\\n\\n    def apply_shared(self, X, use_drop=False):\\n        \\\"\\\"\\\"\\n        Pass input X through this HydraNet's shared layers.\\n        \\\"\\\"\\\"\\n        # pass activations through the shared layers\\n        shared_acts = [X]\\n        for layer in self.shared_layers:\\n            layer_acts, _ = layer.apply(shared_acts[-1], use_drop=use_drop)\\n            shared_acts.append(layer_acts)\\n        shared_output = shared_acts[-1]\\n        return shared_output\\n\\n    def init_biases(self, b_init=0.0, b_std=1e-2):\\n        \\\"\\\"\\\"\\n        Initialize the biases in all shred layers to some constant.\\n        \\\"\\\"\\\"\\n        for layer in self.shared_layers:\\n            b_vec = (0.0 * layer.b.get_value(borrow=False)) + b_init\\n            b_vec = b_vec + (b_std * npr.randn(*b_vec.shape))\\n            layer.b.set_value(to_fX(b_vec))\\n        return\\n\\n    def shared_param_clone(self, rng=None, Xd=None):\\n        \\\"\\\"\\\"\\n        Return a clone of this network, with shared parameters but with\\n        different symbolic input variables.\\n        \\\"\\\"\\\"\\n        clone_net = HydraNet(rng=rng, Xd=Xd, params=self.params, \\\\\\n                shared_param_dicts=self.shared_param_dicts)\\n        return clone_net\\n\\n    def forked_param_clone(self, rng=None, Xd=None):\\n        \\\"\\\"\\\"\\n        Return a clone of this network, with forked copies of the current\\n        shared parameters of this HydraNet, with different symbolic inputs.\\n        \\\"\\\"\\\"\\n        new_spds = {}\\n        old_spds = self.shared_param_dicts\\n        # shared param dicts is nested like: dict of list of dicts\\n        # i.e., spd[k] is a list and spd[k][i] is a dict\\n        for k1 in old_spds:\\n            new_spds[k1] = []\\n            for i in range(len(old_spds[k1])):\\n                new_spds[k1].append({})\\n                for k2 in old_spds[k1][i]:\\n                    old_sp = old_spds[k1][i][k2]\\n                    old_sp_forked = old_sp.get_value(borrow=False)\\n                    new_sp = theano.shared(value=old_sp_forked)\\n                    new_spds[k1][i][k2] = new_sp\\n        clone_net = HydraNet(rng=rng, Xd=Xd, params=self.params, \\\\\\n                shared_param_dicts=new_spds)\\n        return clone_net\\n\\n    def save_to_file(self, f_name=None):\\n        \\\"\\\"\\\"\\n        Dump important stuff to a Python pickle, so that we can reload this\\n        model later. We'll pickle everything required to create a clone of\\n        this model given the pickle and the rng/Xd params to the cloning\\n        function: \\\"HydraNet.shared_param_clone()\\\".\\n        \\\"\\\"\\\"\\n        assert(not (f_name is None))\\n        f_handle = file(f_name, 'wb')\\n        # dump the dict self.params, which just holds \\\"simple\\\" python values\\n        cPickle.dump(self.params, f_handle, protocol=-1)\\n        # make a copy of self.shared_param_dicts, with numpy arrays in place\\n        # of the theano shared variables\\n        numpy_param_dicts = {'shared': [], 'output': []}\\n        for layer_group in ['shared', 'output']:\\n            for shared_dict in self.shared_param_dicts[layer_group]:\\n                numpy_dict = {}\\n                for key in shared_dict:\\n                    numpy_dict[key] = shared_dict[key].get_value(borrow=False)\\n                numpy_param_dicts[layer_group].append(numpy_dict)\\n        # dump the numpy version of self.shared_param_dicts to pickle file\\n        cPickle.dump(numpy_param_dicts, f_handle, protocol=-1)\\n        f_handle.close()\\n        return\\n\\n    def save_to_dict(self):\\n        \\\"\\\"\\\"\\n        Dump important stuff to a dict capable of rebooting the model.\\n        \\\"\\\"\\\"\\n        model_dict = {}\\n        # dump the dict self.params, which just holds \\\"simple\\\" python values\\n        model_dict['params'] = self.params\\n        # make a copy of self.shared_param_dicts, with numpy arrays in place\\n        # of the theano shared variables\\n        numpy_param_dicts = {'shared': [], 'output': []}\\n        for layer_group in ['shared', 'output']:\\n            for shared_dict in self.shared_param_dicts[layer_group]:\\n                numpy_dict = {}\\n                for key in shared_dict:\\n                    numpy_dict[key] = shared_dict[key].get_value(borrow=False)\\n                numpy_param_dicts[layer_group].append(numpy_dict)\\n        # dump the numpy version of self.shared_param_dicts to the dict\\n        model_dict['numpy_param_dicts'] = numpy_param_dicts\\n        return model_dict\"}, {\"identifier\":\"GPSImputer\", \"path\":\"GPSImputer.py\", \"snippet\":\"class GPSImputer(object):\\n    \\\"\\\"\\\"\\n    Controller for training a multi-step imputater via guided policy search.\\n\\n    Parameters:\\n        rng: numpy.random.RandomState (for reproducibility)\\n        x_in: the initial state for imputation\\n        x_out: the goal state for imputation\\n        x_mask: mask for state dims to keep fixed during imputation\\n        p_zi_given_xi: HydraNet for stochastic part of step (2 outputs)\\n        p_sip1_given_zi: HydraNet for deterministic part of step (3 outputs)\\n        q_zi_given_xi: HydraNet for the guide policy (2 outputs)\\n        params: REQUIRED PARAMS SHOWN BELOW\\n                x_dim: dimension of inputs to reconstruct\\n                z_dim: dimension of latent space for policy wobble\\n                imp_steps: number of reconstruction steps to perform\\n                step_type: either \\\"add\\\", \\\"jump\\\", \\\"lstm\\\", or \\\"layer\\\"\\n                x_type: can be \\\"bernoulli\\\" or \\\"gaussian\\\"\\n    \\\"\\\"\\\"\\n    def __init__(self, rng=None,\\n            x_in=None, x_mask=None, x_out=None, \\\\\\n            p_zi_given_xi=None, \\\\\\n            p_sip1_given_zi=None, \\\\\\n            q_zi_given_xi=None, \\\\\\n            params=None, \\\\\\n            shared_param_dicts=None):\\n        # setup a rng for this GIPair\\n        self.rng = RandStream(rng.randint(100000))\\n\\n        # grab the user-provided parameters\\n        self.params = params\\n        self.x_dim = self.params['x_dim']\\n        self.z_dim = self.params['z_dim']\\n        self.imp_steps = self.params['imp_steps']\\n        self.step_type = self.params['step_type']\\n        self.x_type = self.params['x_type']\\n        assert((self.x_type == 'bernoulli') or (self.x_type == 'gaussian'))\\n        self.shared_param_dicts = shared_param_dicts\\n\\n        # grab handles to the relevant InfNets\\n        self.p_zi_given_xi = p_zi_given_xi\\n        self.p_sip1_given_zi = p_sip1_given_zi\\n        self.q_zi_given_xi = q_zi_given_xi\\n\\n        # record the symbolic variables that will provide inputs to the\\n        # computation graph created to describe this MultiStageModel\\n        self.x_in = x_in\\n        self.x_out = x_out\\n        self.x_mask = x_mask\\n        self.zi_zmuv = T.tensor3()\\n\\n        # setup switching variable for changing between sampling/training\\n        zero_ary = to_fX( np.zeros((1,)) )\\n        self.train_switch = theano.shared(value=zero_ary, name='msm_train_switch')\\n        self.set_train_switch(1.0)\\n\\n        if self.shared_param_dicts is None:\\n            # initialize parameters \\\"owned\\\" by this model\\n            s0_init = to_fX( np.zeros((self.x_dim,)) )\\n            init_ary = to_fX( np.zeros((self.x_dim,)) )\\n            self.x_null = theano.shared(value=init_ary, name='gpis_xn')\\n            self.grad_null = theano.shared(value=init_ary, name='gpsi_gn')\\n            self.s0 = theano.shared(value=s0_init, name='gpsi_s0')\\n            self.obs_logvar = theano.shared(value=zero_ary, name='gpsi_obs_logvar')\\n            self.bounded_logvar = 8.0 * T.tanh((1.0/8.0) * self.obs_logvar[0])\\n            self.shared_param_dicts = {}\\n            self.shared_param_dicts['x_null'] = self.x_null\\n            self.shared_param_dicts['grad_null'] = self.grad_null\\n            self.shared_param_dicts['s0'] = self.s0\\n            self.shared_param_dicts['obs_logvar'] = self.obs_logvar\\n        else:\\n            # grab the parameters required by this model from a given dict\\n            self.x_null = self.shared_param_dicts['x_null']\\n            self.grad_null = self.shared_param_dicts['grad_null']\\n            self.s0 = self.shared_param_dicts['s0']\\n            self.obs_logvar = self.shared_param_dicts['obs_logvar']\\n            self.bounded_logvar = 8.0 * T.tanh((1.0/8.0) * self.obs_logvar[0])\\n\\n        ##################################################\\n        # Setup the iterative imputation loop using scan #\\n        ##################################################\\n        self.ones_mask = T.ones_like(self.x_mask)\\n        def imp_step_func(zi_zmuv, si):\\n            si_as_x = self._si_as_x(si)\\n            xi_unmasked = self.x_out\\n            xi_masked = (self.x_mask * xi_unmasked) + \\\\\\n                        ((1.0 - self.x_mask) * si_as_x)\\n            grad_unmasked = self.x_out - si_as_x\\n            grad_masked = self.x_mask * grad_unmasked\\n            # get samples of next zi, according to the global policy\\n            zi_p_mean, zi_p_logvar = self.p_zi_given_xi.apply(xi_masked)\\n            zi_p = zi_p_mean + (T.exp(0.5 * zi_p_logvar) * zi_zmuv)\\n            # get samples of next zi, according to the guide policy\\n            zi_q_mean, zi_q_logvar = self.q_zi_given_xi.apply(\\n                    T.concatenate([xi_masked, xi_unmasked], axis=1))\\n            zi_q = zi_q_mean + (T.exp(0.5 * zi_q_logvar) * zi_zmuv)\\n\\n            # make zi samples that can be switched between zi_p and zi_q\\n            zi = ((self.train_switch[0] * zi_q) + \\\\\\n                 ((1.0 - self.train_switch[0]) * zi_p))\\n            # compute relevant KLds for this step\\n            kldi_q2p = gaussian_kld(zi_q_mean, zi_q_logvar,\\n                                    zi_p_mean, zi_p_logvar) # KL(q || p)\\n            kldi_p2q = gaussian_kld(zi_p_mean, zi_p_logvar,\\n                                    zi_q_mean, zi_q_logvar) # KL(p || q)\\n            kldi_p2g = gaussian_kld(zi_p_mean, zi_p_logvar,\\n                                    0.0, 0.0) # KL(p || global prior)\\n\\n            # compute the next si, given the sampled zi\\n            hydra_out = self.p_sip1_given_zi.apply(zi)\\n            si_step = hydra_out[0]\\n            if (self.step_type == 'jump'):\\n                # jump steps always completely overwrite the current guesses\\n                sip1 = si_step\\n            elif (self.step_type == 'add'):\\n                # add steps just update the guesses additively\\n                sip1 = si + si_step\\n            elif (self.step_type == 'lstm'):\\n                # LSTM-style updates with write and erase gates\\n                write_gate = 1.1 * T.nnet.sigmoid(1.0 + hydra_out[1])\\n                erase_gate = 1.1 * T.nnet.sigmoid(1.0 + hydra_out[2])\\n                sip1 = (erase_gate * si) + (write_gate * si_step)\\n            elif (self.step_type == 'layer'):\\n                alpha_gate = T.nnet.sigmoid(hydra_out[1])\\n                sip1 = (alpha_gate * si) + ((1.0 - alpha_gate) * si_step)\\n            else:\\n                assert False, \\\"Unknown step type!\\\"\\n\\n            # compute NLL for the current imputation\\n            nlli = self._construct_nll_costs(sip1, self.x_out, self.x_mask)\\n            return sip1, nlli, kldi_q2p, kldi_p2q, kldi_p2g\\n\\n        # apply scan op for the sequential imputation loop\\n        self.s0_full = T.alloc(0.0, self.x_in.shape[0], self.x_dim) + self.s0\\n        init_vals = [self.s0_full, None, None, None, None]\\n        self.scan_results, self.scan_updates = theano.scan(imp_step_func, \\\\\\n                    outputs_info=init_vals, sequences=self.zi_zmuv)\\n\\n        self.si = self.scan_results[0]\\n        self.nlli = self.scan_results[1]\\n        self.kldi_q2p = self.scan_results[2]\\n        self.kldi_p2q = self.scan_results[3]\\n        self.kldi_p2g = self.scan_results[4]\\n\\n        # get the initial imputation state\\n        self.x0 = (self.x_mask * self.x_in) + \\\\\\n                  ((1.0 - self.x_mask) * self._si_as_x(self.s0_full))\\n\\n        ######################################################################\\n        # ALL SYMBOLIC VARS NEEDED FOR THE OBJECTIVE SHOULD NOW BE AVAILABLE #\\n        ######################################################################\\n\\n        # shared var learning rate for generator and inferencer\\n        zero_ary = to_fX( np.zeros((1,)) )\\n        self.lr = theano.shared(value=zero_ary, name='gpsi_lr')\\n        # shared var momentum parameters for generator and inferencer\\n        self.mom_1 = theano.shared(value=zero_ary, name='gpsi_mom_1')\\n        self.mom_2 = theano.shared(value=zero_ary, name='gpsi_mom_2')\\n        # init parameters for controlling learning dynamics\\n        self.set_sgd_params()\\n        # init shared var for weighting nll of data given posterior sample\\n        self.lam_nll = theano.shared(value=zero_ary, name='gpsi_lam_nll')\\n        self.set_lam_nll(lam_nll=1.0)\\n        # init shared var for weighting prior kld against reconstruction\\n        self.lam_kld_p = theano.shared(value=zero_ary, name='gpsi_lam_kld_p')\\n        self.lam_kld_q = theano.shared(value=zero_ary, name='gpsi_lam_kld_q')\\n        self.lam_kld_g = theano.shared(value=zero_ary, name='gpsi_lam_kld_g')\\n        self.set_lam_kld(lam_kld_p=0.05, lam_kld_q=0.95, lam_kld_g=0.0)\\n        # init shared var for controlling l2 regularization on params\\n        self.lam_l2w = theano.shared(value=zero_ary, name='msm_lam_l2w')\\n        self.set_lam_l2w(1e-5)\\n\\n        # Grab all of the \\\"optimizable\\\" parameters in \\\"group 1\\\"\\n        self.joint_params = [self.s0, self.obs_logvar]\\n        self.joint_params.extend(self.p_zi_given_xi.mlp_params)\\n        self.joint_params.extend(self.p_sip1_given_zi.mlp_params)\\n        self.joint_params.extend(self.q_zi_given_xi.mlp_params)\\n\\n        #################################\\n        # CONSTRUCT THE KLD-BASED COSTS #\\n        #################################\\n        self.kld_p, self.kld_q, self.kld_g = self._construct_kld_costs(p=1.0)\\n        self.kld_costs = (self.lam_kld_p[0] * self.kld_p) + \\\\\\n                         (self.lam_kld_q[0] * self.kld_q) + \\\\\\n                         (self.lam_kld_g[0] * self.kld_g)\\n        self.kld_cost = T.mean(self.kld_costs)\\n        #################################\\n        # CONSTRUCT THE NLL-BASED COSTS #\\n        #################################\\n        self.nll_costs = self.nlli[-1]\\n        self.nll_cost = self.lam_nll[0] * T.mean(self.nll_costs)\\n        self.nll_bounds = self.nll_costs.ravel() + self.kld_q.ravel()\\n        self.nll_bound = T.mean(self.nll_bounds)\\n        ########################################\\n        # CONSTRUCT THE REST OF THE JOINT COST #\\n        ########################################\\n        param_reg_cost = self._construct_reg_costs()\\n        self.reg_cost = self.lam_l2w[0] * param_reg_cost\\n        self.joint_cost = self.nll_cost + self.kld_cost + self.reg_cost\\n        ##############################\\n        # CONSTRUCT A PER-TRIAL COST #\\n        ##############################\\n        self.obs_costs = self.nll_costs + self.kld_costs\\n\\n        # Get the gradient of the joint cost for all optimizable parameters\\n        print(\\\"Computing gradients of self.joint_cost...\\\")\\n        self.joint_grads = OrderedDict()\\n        grad_list = T.grad(self.joint_cost, self.joint_params)\\n        for i, p in enumerate(self.joint_params):\\n            self.joint_grads[p] = grad_list[i]\\n\\n        # Construct the updates for the generator and inferencer networks\\n        self.joint_updates = get_adam_updates(params=self.joint_params, \\\\\\n                grads=self.joint_grads, alpha=self.lr, \\\\\\n                beta1=self.mom_1, beta2=self.mom_2, \\\\\\n                mom2_init=1e-3, smoothing=1e-4, max_grad_norm=10.0)\\n        for k, v in self.scan_updates.items():\\n            self.joint_updates[k] = v\\n\\n        # Construct a function for jointly training the generator/inferencer\\n        print(\\\"Compiling cost computer...\\\")\\n        self.compute_raw_costs = self._construct_raw_costs()\\n        print(\\\"Compiling training function...\\\")\\n        self.train_joint = self._construct_train_joint()\\n        print(\\\"Compiling free-energy sampler...\\\")\\n        self.compute_fe_terms = self._construct_compute_fe_terms()\\n        print(\\\"Compiling best step cost computer...\\\")\\n        self.compute_per_step_cost = self._construct_compute_per_step_cost()\\n        print(\\\"Compiling data-guided imputer sampler...\\\")\\n        self.sample_imputer = self._construct_sample_imputer()\\n        # make easy access points for some interesting parameters\\n        #self.gen_inf_weights = self.p_zi_given_xi.shared_layers[0].W\\n        return\\n\\n    def _si_as_x(self, si):\\n        \\\"\\\"\\\"\\n        Convert from \\\"state\\\" to \\\"observation\\\".\\n        \\\"\\\"\\\"\\n        si_as_x = T.nnet.sigmoid(si)\\n        return si_as_x\\n\\n    def set_sgd_params(self, lr=0.01, mom_1=0.9, mom_2=0.999):\\n        \\\"\\\"\\\"\\n        Set learning rate and momentum parameter for all updates.\\n        \\\"\\\"\\\"\\n        zero_ary = np.zeros((1,))\\n        # set learning rate\\n        new_lr = zero_ary + lr\\n        self.lr.set_value(to_fX(new_lr))\\n        # set momentums (use first and second order \\\"momentum\\\")\\n        new_mom_1 = zero_ary + mom_1\\n        self.mom_1.set_value(to_fX(new_mom_1))\\n        new_mom_2 = zero_ary + mom_2\\n        self.mom_2.set_value(to_fX(new_mom_2))\\n        return\\n\\n    def set_lam_nll(self, lam_nll=1.0):\\n        \\\"\\\"\\\"\\n        Set weight for controlling the influence of the data likelihood.\\n        \\\"\\\"\\\"\\n        zero_ary = np.zeros((1,))\\n        new_lam = zero_ary + lam_nll\\n        self.lam_nll.set_value(to_fX(new_lam))\\n        return\\n\\n    def set_lam_kld(self, lam_kld_p=0.0, lam_kld_q=1.0, lam_kld_g=0.0):\\n        \\\"\\\"\\\"\\n        Set the relative weight of prior KL-divergence vs. data likelihood.\\n        \\\"\\\"\\\"\\n        zero_ary = np.zeros((1,))\\n        new_lam = zero_ary + lam_kld_p\\n        self.lam_kld_p.set_value(to_fX(new_lam))\\n        new_lam = zero_ary + lam_kld_q\\n        self.lam_kld_q.set_value(to_fX(new_lam))\\n        new_lam = zero_ary + lam_kld_g\\n        self.lam_kld_g.set_value(to_fX(new_lam))\\n        return\\n\\n    def set_lam_l2w(self, lam_l2w=1e-3):\\n        \\\"\\\"\\\"\\n        Set the relative strength of l2 regularization on network params.\\n        \\\"\\\"\\\"\\n        zero_ary = np.zeros((1,))\\n        new_lam = zero_ary + lam_l2w\\n        self.lam_l2w.set_value(to_fX(new_lam))\\n        return\\n\\n    def set_train_switch(self, switch_val=0.0):\\n        \\\"\\\"\\\"\\n        Set the switch for changing between training and sampling behavior.\\n        \\\"\\\"\\\"\\n        if (switch_val < 0.5):\\n            switch_val = 0.0\\n        else:\\n            switch_val = 1.0\\n        zero_ary = np.zeros((1,))\\n        new_val = zero_ary + switch_val\\n        self.train_switch.set_value(to_fX(new_val))\\n        return\\n\\n    def _construct_zi_zmuv(self, xi, br):\\n        \\\"\\\"\\\"\\n        Construct the necessary (symbolic) samples for computing through this\\n        GPSImputer for input (sybolic) matrix xi.\\n        \\\"\\\"\\\"\\n        zi_zmuv = self.rng.normal( \\\\\\n                size=(self.imp_steps, xi.shape[0]*br, self.z_dim), \\\\\\n                avg=0.0, std=1.0, dtype=theano.config.floatX)\\n        return zi_zmuv\\n\\n    def _construct_nll_costs(self, si, xo, xm):\\n        \\\"\\\"\\\"\\n        Construct the negative log-likelihood part of free energy.\\n        \\\"\\\"\\\"\\n        # average log-likelihood over the refinement sequence\\n        xh = self._si_as_x(si)\\n        xm_inv = 1.0 - xm # we will measure nll only where xm_inv is 1\\n        if self.x_type == 'bernoulli':\\n            ll_costs = log_prob_bernoulli(xo, xh, mask=xm_inv)\\n        else:\\n            ll_costs = log_prob_gaussian2(xo, xh, \\\\\\n                    log_vars=self.bounded_logvar, mask=xm_inv)\\n        nll_costs = -ll_costs.flatten()\\n        return nll_costs\\n\\n    def _construct_kld_costs(self, p=1.0):\\n        \\\"\\\"\\\"\\n        Construct the policy KL-divergence part of cost to minimize.\\n        \\\"\\\"\\\"\\n        kld_pis = []\\n        kld_qis = []\\n        kld_gis = []\\n        for i in range(self.imp_steps):\\n            kld_pis.append(T.sum(self.kldi_p2q[i]**p, axis=1))\\n            kld_qis.append(T.sum(self.kldi_q2p[i]**p, axis=1))\\n            kld_gis.append(T.sum(self.kldi_p2g[i]**p, axis=1))\\n        # compute the batch-wise costs\\n        kld_pi = sum(kld_pis)\\n        kld_qi = sum(kld_qis)\\n        kld_gi = sum(kld_gis)\\n        return [kld_pi, kld_qi, kld_gi]\\n\\n    def _construct_reg_costs(self):\\n        \\\"\\\"\\\"\\n        Construct the cost for low-level basic regularization. E.g. for\\n        applying l2 regularization to the network activations and parameters.\\n        \\\"\\\"\\\"\\n        param_reg_cost = sum([T.sum(p**2.0) for p in self.joint_params])\\n        return param_reg_cost\\n\\n    def _construct_compute_fe_terms(self):\\n        \\\"\\\"\\\"\\n        Construct a function for computing terms in variational free energy.\\n        \\\"\\\"\\\"\\n        # setup some symbolic variables for theano to deal with\\n        xi = T.matrix()\\n        xo = T.matrix()\\n        xm = T.matrix()\\n        zizmuv = self._construct_zi_zmuv(xi, 1)\\n        # construct values to output\\n        nll = self.nll_costs.flatten()\\n        kld = self.kld_q.flatten()\\n        # compile theano function for a one-sample free-energy estimate\\n        fe_term_sample = theano.function(inputs=[ xi, xo, xm ], \\\\\\n                outputs=[nll, kld], \\\\\\n                givens={self.x_in: xi, \\\\\\n                        self.x_out: xo, \\\\\\n                        self.x_mask: xm, \\\\\\n                        self.zi_zmuv: zizmuv}, \\\\\\n                updates=self.scan_updates, \\\\\\n                on_unused_input='ignore')\\n        # construct a wrapper function for multi-sample free-energy estimate\\n        def fe_term_estimator(XI, XO, XM, sample_count=20, use_guide_policy=True):\\n            # set model to desired generation mode\\n            old_switch = self.train_switch.get_value(borrow=False)\\n            if use_guide_policy:\\n                # take samples from guide policies (i.e. variational q)\\n                self.set_train_switch(switch_val=1.0)\\n            else:\\n                # take samples from model's imputation policy\\n                self.set_train_switch(switch_val=0.0)\\n            # compute a multi-sample estimate of variational free-energy\\n            nll_sum = np.zeros((XI.shape[0],))\\n            kld_sum = np.zeros((XI.shape[0],))\\n            for i in range(sample_count):\\n                result = fe_term_sample(XI, XO, XM)\\n                nll_sum += result[0].ravel()\\n                kld_sum += result[1].ravel()\\n            mean_nll = nll_sum / float(sample_count)\\n            mean_kld = kld_sum / float(sample_count)\\n            # set model back to either training or generation mode\\n            self.set_train_switch(switch_val=old_switch)\\n            if not use_guide_policy:\\n                # no KLd if samples are from the primary policy...\\n                mean_kld = 0.0 * mean_kld\\n            return [mean_nll, mean_kld]\\n        return fe_term_estimator\\n\\n    def _construct_raw_costs(self):\\n        \\\"\\\"\\\"\\n        Construct all the raw, i.e. not weighted by any lambdas, costs.\\n        \\\"\\\"\\\"\\n        # setup some symbolic variables for theano to deal with\\n        xi = T.matrix()\\n        xo = T.matrix()\\n        xm = T.matrix()\\n        zizmuv = self._construct_zi_zmuv(xi, 1)\\n        # compile theano function for computing the costs\\n        all_step_costs = [self.nlli, self.kldi_q2p, self.kldi_p2q, self.kldi_p2g]\\n        cost_func = theano.function(inputs=[xi, xo, xm], \\\\\\n                    outputs=all_step_costs, \\\\\\n                    givens={ self.x_in: xi, \\\\\\n                             self.x_out: xo, \\\\\\n                             self.x_mask: xm, \\\\\\n                             self.zi_zmuv: zizmuv }, \\\\\\n                    updates=self.scan_updates, \\\\\\n                    on_unused_input='ignore')\\n        # make a function for computing multi-sample estimates of cost\\n        def raw_cost_computer(XI, XO, XM):\\n            _all_costs = cost_func(to_fX(XI), to_fX(XO), to_fX(XM))\\n            _kld_q2p = np.sum(np.mean(_all_costs[1], axis=1, keepdims=True), axis=0)\\n            _kld_p2q = np.sum(np.mean(_all_costs[2], axis=1, keepdims=True), axis=0)\\n            _kld_p2g = np.sum(np.mean(_all_costs[3], axis=1, keepdims=True), axis=0)\\n            _step_klds = np.mean(np.sum(_all_costs[1], axis=2, keepdims=True), axis=1)\\n            _step_klds = to_fX( np.asarray([k for k in _step_klds]) )\\n            _step_nlls = np.mean(_all_costs[0], axis=1)\\n            _step_nlls = to_fX( np.asarray([k for k in _step_nlls]) )\\n            results = [_step_nlls, _step_klds, _kld_q2p, _kld_p2q, _kld_p2g]\\n            return results\\n        return raw_cost_computer\\n\\n    def _construct_compute_per_step_cost(self):\\n        \\\"\\\"\\\"\\n        Construct a theano function for computing the best possible cost\\n        achieved by sequential imputation.\\n        \\\"\\\"\\\"\\n        # setup some symbolic variables for theano to deal with\\n        xi = T.matrix()\\n        xo = T.matrix()\\n        xm = T.matrix()\\n        zizmuv = self._construct_zi_zmuv(xi, 1)\\n        # construct symbolic variables for the step-wise cost\\n        step_mean_nll = T.mean(self.nlli, axis=1).flatten()\\n        step_lone_kld = T.sum(self.kldi_q2p, axis=2)\\n        step_cumu_kld = T.extra_ops.cumsum(step_lone_kld, axis=0)\\n        step_mean_kld = T.mean(step_cumu_kld, axis=1).flatten()\\n        # compile theano function for computing the step-wise cost\\n        step_cost_func = theano.function(inputs=[xi, xo, xm], \\\\\\n                    outputs=[step_mean_nll, step_mean_kld], \\\\\\n                    givens={ self.x_in: xi, \\\\\\n                             self.x_out: xo, \\\\\\n                             self.x_mask: xm, \\\\\\n                             self.zi_zmuv: zizmuv }, \\\\\\n                    updates=self.scan_updates, \\\\\\n                    on_unused_input='ignore')\\n        def best_cost_computer(XI, XO, XM, sample_count=20):\\n            # compute a multi-sample estimate of variational free-energy\\n            step_nll_sum = np.zeros((self.imp_steps,))\\n            step_kld_sum = np.zeros((self.imp_steps,))\\n            for i in range(sample_count):\\n                result = step_cost_func(XI, XO, XM)\\n                step_nll_sum += result[0].ravel()\\n                step_kld_sum += result[1].ravel()\\n            mean_step_nll = step_nll_sum / float(sample_count)\\n            mean_step_kld = step_kld_sum / float(sample_count)\\n            return [mean_step_nll, mean_step_kld]\\n        return best_cost_computer\\n\\n    def _construct_train_joint(self):\\n        \\\"\\\"\\\"\\n        Construct theano function to train all networks jointly.\\n        \\\"\\\"\\\"\\n        # setup some symbolic variables for theano to deal with\\n        xi = T.matrix()\\n        xo = T.matrix()\\n        xm = T.matrix()\\n        br = T.lscalar()\\n        zizmuv = self._construct_zi_zmuv(xi, br)\\n        # collect the outputs to return from this function\\n        outputs = [self.joint_cost, self.nll_bound, self.nll_cost, \\\\\\n                   self.kld_cost, self.reg_cost, self.obs_costs]\\n        # compile the theano function\\n        func = theano.function(inputs=[ xi, xo, xm, br ], \\\\\\n                outputs=outputs, \\\\\\n                givens={ self.x_in: xi.repeat(br, axis=0), \\\\\\n                         self.x_out: xo.repeat(br, axis=0), \\\\\\n                         self.x_mask: xm.repeat(br, axis=0), \\\\\\n                         self.zi_zmuv: zizmuv }, \\\\\\n                updates=self.joint_updates, \\\\\\n                on_unused_input='ignore')\\n        return func\\n\\n    def _construct_sample_imputer(self):\\n        \\\"\\\"\\\"\\n        Construct a function for drawing samples from the distribution\\n        generated by running this imputer.\\n        \\\"\\\"\\\"\\n        xi = T.matrix()\\n        xo = T.matrix()\\n        xm = T.matrix()\\n        zizmuv = self._construct_zi_zmuv(xi, 1)\\n        oputs = [self.x0] + [self._si_as_x(self.si[i]) for i in range(self.imp_steps)]\\n        sample_func = theano.function(inputs=[xi, xo, xm], outputs=oputs, \\\\\\n                givens={self.x_in: xi, \\\\\\n                        self.x_out: xo, \\\\\\n                        self.x_mask: xm, \\\\\\n                        self.zi_zmuv: zizmuv}, \\\\\\n                updates=self.scan_updates, \\\\\\n                on_unused_input='ignore')\\n        def imputer_sampler(XI, XO, XM, use_guide_policy=False):\\n            XI = to_fX( XI )\\n            XO = to_fX( XO )\\n            XM = to_fX( XM )\\n            # set model to desired generation mode\\n            old_switch = self.train_switch.get_value(borrow=False)\\n            if use_guide_policy:\\n                # take samples from guide policies (i.e. variational q)\\n                self.set_train_switch(switch_val=1.0)\\n            else:\\n                # take samples from model's imputation policy\\n                self.set_train_switch(switch_val=0.0)\\n            # draw guided/unguided conditional samples\\n            model_samps = sample_func(XI, XO, XM)\\n            # set model back to either training or generation mode\\n            self.set_train_switch(switch_val=old_switch)\\n            # reverse engineer the \\\"masked\\\" samples...\\n            masked_samps = []\\n            for xs in model_samps:\\n                xsm = (XM * XI) + ((1.0 - XM) * xs)\\n                masked_samps.append(xsm)\\n            return model_samps, masked_samps\\n        return imputer_sampler\\n\\n    def save_to_file(self, f_name=None):\\n        \\\"\\\"\\\"\\n        Dump important stuff to a Python pickle, so that we can reload this\\n        model later.\\n        \\\"\\\"\\\"\\n        assert(not (f_name is None))\\n        f_handle = file(f_name, 'wb')\\n        # dump the dict self.params, which just holds \\\"simple\\\" python values\\n        cPickle.dump(self.params, f_handle, protocol=-1)\\n        # make a copy of self.shared_param_dicts, with numpy arrays in place\\n        # of the theano shared variables\\n        numpy_param_dicts = {}\\n        for key in self.shared_param_dicts:\\n            numpy_ary = self.shared_param_dicts[key].get_value(borrow=False)\\n            numpy_param_dicts[key] = numpy_ary\\n        # dump the numpy version of self.shared_param_dicts to pickle file\\n        cPickle.dump(numpy_param_dicts, f_handle, protocol=-1)\\n        # get numpy dicts for each of the \\\"child\\\" models that we must save\\n        child_model_dicts = {}\\n        child_model_dicts['p_zi_given_xi'] = self.p_zi_given_xi.save_to_dict()\\n        child_model_dicts['p_sip1_given_zi'] = self.p_sip1_given_zi.save_to_dict()\\n        child_model_dicts['q_zi_given_xi'] = self.q_zi_given_xi.save_to_dict()\\n        # dump the numpy child model dicts to the pickle file\\n        cPickle.dump(child_model_dicts, f_handle, protocol=-1)\\n        f_handle.close()\\n        return\"}, {\"identifier\":\"OneStageModel\", \"path\":\"OneStageModel.py\", \"snippet\":\"class OneStageModel(object):\\n    \\\"\\\"\\\"\\n    Controller for training a basic one step VAE.\\n\\n    Parameters:\\n        rng: numpy.random.RandomState (for reproducibility)\\n        x_in: symbolic input to this VAE\\n        p_x_given_z: HydraNet for x given z (2 outputs)\\n        q_z_given_x: HydraNet for z given x (2 outputs)\\n        x_dim: dimension of the \\\"observation\\\" variables\\n        z_dim: dimension of the \\\"latent\\\" variables\\n        params:\\n            x_type: can be \\\"bernoulli\\\" or \\\"gaussian\\\"\\n            xt_transform: optional transform for gaussian means\\n            logvar_bound: optional bound on gaussian output logvar\\n                          -- this can be useful for preventing output\\n                             distributions that are too \\\"spiky\\\" and thus\\n                             destabilize training.\\n    \\\"\\\"\\\"\\n    def __init__(self, rng=None, x_in=None, \\\\\\n            p_x_given_z=None, q_z_given_x=None, \\\\\\n            x_dim=None, z_dim=None, \\\\\\n            params=None):\\n        # setup a rng for this GIPair\\n        self.rng = RandStream(rng.randint(100000))\\n\\n        # grab the user-provided parameters\\n        if params is None:\\n            self.params = {}\\n        else:\\n            self.params = params\\n        if 'xt_transform' in self.params:\\n            assert((self.params['xt_transform'] == 'sigmoid') or \\\\\\n                    (self.params['xt_transform'] == 'none'))\\n            if self.params['xt_transform'] == 'sigmoid':\\n                self.xt_transform = lambda x: T.nnet.sigmoid(x)\\n            else:\\n                self.xt_transform = lambda x: x\\n        else:\\n            self.xt_transform = lambda x: T.nnet.sigmoid(x)\\n        if 'logvar_bound' in self.params:\\n            self.logvar_bound = self.params['logvar_bound']\\n        else:\\n            self.logvar_bound = 10.0\\n        #\\n        # x_type: this tells if we're using bernoulli or gaussian model for\\n        #         the observations\\n        #\\n        self.x_type = self.params['x_type']\\n        assert((self.x_type == 'bernoulli') or (self.x_type == 'gaussian'))\\n\\n        # record the dimensions of various spaces relevant to this model\\n        self.x_dim = x_dim\\n        self.z_dim = z_dim\\n\\n        # set parameters for the isotropic Gaussian prior over z\\n        self.prior_mean = 0.0\\n        self.prior_logvar = 0.0\\n\\n        # record the symbolic variables that will provide inputs to the\\n        # computation graph created to describe this OneStageModel\\n        self.x_in = x_in\\n\\n        #####################################################################\\n        # Setup the computation graph that provides values in our objective #\\n        #####################################################################\\n        # inferencer model for latent variables given observations\\n        self.q_z_given_x = q_z_given_x\\n        self.z_mean, self.z_logvar = self.q_z_given_x.apply(self.x_in)\\n        # reparametrize ZMUV Gaussian samples to get latent samples...\\n        self.z = reparametrize(self.z_mean, self.z_logvar, rng=self.rng)\\n\\n        # generator model for observations given latent variables\\n        self.p_x_given_z = p_x_given_z\\n        self.xt, _ = self.p_x_given_z.apply(self.z)\\n\\n        # construct the final output of generator, conditioned on z\\n        if self.x_type == 'bernoulli':\\n            self.xg = T.nnet.sigmoid(self.xt)\\n        else:\\n            self.xg = self.xt_transform(self.xt)\\n\\n        # self.output_logvar modifies the output distribution\\n        zero_ary = to_fX( np.zeros((1,)) )\\n        self.output_logvar = theano.shared(value=zero_ary, name='osm_output_logvar')\\n        self.bounded_logvar = self.logvar_bound * \\\\\\n                    T.tanh(self.output_logvar[0] / self.logvar_bound)\\n\\n        ######################################################################\\n        # ALL SYMBOLIC VARS NEEDED FOR THE OBJECTIVE SHOULD NOW BE AVAILABLE #\\n        ######################################################################\\n\\n        # shared var learning rate for generator and inferencer\\n        zero_ary = to_fX( np.zeros((1,)) )\\n        self.lr = theano.shared(value=zero_ary, name='osm_lr')\\n        # shared var momentum parameters for generator and inferencer\\n        self.mom_1 = theano.shared(value=zero_ary, name='osm_mom_1')\\n        self.mom_2 = theano.shared(value=zero_ary, name='osm_mom_2')\\n        # init parameters for controlling learning dynamics\\n        self.set_sgd_params()\\n        # init shared var for weighting nll of data given posterior sample\\n        self.lam_nll = theano.shared(value=zero_ary, name='osm_lam_nll')\\n        self.set_lam_nll(lam_nll=1.0)\\n        # init shared var for weighting controlling KL(q(z|x) || p(z))\\n        self.lam_kld = theano.shared(value=zero_ary, name='osm_lam_kld')\\n        self.set_lam_kld(lam_kld=1.0)\\n        # init shared var for controlling l2 regularization on params\\n        self.lam_l2w = theano.shared(value=zero_ary, name='osm_lam_l2w')\\n        self.set_lam_l2w(1e-4)\\n\\n        # grab a list of all the parameters to optimize\\n        self.joint_params = [self.output_logvar]\\n        self.joint_params.extend(self.q_z_given_x.mlp_params)\\n        self.joint_params.extend(self.p_x_given_z.mlp_params)\\n\\n        ###################################\\n        # CONSTRUCT THE COSTS TO OPTIMIZE #\\n        ###################################\\n        # first, do NLL\\n        self.nll_costs = self.lam_nll[0] * self._construct_nll_costs()\\n        self.nll_cost = T.mean(self.nll_costs)\\n        # second, do KLd\\n        self.kld_costs = self.lam_kld[0] * self._construct_kld_costs()\\n        self.kld_cost = T.mean(self.kld_costs)\\n        # third, do regularization\\n        self.reg_cost = self.lam_l2w[0] * self._construct_reg_costs()\\n        # finally, combine them for the joint cost.\\n        self.joint_cost = self.nll_cost + self.kld_cost + self.reg_cost\\n\\n        # Get the gradient of the joint cost for all optimizable parameters\\n        print(\\\"Computing gradients of self.joint_cost...\\\")\\n        self.joint_grads = OrderedDict()\\n        grad_list = T.grad(self.joint_cost, self.joint_params)\\n        for i, p in enumerate(self.joint_params):\\n            self.joint_grads[p] = grad_list[i]\\n\\n        # Construct the updates for the generator and inferencer networks\\n        self.joint_updates = get_adam_updates(params=self.joint_params, \\\\\\n                grads=self.joint_grads, alpha=self.lr, \\\\\\n                beta1=self.mom_1, beta2=self.mom_2, \\\\\\n                mom2_init=1e-3, smoothing=1e-4, max_grad_norm=10.0)\\n\\n        # Construct a function for jointly training the generator/inferencer\\n        print(\\\"Compiling self.train_joint...\\\")\\n        self.train_joint = self._construct_train_joint()\\n        print(\\\"Compiling self.compute_fe_terms...\\\")\\n        self.compute_fe_terms = self._construct_compute_fe_terms()\\n        print(\\\"Compiling self.compute_post_klds...\\\")\\n        self.compute_post_klds = self._construct_compute_post_klds()\\n        print(\\\"Compiling self.sample_from_prior...\\\")\\n        self.sample_from_prior = self._construct_sample_from_prior()\\n        self.transform_x_to_z = theano.function(inputs=[self.x_in], \\\\\\n                                                outputs=self.z_mean)\\n        self.transform_z_to_x = theano.function(inputs=[self.z], \\\\\\n                                                outputs=self.xg)\\n        self.inf_weights = self.q_z_given_x.shared_layers[0].W\\n        self.gen_weights = self.p_x_given_z.output_layers[-1].W\\n        return\\n\\n    def set_sgd_params(self, lr=0.01, mom_1=0.9, mom_2=0.999):\\n        \\\"\\\"\\\"\\n        Set learning rate and momentum parameter for all updates.\\n        \\\"\\\"\\\"\\n        zero_ary = np.zeros((1,))\\n        # set learning rates\\n        new_lr = zero_ary + lr\\n        self.lr.set_value(to_fX(new_lr))\\n        # set momentums\\n        new_mom_1 = zero_ary + mom_1\\n        self.mom_1.set_value(to_fX(new_mom_1))\\n        new_mom_2 = zero_ary + mom_2\\n        self.mom_2.set_value(to_fX(new_mom_2))\\n        return\\n\\n    def set_lam_nll(self, lam_nll=1.0):\\n        \\\"\\\"\\\"\\n        Set weight for controlling the influence of the data likelihood.\\n        \\\"\\\"\\\"\\n        zero_ary = np.zeros((1,))\\n        new_lam = zero_ary + lam_nll\\n        self.lam_nll.set_value(to_fX(new_lam))\\n        return\\n\\n    def set_lam_kld(self, lam_kld=1.0):\\n        \\\"\\\"\\\"\\n        Set the relative weight of prior KL-divergence vs. data likelihood.\\n        \\\"\\\"\\\"\\n        zero_ary = np.zeros((1,))\\n        new_lam = zero_ary + lam_kld\\n        self.lam_kld.set_value(to_fX(new_lam))\\n        return\\n\\n    def set_lam_l2w(self, lam_l2w=1e-3):\\n        \\\"\\\"\\\"\\n        Set the relative strength of l2 regularization on network params.\\n        \\\"\\\"\\\"\\n        zero_ary = np.zeros((1,))\\n        new_lam = zero_ary + lam_l2w\\n        self.lam_l2w.set_value(to_fX(new_lam))\\n        return\\n\\n    def _construct_nll_costs(self):\\n        \\\"\\\"\\\"\\n        Construct the negative log-likelihood part of cost to minimize.\\n        \\\"\\\"\\\"\\n        if self.x_type == 'bernoulli':\\n            ll_cost = log_prob_bernoulli(self.x_in, self.xg)\\n        else:\\n            ll_cost = log_prob_gaussian2(self.x_in, self.xg, \\\\\\n                    log_vars=self.bounded_logvar)\\n        nll_cost = -ll_cost\\n        return nll_cost\\n\\n    def _construct_kld_costs(self):\\n        \\\"\\\"\\\"\\n        Construct the posterior KL-d from prior part of cost to minimize.\\n        \\\"\\\"\\\"\\n        # compute the KLds between posteriors and priors. we compute the KLd\\n        # independently for each input and each latent variable dimension\\n        kld_z = gaussian_kld(self.z_mean, self.z_logvar, \\\\\\n                             self.prior_mean, self.prior_logvar)\\n        kld_costs = T.sum(kld_z, axis=1, keepdims=True)\\n        return kld_costs\\n\\n    def _construct_reg_costs(self):\\n        \\\"\\\"\\\"\\n        Construct the cost for low-level basic regularization. E.g. for\\n        applying l2 regularization to the network activations and parameters.\\n        \\\"\\\"\\\"\\n        param_reg_cost = sum([T.sum(p**2.0) for p in self.joint_params])\\n        return param_reg_cost\\n\\n    def _construct_train_joint(self):\\n        \\\"\\\"\\\"\\n        Construct theano function to train all networks jointly.\\n        \\\"\\\"\\\"\\n        # setup some symbolic variables for theano to deal with\\n        xi = T.matrix()\\n        br = T.lscalar()\\n        # collect the values to output with each function evaluation\\n        outputs = [self.joint_cost, self.nll_cost, self.kld_cost, \\\\\\n                   self.reg_cost, self.nll_costs, self.kld_costs]\\n        func = theano.function(inputs=[ xi, br ], \\\\\\n                outputs=outputs, \\\\\\n                givens={ self.x_in: xi.repeat(br, axis=0) }, \\\\\\n                updates=self.joint_updates)\\n        return func\\n\\n    def _construct_compute_post_klds(self):\\n        \\\"\\\"\\\"\\n        Construct theano function to compute the info about the variational\\n        approximate posteriors for some inputs.\\n        \\\"\\\"\\\"\\n        # setup some symbolic variables for theano to deal with\\n        all_klds = gaussian_kld(self.z_mean, self.z_logvar, \\\\\\n                                self.prior_mean, self.prior_logvar)\\n        # compile theano function for a one-sample free-energy estimate\\n        kld_func = theano.function(inputs=[self.x_in], outputs=all_klds)\\n        return kld_func\\n\\n    def _construct_compute_fe_terms(self):\\n        \\\"\\\"\\\"\\n        Construct theano function to compute the log-likelihood and posterior\\n        KL-divergence terms for the variational free-energy.\\n        \\\"\\\"\\\"\\n        # construct values to output\\n        if self.x_type == 'bernoulli':\\n            ll_term = log_prob_bernoulli(self.x_in, self.xg)\\n        else:\\n            ll_term = log_prob_gaussian2(self.x_in, self.xg, \\\\\\n                    log_vars=self.bounded_logvar)\\n        all_klds = gaussian_kld(self.z_mean, self.z_logvar, \\\\\\n                                self.prior_mean, self.prior_logvar)\\n        kld_term = T.sum(all_klds, axis=1)\\n        # compile theano function for a one-sample free-energy estimate\\n        fe_term_sample = theano.function(inputs=[self.x_in], \\\\\\n                                         outputs=[ll_term, kld_term])\\n        # construct a wrapper function for multi-sample free-energy estimate\\n        def fe_term_estimator(X, sample_count):\\n            X = to_fX(X)\\n            ll_sum = np.zeros((X.shape[0],))\\n            kld_sum = np.zeros((X.shape[0],))\\n            for i in range(sample_count):\\n                result = fe_term_sample(X)\\n                ll_sum = ll_sum + result[0].ravel()\\n                kld_sum = kld_sum + result[1].ravel()\\n            mean_nll = -ll_sum / float(sample_count)\\n            mean_kld = kld_sum / float(sample_count)\\n            return [mean_nll, mean_kld]\\n        return fe_term_estimator\\n\\n    def _construct_sample_from_prior(self):\\n        \\\"\\\"\\\"\\n        Construct a function for drawing independent samples from the\\n        distribution generated by this OneStageModel.\\n        \\\"\\\"\\\"\\n        z_sym = T.matrix()\\n        oputs = self.xg\\n        sample_func = theano.function(inputs=[z_sym], outputs=oputs, \\\\\\n                                      givens={ self.z: z_sym })\\n        def prior_sampler(samp_count):\\n            z_samps = npr.randn(samp_count, self.z_dim)\\n            z_samps = (np.exp(0.5 * self.prior_logvar) * z_samps) + \\\\\\n                      self.prior_mean\\n            z_samps =to_fX(z_samps)\\n            model_samps = sample_func(z_samps)\\n            return model_samps\\n        return prior_sampler\\n\\n    def sample_from_chain(self, X_d, X_c=None, X_m=None, loop_iters=5, \\\\\\n                          sigma_scale=None):\\n        \\\"\\\"\\\"\\n        Sample for several rounds through the I<->G loop, initialized with the\\n        the \\\"data variable\\\" samples in X_d.\\n        \\\"\\\"\\\"\\n        data_samples = []\\n        prior_samples = []\\n        if X_c is None:\\n            X_c = 0.0 * X_d\\n        if X_m is None:\\n            X_m = 0.0 * X_d\\n        if sigma_scale is None:\\n            sigma_scale = 1.0\\n        # set sigma_scale on our InfNet\\n        old_scale = self.q_z_given_x.sigma_scale.get_value(borrow=False)\\n        self.q_z_given_x.set_sigma_scale(sigma_scale)\\n        for i in range(loop_iters):\\n            # apply mask, mixing foreground and background data\\n            X_d = apply_mask(Xd=X_d, Xc=X_c, Xm=X_m)\\n            # record the data samples for this iteration\\n            data_samples.append(1.0 * X_d)\\n            # sample from their inferred posteriors\\n            X_p = self.q_z_given_x.sample_posterior(X_d)\\n            # record the sampled points (in the \\\"prior space\\\")\\n            prior_samples.append(1.0 * X_p)\\n            # get next data samples by transforming the prior-space points\\n            X_d = self.transform_z_to_x(X_p)\\n        # reset sigma_scale on our InfNet\\n        self.q_z_given_x.set_sigma_scale(old_scale[0])\\n        result = {\\\"data samples\\\": data_samples, \\\"prior samples\\\": prior_samples}\\n        return result\"}, {\"identifier\":\"load_udm\", \"path\":\"load_data.py\", \"snippet\":\"def load_udm(dataset, as_shared=True, zero_mean=True):\\n    \\\"\\\"\\\"Loads the UdM train/validate/test split of MNIST.\\\"\\\"\\\"\\n\\n    #############\\n    # LOAD DATA #\\n    #############\\n\\n    # Download the MNIST dataset if it is not present\\n    data_dir, data_file = os.path.split(dataset)\\n    if (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\\n        import urllib\\n        origin = 'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\\n        print 'Downloading data from %s' % origin\\n        urllib.urlretrieve(origin, dataset)\\n\\n    print '... loading data'\\n\\n    # Load the dataset\\n    f = gzip.open(dataset, 'rb')\\n    train_set, valid_set, test_set = cPickle.load(f)\\n    f.close()\\n    #train_set, valid_set, test_set format: tuple(input, target)\\n    #input is an np.ndarray of 2 dimensions (a matrix)\\n    #witch row's correspond to an example. target is a\\n    #np.ndarray of 1 dimensions (vector)) that have the same length as\\n    #the number of rows in the input. It should give the target\\n    #target to the example with the same index in the input.\\n    train_set = [v for v in train_set]\\n    valid_set = [v for v in valid_set]\\n    test_set = [v for v in test_set]\\n    train_set[0] = np.asarray(train_set[0]).astype(np.float32)\\n    valid_set[0] = np.asarray(valid_set[0]).astype(np.float32)\\n    test_set[0] = np.asarray(test_set[0]).astype(np.float32)\\n    if zero_mean:\\n        obs_mean = np.mean(train_set[0], axis=0, keepdims=True)\\n        train_set[0] = train_set[0] - obs_mean\\n        valid_set[0] = valid_set[0] - obs_mean\\n        test_set[0] = test_set[0] - obs_mean\\n    if as_shared:\\n        test_set_x, test_set_y = _shared_dataset((test_set[0],test_set[1]+1))\\n        valid_set_x, valid_set_y = _shared_dataset((valid_set[0],valid_set[1]+1))\\n        train_set_x, train_set_y = _shared_dataset((train_set[0],train_set[1]+1))\\n    else:\\n        test_set_x, test_set_y = (test_set[0], test_set[1]+1)\\n        valid_set_x, valid_set_y = (valid_set[0], valid_set[1]+1)\\n        train_set_x, train_set_y = (train_set[0], train_set[1]+1)\\n\\n    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\\n            (test_set_x, test_set_y)]\\n    return rval\"}, {\"identifier\":\"load_udm_ss\", \"path\":\"load_data.py\", \"snippet\":\"def load_udm_ss(dataset, sup_count, rng, zero_mean=True):\\n    \\\"\\\"\\\"\\n    Load semi-supervised version of the standard UdM MNIST data.\\n\\n    For this, the training data is split into labeled and unlabeled portions.\\n    The number of labeled examples is 'sup_count', and an equal number of\\n    labeled examples will be selected for each class. The remaining (50000 -\\n    sup_count) examples are provided as unlabeled training data. The validate\\n    and test sets are left unchanged.\\n\\n    Note: labels for the normal digit classes will range from 1-10, i.e. +1\\n    compared to their standard value, as 'un-classed' examples take label 0.\\n    \\\"\\\"\\\"\\n\\n    udm_data = load_udm(dataset, as_shared=False, zero_mean=zero_mean)\\n    Xtr = udm_data[0][0]\\n    Ytr = udm_data[0][1][:,np.newaxis]\\n\\n    all_count = Xtr.shape[0]\\n    pc_count = int(np.ceil(sup_count / 10.0))\\n    sup_count = int(10 * pc_count)\\n    unsup_count = all_count - sup_count\\n\\n    Xtr_su = []\\n    Ytr_su = []\\n    Xtr_un = []\\n    Ytr_un = []\\n\\n    # Sample supervised and unsupervised subsets of each class' observations\\n    for c_label in np.unique(Ytr):\\n        c_idx = [i for i in range(all_count) if (Ytr[i] == c_label)]\\n        rng.shuffle(c_idx)\\n        Xtr_su.append(Xtr[c_idx[0:pc_count],:])\\n        Ytr_su.append(Ytr[c_idx[0:pc_count],:])\\n        Xtr_un.append(Xtr[c_idx[pc_count:],:])\\n        Ytr_un.append(Ytr[c_idx[pc_count:],:])\\n\\n    # Stack per-class supervised/unsupervised splits into matrices\\n    Xtr_su = np.vstack(Xtr_su)\\n    Ytr_su = np.vstack(Ytr_su)\\n    Xtr_un = np.vstack(Xtr_un)\\n    Ytr_un = np.vstack(Ytr_un)\\n    # Also keep \\\"unsupervised\\\" copies of the \\\"supervised\\\" data\\n    Xtr_un = Xtr_un #np.vstack([Xtr_un, Xtr_su])\\n    Ytr_un = 0 * Ytr_un #np.vstack([Ytr_un, Ytr_su])\\n\\n    # Shuffle the rows so that observations are not grouped by class\\n    shuf_idx = rng.permutation(Xtr_su.shape[0])\\n    Xtr_su = Xtr_su[shuf_idx,:]\\n    Ytr_su = Ytr_su[shuf_idx].ravel() + 1\\n    shuf_idx = rng.permutation(Xtr_un.shape[0])\\n    Xtr_un = Xtr_un[shuf_idx,:]\\n    Ytr_un = Ytr_un[shuf_idx].ravel()\\n\\n    # Put matrices into GPU shared variables, for great justice\\n    Xtr_su, Ytr_su = _shared_dataset((Xtr_su, Ytr_su))\\n    Xtr_un, Ytr_un = _shared_dataset((Xtr_un, Ytr_un))\\n    Xva, Yva = _shared_dataset((udm_data[1][0], (udm_data[1][1] + 1)))\\n    Xte, Yte = _shared_dataset((udm_data[2][0], (udm_data[2][1] + 1)))\\n\\n    rval = [(Xtr_su, Ytr_su), (Xtr_un, Ytr_un), (Xva, Yva), (Xte, Yte)]\\n\\n    return rval\"}, {\"identifier\":\"load_mnist\", \"path\":\"load_data.py\", \"snippet\":\"def load_mnist(path, zero_mean=True):\\n    mnist = np.load(path)\\n    train_set_x = mnist['train_data']\\n    train_set_y = mnist['train_labels'] + 1\\n    test_set_x = mnist['test_data']\\n    test_set_y = mnist['test_labels'] + 1\\n\\n    if zero_mean:\\n        obs_mean = np.mean(train_set_x, axis=0, keepdims=True)\\n        train_set_x = train_set_x - obs_mean\\n        test_set_x = test_set_x - obs_mean\\n\\n    train_set_x, train_set_y = _shared_dataset((train_set_x, train_set_y))\\n    test_set_x, test_set_y = _shared_dataset((test_set_x, test_set_y))\\n    valid_set_x, valid_set_y = test_set_x, test_set_y\\n\\n    rval = [(train_set_x, train_set_y), (valid_set_x, valid_set_y),\\n            (test_set_x, test_set_y)]\\n    return rval\"}, {\"identifier\":\"load_binarized_mnist\", \"path\":\"load_data.py\", \"snippet\":\"def load_binarized_mnist(data_path='./'):\\n    #binarized_mnist_test.amat  binarized_mnist_train.amat  binarized_mnist_valid.amat\\n    print 'loading binary MNIST, sampled version'\\n    train_x = np.loadtxt(data_path + 'binarized_mnist_train.amat').astype('float32')\\n    valid_x = np.loadtxt(data_path + 'binarized_mnist_valid.amat').astype('float32')\\n    test_x = np.loadtxt(data_path + 'binarized_mnist_test.amat').astype('float32')\\n    # shuffle dataset\\n    train_x = row_shuffle(train_x)\\n    valid_x = row_shuffle(valid_x)\\n    test_x = row_shuffle(test_x)\\n    return train_x, valid_x, test_x\"}, {\"identifier\":\"load_tfd\", \"path\":\"load_data.py\", \"snippet\":\"def load_tfd(tfd_pkl_name='', which_set='', fold=0):\\n    \\\"\\\"\\\"\\n    Load TFD dataset, stored as pickled dict rather than a .mat file.\\n    \\\"\\\"\\\"\\n    assert(((fold >= 0) and (fold < 5)) or (fold == 'all'))\\n    # setup a map for grabbing indices to access the requested images\\n    set_type_map = {'unlabeled': 0,\\n                    'train': 1,\\n                    'valid': 2,\\n                    'test': 3}\\n    assert(which_set in set_type_map)\\n    data = cPickle.load(open(tfd_pkl_name))\\n\\n    # get indices of images in the requested set\\n    set_indices = np.zeros((data['folds'][:,0].shape[0],))\\n    for i in range(data['folds'].shape[1]):\\n        if ((i == fold) or (fold == 'all')):\\n            set_indices = set_indices + \\\\\\n                    (data['folds'][:,i] == set_type_map[which_set])\\n    set_indices = set_indices > 0.1\\n    assert(set_indices.sum() > 0)\\n\\n    # get the requested images and cast to theano.config.floatX\\n    data_x = data['images'][set_indices].astype(theano.config.floatX)\\n\\n    # scale data into range [0...1]\\n    data_x = data_x / 255.\\n    # reshape to 1-d images\\n    data_x = data_x.reshape((data_x.shape[0], data_x.shape[1]*data_x.shape[2]))\\n\\n    # some of the original unlabeled faces are all zero?\\n    good_idx = np.sum(data_x, axis=1) > 0.1\\n    data_x = data_x[good_idx]\\n\\n    # get labels if they were requested\\n    if which_set != 'unlabeled':\\n        data_y = data['labs_ex'][set_indices] - 1\\n        data_y_identity = data['labs_id'][set_indices]\\n        y_labels = 7\\n    else:\\n        data_y = None\\n        data_y_identity = None\\n        y_labels = None\\n\\n    # check label info\\n    mask = data['labs_ex'][set_indices] > -1\\n    print(\\\"lab_ex > -1: {0:d}\\\".format(np.sum(mask)))\\n\\n    return [data_x, data_y, data_y_identity, y_labels]\"}, {\"identifier\":\"load_svhn_gray\", \"path\":\"load_data.py\", \"snippet\":\"def load_svhn_gray(tr_file, te_file, ex_file=None, ex_count=None):\\n    \\\"\\\"\\\"\\n    Load pickle files with grayscale versions of the SVHN data.\\n    \\\"\\\"\\\"\\n    # load the training set as a numpy arrays\\n    pickle_file = open(tr_file)\\n    data_dict = cPickle.load(pickle_file)\\n    Xtr = data_dict['X']\\n    print(\\\"Xtr.shape: {0:s}\\\".format(str(Xtr.shape)))\\n    Ytr = data_dict['y'].astype(np.int32)\\n    Xtr_vec = np.zeros((Xtr.shape[2], 32*32), dtype=theano.config.floatX)\\n    for i in range(Xtr.shape[2]):\\n        c_pix = 32*32\\n        Xtr_vec[i,:] = Xtr[:,:,i].reshape((32*32,)).astype(theano.config.floatX)\\n    del Xtr\\n    Xtr = Xtr_vec\\n    pickle_file.close()\\n\\n    # load the test set as numpy arrays\\n    pickle_file = open(te_file)\\n    data_dict = cPickle.load(pickle_file)\\n    pickle_file.close()\\n    Xte = data_dict['X']\\n    print(\\\"Xte.shape: {0:s}\\\".format(str(Xte.shape)))\\n    Yte = data_dict['y'].astype(np.int32)\\n    Xte_vec = np.zeros((Xte.shape[2], 32*32), dtype=theano.config.floatX)\\n    for i in range(Xte.shape[2]):\\n        c_pix = 32*32\\n        Xte_vec[i,:] = Xte[:,:,i].reshape((32*32,)).astype(theano.config.floatX)\\n    del Xte\\n    Xte = Xte_vec\\n\\n    # process extra data as desired\\n    if ex_file is None:\\n        Xex = None\\n    else:\\n        if ex_count is None:\\n            ex_count = 100000000\\n        # load the extra digit examples and only keep a subset\\n        pickle_file = open(ex_file)\\n        data_dict = cPickle.load(pickle_file)\\n        pickle_file.close()\\n        Xex = data_dict['X']\\n        print(\\\"Xex.shape: {0:s}\\\".format(str(Xex.shape)))\\n        max_idx = min(ex_count, Xex.shape[2])\\n        Xex_vec = np.zeros((max_idx, 32*32)).astype(theano.config.floatX)\\n        for i in range(max_idx):\\n            c_pix = 32*32\\n            Xex_vec[i,:] = Xex[:,:,i].reshape((32*32,)).astype(theano.config.floatX)\\n        del Xex\\n        Xex = Xex_vec\\n\\n    # package data up for easy returnage\\n    data_dict = {'Xtr': Xtr, 'Ytr': Ytr, \\\\\\n                 'Xte': Xte, 'Yte': Yte, \\\\\\n                 'Xex': Xex}\\n    return data_dict\"}, {\"identifier\":\"construct_masked_data\", \"path\":\"HelperFuncs.py\", \"snippet\":\"def construct_masked_data(xi, \\\\\\n                          drop_prob=0.0, \\\\\\n                          occ_dim=None, \\\\\\n                          data_mean=None):\\n    \\\"\\\"\\\"\\n    Construct randomly masked data from xi.\\n    \\\"\\\"\\\"\\n    if data_mean is None:\\n        data_mean = np.zeros((xi.shape[1],))\\n    im_dim = int(xi.shape[1]**0.5) # images should be square\\n    xo = xi.copy()\\n    if drop_prob > 0.0:\\n        # apply fully-random occlusion\\n        xm_rand = sample_masks(xi, drop_prob=drop_prob)\\n    else:\\n        # don't apply fully-random occlusion\\n        xm_rand = np.ones(xi.shape)\\n    if occ_dim is None:\\n        # don't apply rectangular occlusion\\n        xm_patch = np.ones(xi.shape)\\n    else:\\n        # apply rectangular occlusion\\n        xm_patch = sample_patch_masks(xi, (im_dim,im_dim), (occ_dim,occ_dim))\\n    xm = xm_rand * xm_patch\\n    xi = (xm * xi) + ((1.0 - xm) * data_mean)\\n    xi = to_fX(xi)\\n    xo = to_fX(xo)\\n    xm = to_fX(xm)\\n    return xi, xo, xm\"}, {\"identifier\":\"shift_and_scale_into_01\", \"path\":\"HelperFuncs.py\", \"snippet\":\"def shift_and_scale_into_01(X):\\n    X = X - np.min(X, axis=1, keepdims=True)\\n    X = X / np.max(X, axis=1, keepdims=True)\\n    return X\"}, {\"identifier\":\"row_shuffle\", \"path\":\"HelperFuncs.py\", \"snippet\":\"def row_shuffle(X, Y=None):\\n    \\\"\\\"\\\"\\n    Return a copy of X with shuffled rows.\\n    \\\"\\\"\\\"\\n    shuf_idx = np.arange(X.shape[0])\\n    npr.shuffle(shuf_idx)\\n    X_shuf = X[shuf_idx]\\n    if Y is None:\\n        result = X_shuf\\n    else:\\n        Y_shuf = Y[shuf_idx]\\n        result = [X_shuf, Y_shuf]\\n    return result\"}, {\"identifier\":\"to_fX\", \"path\":\"HelperFuncs.py\", \"snippet\":\"def to_fX(np_ary):\\n    np_ary_fX = np_ary.astype(theano.config.floatX)\\n    return np_ary_fX\"}]", "import_statement": "import numpy as np\nimport numpy.random as npr\nimport cPickle\nimport theano\nimport theano.tensor as T\nimport utils\nfrom NetLayers import relu_actfun, softplus_actfun, tanh_actfun\nfrom InfNet import InfNet\nfrom HydraNet import HydraNet\nfrom GPSImputer import GPSImputer\nfrom OneStageModel import OneStageModel\nfrom load_data import load_udm, load_udm_ss, load_mnist, load_binarized_mnist, \\\n                      load_tfd, load_svhn_gray\nfrom HelperFuncs import construct_masked_data, shift_and_scale_into_01, \\\n                        row_shuffle, to_fX", "code": "##################################################################\n# Code for testing the variational Multi-Stage Generative Model. #\n##################################################################\n\n# basic python\n\n# theano business\n\n# phil's sweetness\n\nRESULT_PATH = \"IMP_MNIST_VAE_500/\"\n\n###############################\n###############################\n## TEST GPS IMPUTER ON MNIST ##\n###############################\n###############################\n\ndef test_mnist(occ_dim=15, drop_prob=0.0):\n    #########################################\n    # Format the result tag more thoroughly #\n    #########################################\n    dp_int = int(100.0 * drop_prob)\n    result_tag = \"{}VAE_OD{}_DP{}\".format(RESULT_PATH, occ_dim, dp_int)\n\n    ##########################\n    # Get some training data #\n    ##########################\n    rng = np.random.RandomState(1234)\n    dataset = 'data/mnist.pkl.gz'\n    datasets = load_udm(dataset, as_shared=False, zero_mean=False)\n    Xtr = datasets[0][0]\n    Xva = datasets[1][0]\n    Xtr = to_fX(shift_and_scale_into_01(Xtr))\n    Xva = to_fX(shift_and_scale_into_01(Xva))\n    tr_samples = Xtr.shape[0]\n    va_samples = Xva.shape[0]\n    batch_size = 200\n    batch_reps = 1\n    all_pix_mean = np.mean(np.mean(Xtr, axis=1))\n    data_mean = to_fX(all_pix_mean * np.ones((Xtr.shape[1],)))\n\n    ############################################################\n    # Setup some parameters for the Iterative Refinement Model #\n    ############################################################\n    obs_dim = Xtr.shape[1]\n    z_dim = 100\n    imp_steps = 15 # we'll check for the best step count (found oracularly)\n    init_scale = 1.0\n\n    x_in_sym = T.matrix('x_in_sym')\n    x_out_sym = T.matrix('x_out_sym')\n    x_mask_sym = T.matrix('x_mask_sym')\n\n    #################\n    # p_zi_given_xi #\n    #################\n    params = {}\n    shared_config = [obs_dim, 500, 500]\n    top_config = [shared_config[-1], z_dim]\n    params['shared_config'] = shared_config\n    params['mu_config'] = top_config\n    params['sigma_config'] = top_config\n", "next_line": "    params['activation'] = relu_actfun", "gold_snippet_index": 0, "id": 10, "__internal_uuid__": "6f64384c-5611-429b-84a3-d1696e42def0"}
{"repo_name": "ranea/ArxPy", "file_path": "arxpy/primitives/tests/test_ciphers.py", "context": "[{\"identifier\":\"Concat\", \"path\":\"arxpy/bitvector/operation.py\", \"snippet\":\"class Concat(Operation):\\n    \\\"\\\"\\\"Concatenation operation.\\n\\n    Given the bit-vectors :math:`(x_{n-1}, \\\\dots, x_0)` and\\n    :math:`(y_{m-1}, \\\\dots, y_0)`, ``Concat(x, y)`` returns the bit-vector\\n    :math:`(x_{n-1}, \\\\dots, x_0, y_{m-1}, \\\\dots, y_0)`.\\n\\n        >>> from arxpy.bitvector.core import Constant, Variable\\n        >>> from arxpy.bitvector.operation import Concat\\n        >>> Concat(Constant(0x12, 8), Constant(0x345, 12))\\n        0x12345\\n        >>> Concat(Variable(\\\"x\\\", 8), Variable(\\\"y\\\", 8))\\n        x :: y\\n\\n    \\\"\\\"\\\"\\n\\n    arity = [2, 0]\\n    is_symmetric = False\\n    infix_symbol = \\\"::\\\"\\n\\n    @classmethod\\n    def output_width(cls, x, y):\\n        return x.width + y.width\\n\\n    @classmethod\\n    def eval(cls, x, y):\\n        def doit(x, y):\\n            \\\"\\\"\\\"Concatenation when both operands are int.\\\"\\\"\\\"\\n            return int(x.bin() + y.bin()[2:], 2)\\n\\n        if isinstance(x, core.Constant) and isinstance(y, core.Constant):\\n            return core.Constant(doit(x, y), cls.output_width(x, y))\\n        elif isinstance(x, core.Constant) and isinstance(y, Concat) and \\\\\\n                isinstance(y.args[0], core.Constant):\\n            return Concat(Concat(x, y.args[0]), y.args[1])\\n        elif isinstance(y, core.Constant) and isinstance(x, Concat) and \\\\\\n                isinstance(x.args[1], core.Constant):\\n            return Concat(x.args[0], Concat(x.args[1], y))\\n        elif isinstance(x, Extract) and isinstance(y, Extract):\\n            # x[5:4] concat x[3:2] = x[5:2]\\n            if x.args[0] == y.args[0] and x.args[2] == y.args[1] + 1:\\n                return Extract(x.args[0], x.args[1], y.args[2])\"}, {\"identifier\":\"BvComp\", \"path\":\"arxpy/bitvector/operation.py\", \"snippet\":\"class BvComp(Operation):\\n    \\\"\\\"\\\"Equality operator.\\n\\n    Provides Automatic Constant Conversion. See `Operation` for more\\n    information.\\n\\n        >>> from arxpy.bitvector.core import Constant, Variable\\n        >>> from arxpy.bitvector.operation import BvComp\\n        >>> BvComp(Constant(1, 8), Constant(2, 8))\\n        0b0\\n        >>> BvComp(Constant(1, 8), 2)\\n        0b0\\n        >>> BvComp(Constant(1, 8), Variable(\\\"y\\\", 8))\\n        0x01 == y\\n\\n    The operator == is used for exact structural equality testing and\\n    it returns either True or False. On the other hand, BvComp\\n    performs symbolic equality testing and it leaves the relation unevaluated\\n    if it cannot prove the objects are equal (or unequal).\\n\\n        >>> Variable(\\\"x\\\", 8) == Variable(\\\"y\\\", 8)\\n        False\\n        >>> BvComp(Variable(\\\"x\\\", 8), Variable(\\\"y\\\", 8))  # symbolic equality\\n        x == y\\n\\n    \\\"\\\"\\\"\\n\\n    arity = [2, 0]\\n    is_symmetric = True\\n    is_simple = True\\n    infix_symbol = \\\"==\\\"\\n\\n    @classmethod\\n    def condition(cls, x, y):\\n        return x.width == y.width\\n\\n    @classmethod\\n    def output_width(cls, x, y):\\n        return 1\\n\\n    @classmethod\\n    def eval(cls, x, y):\\n        zero = core.Constant(0, 1)\\n        one = core.Constant(1, 1)\\n\\n        if x is y:\\n            return one\\n        elif isinstance(x, core.Constant) and isinstance(y, core.Constant):\\n            return one if x.val == y.val else zero\"}, {\"identifier\":\"XorDiff\", \"path\":\"arxpy/differential/difference.py\", \"snippet\":\"class XorDiff(Difference):\\n    \\\"\\\"\\\"Represent XOR differences.\\n\\n    The XOR difference of two `Term` is given by the XOR\\n    of the terms. In other words, the *difference operation*\\n    of `XorDiff` is the `BvXor` (see `Difference`).\\n\\n        >>> from arxpy.bitvector.core import Constant, Variable\\n        >>> from arxpy.differential.difference import XorDiff\\n        >>> x, y = Constant(0b000, 3), Constant(0b000, 3)\\n        >>> alpha = XorDiff.from_pair(x, y)\\n        >>> alpha\\n        XorDiff(0b000)\\n        >>> alpha.get_pair_element(x)\\n        0b000\\n        >>> x, y = Constant(0b010, 3), Constant(0b101, 3)\\n        >>> alpha = XorDiff.from_pair(x, y)\\n        >>> alpha\\n        XorDiff(0b111)\\n        >>> alpha.get_pair_element(x)\\n        0b101\\n        >>> k = Variable(\\\"k\\\", 8)\\n        >>> alpha = XorDiff.from_pair(k, k)\\n        >>> alpha\\n        XorDiff(0x00)\\n        >>> alpha.get_pair_element(k)\\n        k\\n    \\\"\\\"\\\"\\n\\n    diff_op = operation.BvXor\\n    inv_diff_op = operation.BvXor\\n\\n    @classmethod\\n    def derivative(cls, op, input_diff):\\n        \\\"\\\"\\\"Return the derivative of ``op`` at the point ``input_diff``.\\n\\n        See `Difference.derivative` for more information.\\n\\n            >>> from arxpy.bitvector.core import Variable, Constant\\n            >>> from arxpy.bitvector.operation import BvAdd, BvXor, RotateLeft, BvSub\\n            >>> from arxpy.bitvector.extraop import make_partial_operation\\n            >>> from arxpy.differential.difference import XorDiff\\n            >>> d1, d2 = XorDiff(Variable(\\\"d1\\\", 8)), XorDiff(Variable(\\\"d2\\\", 8))\\n            >>> XorDiff.derivative(BvXor, [d1, d2])\\n            XorDiff(d1 ^ d2)\\n            >>> Xor1 = make_partial_operation(BvXor, tuple([None, Constant(1, 8)]))\\n            >>> XorDiff.derivative(Xor1, d1)\\n            XorDiff(d1)\\n            >>> Rotate1 = make_partial_operation(RotateLeft, tuple([None, 1]))\\n            >>> XorDiff.derivative(Rotate1, d1)\\n            XorDiff(d1 <<< 1)\\n            >>> XorDiff.derivative(BvAdd, [d1, d2])\\n            XDA(XorDiff(d1), XorDiff(d2))\\n            >>> XorDiff.derivative(BvSub, [d1, d2])\\n            XDS(XorDiff(d1), XorDiff(d2))\\n            >>> CteAdd1 = make_partial_operation(BvAdd, tuple([None, Constant(1, 8)]))\\n            >>> XorDiff.derivative(CteAdd1, d1)\\n            XDCA_0x01(XorDiff(d1))\\n\\n        \\\"\\\"\\\"\\n        input_diff = _tuplify(input_diff)\\n        assert len(input_diff) == sum(op.arity)\\n\\n        msg = \\\"invalid arguments: op={}, input_diff={}\\\".format(\\n            op.__name__,\\n            [d.vrepr() if isinstance(d, core.Term) else d for d in input_diff])\\n\\n        if not all(isinstance(diff, cls) for diff in input_diff):\\n            raise ValueError(msg)\\n\\n        if op == operation.BvNot:\\n            return input_diff[0]\\n\\n        if op == operation.BvXor:\\n            return cls(op(*[d.val for d in input_diff]))\\n\\n        if op == operation.Concat:\\n            return cls(op(*[d.val for d in input_diff]))\\n\\n        if op == operation.BvAdd:\\n            from arxpy.differential import derivative\\n            return derivative.XDA(input_diff)\\n\\n        if op == operation.BvSub:\\n            from arxpy.differential import derivative\\n            return derivative.XDS(input_diff)\\n\\n        if issubclass(op, extraop.PartialOperation):\\n            if op.base_op == operation.BvXor:\\n                assert len(input_diff) == 1\\n                d1 = input_diff[0]\\n                val = op.fixed_args[0] if op.fixed_args[0] is not None else op.fixed_args[1]\\n                d2 = cls.from_pair(val, val)\\n                input_diff = [d1, d2]\\n                return cls(op.base_op(*[d.val for d in input_diff]))\\n\\n            if op.base_op == operation.BvAnd:\\n                assert len(input_diff) == 1\\n                d1 = input_diff[0]\\n                val = op.fixed_args[0] if op.fixed_args[0] is not None else op.fixed_args[1]\\n                if isinstance(val, core.Constant):\\n                    return cls(op.base_op(d1.val, val))\\n\\n            if op.base_op in [operation.RotateLeft, operation.RotateRight]:\\n                if op.fixed_args[0] is None and op.fixed_args[1] is not None:\\n                    assert len(input_diff) == 1\\n                    d = input_diff[0]\\n                    return cls(op.base_op(d.val, op.fixed_args[1]))\\n                else:\\n                    raise ValueError(msg)\\n\\n            if op.base_op in [operation.BvShl, operation.BvLshr]:\\n                if op.fixed_args[0] is None and op.fixed_args[1] is not None:\\n                    assert len(input_diff) == 1\\n                    d = input_diff[0]\\n                    return cls(op.base_op(d.val, op.fixed_args[1]))\\n                else:\\n                    raise ValueError(msg)\\n\\n            if op.base_op == operation.Extract:\\n                if op.fixed_args[0] is None and op.fixed_args[1] is not None and op.fixed_args[2] is not None:\\n                    assert len(input_diff) == 1\\n                    d = input_diff[0]\\n                    return cls(op.base_op(d.val, op.fixed_args[1], op.fixed_args[2]))\\n                else:\\n                    raise ValueError(msg)\\n\\n            if op.base_op == operation.Concat:\\n                assert len(input_diff) == 1\\n                d1 = input_diff[0]\\n                if op.fixed_args[0] is not None:\\n                    val = op.fixed_args[0]\\n                    input_diff = [cls.from_pair(val, val), d1]\\n                else:\\n                    val = op.fixed_args[1]\\n                    input_diff = [d1, cls.from_pair(val, val)]\\n                return cls(op.base_op(*[d.val for d in input_diff]))\\n\\n            if op.base_op == operation.BvAdd:\\n                assert len(input_diff) == 1\\n                d = input_diff[0]\\n                cte = op.fixed_args[0] if op.fixed_args[0] is not None else op.fixed_args[1]\\n                from arxpy.differential import derivative\\n                return derivative.XDCA(d, cte)\\n            else:\\n                raise ValueError(msg)\\n\\n        if hasattr(op, \\\"xor_derivative\\\"):\\n            return op.xor_derivative(input_diff)\\n\\n        raise ValueError(msg)\"}, {\"identifier\":\"DerMode\", \"path\":\"arxpy/smt/search.py\", \"snippet\":\"class DerMode(enum.Enum):\\n    \\\"\\\"\\\"Represent the different constraints available for the derivative weights of a characteristic.\\n\\n    Attributes:\\n        Default: no additional constraint is added\\n        ProbabilityOne: fix all derivative weights to zero\\n        Valid: only the validity constraint is added (weight is ignored)\\n        XDCA_Approx: for the weights of `XDCA`, the ``precision`` is set to ``0``\\n\\n    \\\"\\\"\\\"\\n    Default = enum.auto()\\n    ProbabilityOne = enum.auto()\\n    Valid = enum.auto()\\n    XDCA_Approx = enum.auto()\"}, {\"identifier\":\"SkChSearchMode\", \"path\":\"arxpy/smt/search.py\", \"snippet\":\"class SkChSearchMode(enum.Enum):\\n    \\\"\\\"\\\"Represent the different options for searching `SingleKeyCh`.\\n\\n    The options are the same as `ChSearchMode`, but `SkChFound` objects are returned instead of `ChFound`.\\n    \\\"\\\"\\\"\\n    FirstCh = enum.auto()\\n    FirstChValid = enum.auto()\\n    Optimal = enum.auto()\\n    OptimalDifferential = enum.auto()\\n    AllOptimal = enum.auto()\\n    AllValid = enum.auto()\\n    TopDifferentials = enum.auto()\"}, {\"identifier\":\"RkChSearchMode\", \"path\":\"arxpy/smt/search.py\", \"snippet\":\"class RkChSearchMode(enum.Enum):\\n    \\\"\\\"\\\"Represent the different options for searching `RelatedKeyCh`.\\n\\n    A related-key characteristic is said to be *valid* if its encryption weight\\n    is lower than the bit-size of the plaintext difference and\\n    its key schedule weight is lower than the bit-size of the master-key difference.\\n\\n    Attributes:\\n        FirstMinSum: returns the 1st related-key characteristic found\\n            by iteratively searching with increasing *weight*, where the *weight*\\n            is taken as the sum of the key schedule weight and the encryption weight.\\n            The characteristic found is returned as a `RkChFound` object,\\n            but it may not have optimal theoretical weight if it it contains\\n            a non-exact derivative weight or it may be empirically invalid.\\n            If no characteristic is found, ``None`` is returned.\\n        FirstMinSumValid: similar to `FirstMinSum`, but if the characteristic found is\\n            empirically invalid (after computing its empirical weight), the search continues.\\n        OptimalMinSum: similar to `FirstMinSum`, but several characteristics are found to ensure\\n            that the one returned has the optimal theoretical weight.\\n            (only differs from `FirstMinSum` if there is a non-exact derivative weight).\\n        OptimalMinSumDifferential: similar to `OptimalMinSum`, but once a characteristic is found,\\n            only characteristics with different input/output differences are searched.\\n            (to ensure optimality).\\n        FirstValidKeyMinEnc: similar to `FirstMinSum`,\\n            but the *weight* is taken as the encryption weight and the\\n            key schedule weight is restricted to be smaller than the key size.\\n        FirstValidKeyMinEncValid: similar to `FirstMinSumValid`,\\n            but the *weight* is defined as `FirstValidKeyMinEnc`.\\n        OptimalValidKeyMinEnc: similar to `OptimalMinSum`,\\n            but the *weight* is defined as `FirstValidKeyMinEnc`.\\n        OptimalValidKeyMinEncDifferential: similar to `OptimalMinSumDifferential`,\\n            but the *weight* is  defined as `FirstValidKeyMinEnc`.\\n        FirstFixEncMinKey: similar to `FirstMinSum`,\\n            but the *weight* is taken as the key schedule weight and\\n            the encryption weight is fixed to a given value.\\n        FirstFixEncMinKeyValid: similar to `FirstMinSumValid`,\\n            but the *weight* is defined as `FirstFixEncMinKey`.\\n        OptimalFixEncMinKey: similar to `OptimalMinSum`,\\n            but the *weight* is defined as `FirstFixEncMinKey`.\\n        OptimalFixEncMinKeyDifferential: similar to `OptimalMinSumDifferential`,\\n            but the *weight* is defined as `FirstFixEncMinKey`.\\n        AllOptimalMinSum: similar to `OptimalMinSum`, but the search never stops.\\n        AllValid: a non-stop search of valid characteristics with no\\n            additional constraints.\\n\\n    \\\"\\\"\\\"\\n    FirstMinSum = enum.auto()\\n    FirstMinSumValid = enum.auto()\\n    OptimalMinSum = enum.auto()\\n    OptimalMinSumDifferential = enum.auto()\\n    AllOptimalMinSum = enum.auto()\\n    FirstValidKeyMinEnc = enum.auto()\\n    FirstValidKeyMinEncValid = enum.auto()\\n    OptimalValidKeyMinEnc = enum.auto()\\n    OptimalValidKeyMinEncDifferential = enum.auto()\\n    FirstFixEncMinKey = enum.auto()\\n    FirstFixEncMinKeyValid = enum.auto()\\n    OptimalFixEncMinKey = enum.auto()\\n    OptimalFixEncMinKeyDifferential = enum.auto()\\n    AllValid = enum.auto()\"}, {\"identifier\":\"round_based_search_SkCh\", \"path\":\"arxpy/smt/search.py\", \"snippet\":\"def round_based_search_SkCh(cipher, diff_type, initial_weight, solver_name, start_round, end_round,\\n                            der_mode, search_mode, check, verbose_level, filename):\\n    \\\"\\\"\\\"Find valid single-key characteristics over consecutive rounds.\\n\\n    Args:\\n        cipher(Cipher): an (iterated) cipher\\n        diff_type(Difference): a type of difference\\n        initial_weight(int): the initial weight for starting the iterative search\\n        solver_name(str): the name of the solver (according to pySMT) to be used\\n        start_round(int): the minimum number of rounds to consider\\n        end_round(int): the maximum number of rounds to consider\\n        der_mode(DerMode): one of the modes available for the derivative weights\\n        search_mode(SkChSearchMode): one of the search modes available\\n        check(bool): if ``True``, `SkChFound.check_empirical_weight` will be called\\n            after a characteristic is found. If it is not valid, the search will continue.\\n        verbose_level(int): an integer between ``0`` (no verbose) and ``3`` (full verbose).\\n        filename(str): if not ``None``, the output will be  printed to the given file\\n            rather than the to stdout.\\n\\n    See also `SearchSkCh.solve`.\\n\\n        >>> from arxpy.differential.difference import XorDiff\\n        >>> from arxpy.smt.search import SkChSearchMode, DerMode, round_based_search_SkCh\\n        >>> from arxpy.primitives import speck\\n        >>> Speck32 = speck.get_Speck_instance(speck.SpeckInstance.speck_32_64)\\n        >>> round_based_search_SkCh(Speck32, XorDiff, 0, \\\"btor\\\", 1, 2,\\n        ...                         DerMode.Default, SkChSearchMode.Optimal, True, 0, None)  # doctest:+ELLIPSIS\\n        Num rounds: 1\\n        Best characteristic found:\\n        {'ch_weight': 0,\\n         'der_weights': [[w0, 0]],\\n         'emp_weight': Counter({0: 256}),\\n         'exact_weight': 0,\\n         'input_diff': [[dp0, ...], [dp1, ...]],\\n         'nonlinear_diffs': [[dx1, ...]],\\n         'output_diff': [[dx2, ...], [dx4, ...]]}\\n        <BLANKLINE>\\n        Num rounds: 2\\n        Best characteristic found:\\n        {'ch_weight': 1,\\n         'der_weights': [[w0w1, 1]],\\n         'emp_weight': ...,\\n         'exact_weight': 1,\\n         'input_diff': [[dp0, ...], [dp1, ...]],\\n         'nonlinear_diffs': [[dx1, ...], [dx6, ...]],\\n         'output_diff': [[dx7, ...], [dx9, ...]]}\\n        <BLANKLINE>\\n\\n    \\\"\\\"\\\"\\n    assert start_round <= end_round\\n    assert search_mode not in [SkChSearchMode.AllValid, SkChSearchMode.AllOptimal]\\n\\n    smart_print = _get_smart_print(filename)\\n\\n    if verbose_level >= 1:\\n        smart_print(cipher.__name__, \\\"Single-Key Search\\\\n\\\")\\n        smart_print(\\\"Parameters:\\\")\\n        smart_print(\\\"\\\\tcipher:\\\", cipher.__name__)\\n        smart_print(\\\"\\\\tdiff_type:\\\", diff_type.__name__)\\n        smart_print(\\\"\\\\tinitial_weight:\\\", initial_weight)\\n        smart_print(\\\"\\\\tsolver_name:\\\", solver_name)\\n        smart_print(\\\"\\\\tstart:\\\", start_round)\\n        smart_print(\\\"\\\\tend:\\\", end_round)\\n        smart_print(\\\"\\\\tder_mode:\\\", der_mode)\\n        smart_print(\\\"\\\\tsearch_mode:\\\", search_mode)\\n        smart_print(\\\"\\\\tcheck:\\\", check)\\n        smart_print(\\\"\\\\tverbose_level:\\\", verbose_level)\\n        smart_print(\\\"\\\\tfilename:\\\", filename)\\n        smart_print()\\n\\n    for num_rounds in range(start_round, end_round + 1):\\n        cipher.set_rounds(num_rounds)\\n\\n        smart_print(\\\"Num rounds:\\\", num_rounds)\\n\\n        ch = characteristic.SingleKeyCh(cipher, diff_type)\\n\\n        if verbose_level >= 2:\\n            smart_print(\\\"Characteristic:\\\")\\n            smart_print(ch)\\n\\n        problem = SearchSkCh(skch=ch, der_mode=der_mode, allow_zero_input_diff=initial_weight > 0)\\n\\n        if verbose_level >= 2:\\n            smart_print(\\\"SMT problem (size {}):\\\".format(problem.formula_size()))\\n            smart_print(problem.hrepr(full_repr=verbose_level >= 3))\\n\\n        ch_found = problem.solve(initial_weight, solver_name=solver_name, search_mode=search_mode, check=check,\\n                                 verbose_level=verbose_level, filename=filename)\\n\\n        if verbose_level >= 1:\\n            prefix = str(_get_time()) + \\\" | \\\"\\n            smart_print()\\n        else:\\n            prefix = \\\"\\\"\\n\\n        if ch_found is None:\\n            smart_print(prefix + \\\"No characteristic found\\\")\\n            return\\n        else:\\n            if search_mode == SkChSearchMode.TopDifferentials:\\n                ch_found, top_differentials = ch_found\\n            initial_weight = int(ch_found.ch_weight)\\n            smart_print(prefix + \\\"Best characteristic found:\\\")\\n            smart_print(ch_found)\\n            if verbose_level >= 3:\\n                smart_print(ch_found.vrepr())\\n            if search_mode == SkChSearchMode.TopDifferentials:\\n                smart_print(\\\"Best differentials found:\\\")\\n                smart_print(top_differentials)\\n            smart_print()\"}, {\"identifier\":\"round_based_search_RkCh\", \"path\":\"arxpy/smt/search.py\", \"snippet\":\"def round_based_search_RkCh(cipher, diff_type, initial_ew, initial_kw, solver_name,\\n                            start_round, end_round, key_der_mode, enc_der_mode, allow_zero_enc_input_diff,\\n                            search_mode, check, verbose_level, filename,\\n                            allow_zero_key_input_diff=False):  # for RXDiff\\n    \\\"\\\"\\\"Find valid related-key characteristics over consecutive rounds.\\n\\n    Args:\\n        cipher(Cipher): an (iterated) cipher\\n        diff_type(Difference): a type of difference\\n        initial_ew(int): the initial encryption weight for starting the iterative search\\n        initial_kw(int): the initial key schedule weight for starting the iterative search\\n        solver_name(str): the name of the solver (according to pySMT) to be used\\n        start_round(int): the minimum number of rounds to consider\\n        end_round(int): the maximum number of rounds to consider\\n        key_der_mode(DerMode):  the derivative mode for the key schedule characteristic\\n        enc_der_mode(DerMode):  the derivative mode for the encryption characteristic\\n        allow_zero_enc_input_diff (bool): if ``True``, allow the input difference\\n            of the encryption to be zero\\n        search_mode(RkChSearchMode): one of the search modes available\\n        check(bool): if ``True``, `RkChFound.check_empirical_weight` will be called\\n            after a characteristic is found. If it is not valid, the search will continue.\\n        verbose_level(int): an integer between ``0`` (no verbose) and ``3`` (full verbose).\\n        filename(str): if not ``None``, the output will be  printed to the given file\\n            rather than the to stdout.\\n        allow_zero_key_input_diff (bool): if ``True``, allow the input difference\\n            of the key schedule to be zero.\\n\\n    See also `SearchRkCh.solve`.\\n\\n        >>> from arxpy.differential.difference import XorDiff, RXDiff\\n        >>> from arxpy.smt.search import RkChSearchMode, DerMode, round_based_search_RkCh\\n        >>> from arxpy.primitives import speck\\n        >>> Speck32 = speck.get_Speck_instance(speck.SpeckInstance.speck_32_64)\\n        >>> round_based_search_RkCh(Speck32, XorDiff, 0, 0, \\\"btor\\\",\\n        ...                         4, 5, DerMode.Default, DerMode.Default, True,\\n        ...                         RkChSearchMode.OptimalMinSum, True, 0, None)  # doctest:+ELLIPSIS\\n        Num rounds: 4\\n        Best related-key characteristic found:\\n        {'enc_ch_found': {'ch_weight': 0,\\n                          'der_weights': [[w0w1w2, 0], [w3, 0]],\\n                          'emp_weight': Counter({0: 256}),\\n                          'exact_weight': 0,\\n                          'input_diff': [[dp0, ...], [dp1, ...]],\\n                          'nonlinear_diffs': [[dx1, ...], [dx6, ...], [dx11, ...], [dx16, ...]],\\n                          'output_diff': [[dx17, ...], [dx19, ...]]},\\n         'key_ch_found': {'ch_weight': 0,\\n                          'der_weights': [[wk0wk1wk2, 0]],\\n                          'emp_weight': 0.0,\\n                          'exact_weight': 0,\\n                          'input_diff': [[dmk0, ...], [dmk1, ...], [dmk2, ...], [dmk3, ...]],\\n                          'nonlinear_diffs': [[dk1, ...], [dk5, ...], [dk10, ...]],\\n                          'output_diff': [[dmk3, ...], [dk3, ...], [dk8, ...], [dk13, ...]]}}\\n        <BLANKLINE>\\n        Num rounds: 5\\n        Best related-key characteristic found:\\n        {'enc_ch_found': {'ch_weight': 1,\\n                          'der_weights': [[w0w1w2, 0], [w3w4, 1]],\\n                          'emp_weight': ...,\\n                          'exact_weight': 1,\\n                          'input_diff': [[dp0, ...], [dp1, ...]],\\n                          'nonlinear_diffs': [[dx1, ...], [dx6, ...], [dx11, ...], [dx16, ...], [dx21, ...]],\\n                          'output_diff': [[dx22, ...], [dx24, ...]]},\\n         'key_ch_found': {'ch_weight': 0,\\n                          'der_weights': [[wk0wk1wk2, 0], [wk3, 0]],\\n                          'emp_weight': 0.0,\\n                          'exact_weight': 0,\\n                          'input_diff': [[dmk0, ...], [dmk1, ...], [dmk2, ...], [dmk3, ...]],\\n                          'nonlinear_diffs': [[dk1, ...], [dk5, ...], [dk10, ...], [dk15, ...]],\\n                          'output_diff': [[dmk3, ...], [dk3, ...], [dk8, ...], [dk13, ...], [dk18, ...]]}}\\n        <BLANKLINE>\\n        >>> round_based_search_RkCh(Speck32, RXDiff, 0, 0, \\\"btor\\\",\\n        ...                         2, 3, DerMode.Default, DerMode.Default, True,\\n        ...                         RkChSearchMode.FirstMinSumValid, True, 0, None, True)  # doctest:+ELLIPSIS\\n        Num rounds: 2\\n        Best related-key characteristic found:\\n        {'enc_ch_found': {'ch_weight': 2,\\n                          'der_weights': [[w0, ...], [w1, ...]],\\n                          'emp_weight': ...,\\n                          'exact_weight': 2.829986944784033,\\n                          'input_diff': [[dp0, ...], [dp1, ...]],\\n                          'nonlinear_diffs': [[dx1, ...], [dx6, ...]],\\n                          'output_diff': [[dx7, ...], [dx9, ...]]},\\n         'key_ch_found': {'ch_weight': 1,\\n                          'der_weights': [[wk0, ...]],\\n                          'emp_weight': ...,\\n                          'exact_weight': 1.4149934723920166,\\n                          'input_diff': [[dmk0, ...], [dmk1, ...]],\\n                          'nonlinear_diffs': [[dk1, ...]],\\n                          'output_diff': [[dmk1, ...], [dk3, ...]]}}\\n        <BLANKLINE>\\n        Num rounds: 3\\n        Best related-key characteristic found:\\n        {'enc_ch_found': {'ch_weight': 4,\\n                          'der_weights': [[w0, ...], [w1, ...], [w2, ...]],\\n                          'emp_weight': ...,\\n                          'exact_weight': 4.24498041717605,\\n                          'input_diff': [[dp0, ...], [dp1, ...]],\\n                          'nonlinear_diffs': [[dx1, ...], [dx6, ...], [dx11, ...]],\\n                          'output_diff': [[dx12, ...], [dx14, ...]]},\\n         'key_ch_found': {'ch_weight': 2,\\n                          'der_weights': [[wk0, ...], [wk1, ...]],\\n                          'emp_weight': ...,\\n                          'exact_weight': 2.829986944784033,\\n                          'input_diff': [[dmk0, ...], [dmk1, ...], [dmk2, ...]],\\n                          'nonlinear_diffs': [[dk1, ...], [dk5, ...]],\\n                          'output_diff': [[dmk2, ...], [dk3, ...], [dk8, ...]]}}\\n        <BLANKLINE>\\n\\n    \\\"\\\"\\\"\\n    assert start_round <= end_round\\n    assert search_mode != RkChSearchMode.AllValid\\n\\n    smart_print = _get_smart_print(filename)\\n\\n    if verbose_level >= 1:\\n        smart_print(cipher.__name__, \\\"Related-Key Search\\\\n\\\")\\n        smart_print(\\\"Parameters:\\\")\\n        smart_print(\\\"\\\\tcipher:\\\", cipher.__name__)\\n        smart_print(\\\"\\\\tdiff_type:\\\", diff_type.__name__)\\n        smart_print(\\\"\\\\tinitial_ew:\\\", initial_ew)\\n        smart_print(\\\"\\\\tinitial_kw:\\\", initial_kw)\\n        smart_print(\\\"\\\\tsolver_name:\\\", solver_name)\\n        smart_print(\\\"\\\\tstart:\\\", start_round)\\n        smart_print(\\\"\\\\tend:\\\", end_round)\\n        smart_print(\\\"\\\\tkey_der_mode:\\\", key_der_mode)\\n        smart_print(\\\"\\\\tenc_der_mode:\\\", enc_der_mode)\\n        smart_print(\\\"\\\\tallow_zero_enc_input_diff:\\\", allow_zero_enc_input_diff)\\n        smart_print(\\\"\\\\tsearch_mode:\\\", search_mode)\\n        smart_print(\\\"\\\\tcheck:\\\", check)\\n        smart_print(\\\"\\\\tverbose_level:\\\", verbose_level)\\n        smart_print(\\\"\\\\tfilename:\\\", filename)\\n        smart_print()\\n\\n    for num_rounds in range(start_round, end_round + 1):\\n        cipher.set_rounds(num_rounds)\\n\\n        smart_print(\\\"Num rounds:\\\", num_rounds)\\n\\n        ch = characteristic.RelatedKeyCh(cipher, diff_type)\\n\\n        if verbose_level >= 2:\\n            smart_print(\\\"Characteristic:\\\")\\n            smart_print(ch)\\n\\n        problem = SearchRkCh(rkch=ch, key_der_mode=key_der_mode, enc_der_mode=enc_der_mode,\\n                             allow_zero_enc_input_diff=allow_zero_enc_input_diff,\\n                             allow_zero_key_input_diff=allow_zero_key_input_diff)\\n\\n        if verbose_level >= 2:\\n            smart_print(\\\"SMT problem (size {}):\\\".format(problem.formula_size()))\\n            smart_print(problem.hrepr(full_repr=verbose_level >= 3))\\n\\n        rkch_found = problem.solve(initial_ew, initial_kw, solver_name=solver_name, search_mode=search_mode, check=check,\\n                                   verbose_level=verbose_level, filename=filename)\\n\\n        if verbose_level >= 1:\\n            prefix = str(_get_time()) + \\\" | \\\"\\n            smart_print()\\n        else:\\n            prefix = \\\"\\\"\\n\\n        if rkch_found is None:\\n            smart_print(prefix + \\\"No related-key characteristic found\\\")\\n            return\\n        else:\\n            initial_ew = int(rkch_found.enc_ch_found.ch_weight)\\n            initial_kw = int(rkch_found.key_ch_found.ch_weight)\\n            smart_print(prefix + \\\"Best related-key characteristic found:\\\")\\n            smart_print(rkch_found)\\n            if verbose_level >= 3:\\n                smart_print(rkch_found.vrepr())\\n            smart_print()\"}, {\"identifier\":\"_get_smart_print\", \"path\":\"arxpy/smt/search.py\", \"snippet\":\"def _get_smart_print(filename=None):\\n    def smart_print(*msg, **kwargs):\\n        if filename is not None:\\n            with open(filename, \\\"a\\\") as fh:\\n                print(*msg, file=fh, flush=True, **kwargs)\\n        else:\\n            print(*msg, flush=True, **kwargs)\\n    return smart_print\"}, {\"identifier\":\"test_search_ch_skch\", \"path\":\"arxpy/smt/tests/test_search.py\", \"snippet\":\"def test_search_ch_skch(bvf_cipher, diff_type, initial_weight, solver_name, rounds, der_mode, search_mode, check,\\n                        verbose_level, filename):\\n    smart_print = _get_smart_print(filename)\\n\\n    if rounds is not None:\\n        bvf_cipher.set_rounds(rounds)\\n\\n    if issubclass(bvf_cipher, BvFunction):\\n        num_inputs = len(bvf_cipher.input_widths)\\n        input_diff_names = [\\\"dp\\\" + str(i) for i in range(num_inputs)]\\n        ch = BvCharacteristic(bvf_cipher, diff_type, input_diff_names)\\n    else:\\n        assert issubclass(bvf_cipher, Cipher)\\n        ch = SingleKeyCh(bvf_cipher, diff_type)\\n\\n    if verbose_level >= 1:\\n        str_rounds = \\\"\\\" if rounds is None else \\\"{} rounds\\\".format(rounds)\\n        smart_print(str_rounds, bvf_cipher.__name__, diff_type.__name__, type(ch).__name__)\\n        if verbose_level >= 2:\\n            smart_print(\\\"Characteristic:\\\")\\n            smart_print(ch)\\n\\n    if issubclass(bvf_cipher, BvFunction):\\n        problem = SearchCh(ch, der_mode=der_mode)\\n    else:\\n        problem = SearchSkCh(ch, der_mode=der_mode)\\n\\n    if verbose_level >= 1:\\n        smart_print(type(problem).__name__, der_mode, search_mode, solver_name,\\n                    \\\"size:\\\", problem.formula_size())\\n        if verbose_level >= 2:\\n            smart_print(problem.hrepr(verbose_level >= 3))\\n\\n    sol = problem.solve(initial_weight,\\n                        solver_name=solver_name,\\n                        search_mode=search_mode,\\n                        check=check,\\n                        verbose_level=verbose_level,\\n                        filename=filename)\\n\\n    if verbose_level >= 1:\\n        if sol is None:\\n            smart_print(\\\"\\\\nUnsatisfiable\\\")\\n        else:\\n            smart_print(\\\"\\\\nSolution:\\\")\\n            smart_print(sol)\\n            if verbose_level >= 2:\\n                if isinstance(sol, collections.abc.Sequence):\\n                    # for search_mode TopDifferentials\\n                    smart_print(sol[0].vrepr())\\n                else:\\n                    smart_print(sol.vrepr())\\n        smart_print()\\n\\n    return sol\"}, {\"identifier\":\"test_search_related_key_ch\", \"path\":\"arxpy/smt/tests/test_search.py\", \"snippet\":\"def test_search_related_key_ch(cipher, diff_type, initial_ew, initial_kw, solver_name,\\n                               rounds, key_der_mode, enc_der_mode, allow_zero_enc_input_diff,\\n                               search_mode, check, verbose_level, filename):\\n    # assert search_mode != RkChSearchMode.AllValid\\n\\n    smart_print = _get_smart_print(filename)\\n\\n    if rounds is not None:\\n        cipher.set_rounds(rounds)\\n\\n    ch = RelatedKeyCh(cipher, diff_type)\\n\\n    if verbose_level >= 1:\\n        smart_print(rounds, \\\"round(s)\\\", cipher.__name__, diff_type.__name__, type(ch).__name__)\\n        if verbose_level >= 2:\\n            smart_print(\\\"Characteristic:\\\")\\n            smart_print(ch)\\n\\n    problem = SearchRkCh(rkch=ch, key_der_mode=key_der_mode, enc_der_mode=enc_der_mode,\\n                         allow_zero_enc_input_diff=allow_zero_enc_input_diff)\\n\\n    if verbose_level >= 1:\\n        smart_print(type(problem).__name__, \\\"key/enc mode:\\\", key_der_mode, enc_der_mode,\\n                    search_mode, solver_name, \\\"size:\\\", problem.formula_size())\\n        if verbose_level >= 2:\\n            smart_print(problem.hrepr(verbose_level >= 3))\\n\\n    sol = problem.solve(\\n        initial_ew=initial_ew,\\n        initial_kw=initial_kw,\\n        solver_name=solver_name,\\n        search_mode=search_mode,\\n        check=check,\\n        verbose_level=verbose_level,\\n        filename=filename)\\n\\n    if verbose_level >= 1:\\n        if sol is None:\\n            smart_print(\\\"\\\\nUnsatisfiable\\\")\\n        else:\\n            smart_print(\\\"\\\\nSolution:\\\")\\n            smart_print(sol)\\n            if verbose_level >= 2:\\n                smart_print(sol.vrepr())\\n        smart_print()\\n\\n    return sol\"}, {\"identifier\":\"speck\", \"path\":\"arxpy/primitives/speck.py\", \"snippet\":\"class SpeckInstance(enum.Enum):\\n    class SpeckKeySchedule(KeySchedule):\\n    class SpeckEncryption(Encryption):\\n    class SpeckCipher(Cipher):\\ndef get_Speck_instance(speck_instance):\\n    def rf(x, y, k):\\n        def set_rounds(cls, new_rounds):\\n        def eval(cls, *master_key):\\n        def set_rounds(cls, new_rounds):\\n        def eval(cls, x, y):\\n        def set_rounds(cls, new_rounds):\\n        def test(cls):\"}, {\"identifier\":\"simon\", \"path\":\"arxpy/primitives/simon.py\", \"snippet\":\"class SimonRF(Operation):\\nclass XDSimonRF(Derivative):\\nclass SimonInstance(enum.Enum):\\n    class SimonKeySchedule(KeySchedule):\\n    class SimonEncryption(Encryption):\\n    class SimonCipher(Cipher):\\n    def output_width(cls, x):\\n    def eval(cls, x):\\n    def xor_derivative(cls, input_diff):\\n    def is_possible(self, output_diff):\\n        def is_even(x):\\n    def has_probability_one(self, output_diff):\\n    def weight(self, output_diff):\\n    def max_weight(self):\\n    def exact_weight(self, output_diff):\\n    def num_frac_bits(self):\\n    def error(self):\\ndef get_Simon_instance(simon_instance):\\n        def set_rounds(cls, new_rounds):\\n        def eval(cls, *master_key):\\n        def set_rounds(cls, new_rounds):\\n        def eval(cls, x, y):\\n        def set_rounds(cls, new_rounds):\\n        def test(cls):\"}, {\"identifier\":\"simeck\", \"path\":\"arxpy/primitives/simeck.py\", \"snippet\":\"class SimeckRF(SimonRF):\\nclass XDSimeckRF(XDSimonRF):\\nclass SimeckInstance(enum.Enum):\\n    class SimeckKeySchedule(KeySchedule):\\n    class SimeckEncryption(Encryption):\\n    class SimeckCipher(Cipher):\\n    def xor_derivative(cls, input_diff):\\ndef get_Simeck_instance(simeck_instance):\\n    def rf(x, y, k):\\n        def set_rounds(cls, new_rounds):\\n        def eval(cls, *master_key):\\n        def set_rounds(cls, new_rounds):\\n        def eval(cls, x, y):\\n        def set_rounds(cls, new_rounds):\\n        def test(cls):\\n                C = (2 ** n - 4) ^ int(z[i % len(z)])\"}, {\"identifier\":\"HightCipher\", \"path\":\"arxpy/primitives/hight.py\", \"snippet\":\"class HightCipher(Cipher):\\n    key_schedule = HightKeySchedule\\n    encryption = HightEncryption\\n    rounds = 32\\n\\n    @classmethod\\n    def set_rounds(cls, new_rounds):\\n        cls.rounds = new_rounds\\n        cls.key_schedule.set_rounds(new_rounds)\\n        cls.encryption.set_rounds(new_rounds)\\n\\n    @classmethod\\n    def test(cls):\\n        \\\"\\\"\\\"Test Hight with official test vectors.\\\"\\\"\\\"\\n        # https://tools.ietf.org/html/draft-kisa-hight-00#section-5\\n        old_rounds = cls.rounds\\n        cls.set_rounds(32)\\n\\n        plaintext = (0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00, 0x00)\\n        key = (0x00, 0x11, 0x22, 0x33, 0x44, 0x55, 0x66, 0x77,\\n               0x88, 0x99, 0xaa, 0xbb, 0xcc, 0xdd, 0xee, 0xff)\\n        assert cls(plaintext, key) == (0x00, 0xf4, 0x18, 0xae, 0xd9, 0x4f, 0x03, 0xf2)\\n\\n        plaintext = (0x00, 0x11, 0x22, 0x33, 0x44, 0x55, 0x66, 0x77)\\n        key = (0xff, 0xee, 0xdd, 0xcc, 0xbb, 0xaa, 0x99, 0x88, 0x77, 0x66, 0x55, 0x44, 0x33, 0x22, 0x11, 0x00)\\n        assert cls(plaintext, key) == (0x23, 0xce, 0x9f, 0x72, 0xe5, 0x43, 0xe6, 0xd8)\\n\\n        cls.set_rounds(old_rounds)\"}, {\"identifier\":\"LeaCipher\", \"path\":\"arxpy/primitives/lea.py\", \"snippet\":\"class LeaCipher(Cipher):\\n    key_schedule = LeaKeySchedule\\n    encryption = LeaEncryption\\n    rounds = 24\\n\\n    @classmethod\\n    def set_rounds(cls, new_rounds):\\n        cls.rounds = new_rounds\\n        cls.key_schedule.set_rounds(new_rounds)\\n        cls.encryption.set_rounds(new_rounds)\\n\\n    @classmethod\\n    def test(cls):\\n        old_rounds = cls.rounds\\n        cls.set_rounds(24)\\n        key = [\\n            Constant(0x3c2d1e0f, 32),\\n            Constant(0x78695a4b, 32),\\n            Constant(0xb4a59687, 32),\\n            Constant(0xf0e1d2c3, 32)\\n        ]\\n        pt = [\\n            Constant(0x13121110, 32),\\n            Constant(0x17161514, 32),\\n            Constant(0x1b1a1918, 32),\\n            Constant(0x1f1e1d1c, 32)\\n        ]\\n        ct = [\\n            Constant(0x354ec89f, 32),\\n            Constant(0x18c6c628, 32),\\n            Constant(0xa7c73255, 32),\\n            Constant(0xfd8b6404, 32)\\n        ]\\n        assert cls(pt, key) == tuple(ct)\\n        cls.set_rounds(old_rounds)\"}, {\"identifier\":\"Shacal1Cipher\", \"path\":\"arxpy/primitives/shacal1.py\", \"snippet\":\"class Shacal1Cipher(Cipher):\\n    key_schedule = Shacal1KeySchedule\\n    encryption = Shacal1Encryption\\n    rounds = 80\\n\\n    @classmethod\\n    def set_rounds(cls, new_rounds):\\n        cls.rounds = new_rounds\\n        cls.key_schedule.set_rounds(new_rounds)\\n        cls.encryption.set_rounds(new_rounds)\\n\\n    # noinspection SpellCheckingInspection\\n    @classmethod\\n    def test(cls):\\n        # https://www.cosic.esat.kuleuven.be/nessie/testvectors/\\n        # key = 80000000000000000000000000000000\\n        # 00000000000000000000000000000000\\n        # 00000000000000000000000000000000\\n        # 00000000000000000000000000000000\\n        # plain = 0000000000000000000000000000000000000000\\n        # cipher = 0FFD8D43 B4E33C7C 53461BD1 0F27A546 1050D90D\\n\\n        old_rounds = cls.rounds\\n        cls.set_rounds(80)\\n\\n        global REFERENCE_VERSION\\n\\n        for ref_v in [True, False]:\\n            REFERENCE_VERSION = ref_v\\n\\n            key = [Constant(0x80000000, 32)]\\n            key.extend([Constant(0, 32) for _ in range(1, N)])\\n            pt = [Constant(0, 32) for _ in range(5)]\\n            ct = [\\n                Constant(0x0FFD8D43, 32),\\n                Constant(0xB4E33C7C, 32),\\n                Constant(0x53461BD1, 32),\\n                Constant(0x0F27A546, 32),\\n                Constant(0x01050D90D, 32)\\n            ]\\n            assert cls(pt, key) == tuple(ct)\\n\\n            if N == 16:\\n                # key =    00010203 04050607 08090A0B 0C0D0E0F\\n                #          10111213 14151617 18191A1B 1C1D1E1F\\n                #          20212223 24252627 28292A2B 2C2D2E2F\\n                #          30313233 34353637 38393A3B 3C3D3E3F\\n                # plain =  00112233 44556677 8899AABB CCDDEEFF 10213243\\n                # cipher = 213A4657 59CE3572 CA2A86D5 A680E948 45BEAA6B\\n\\n                key = [\\n                    Constant(0x00010203, 32), Constant(0x04050607, 32), Constant(0x08090A0B, 32), Constant(0x0C0D0E0F, 32),\\n                    Constant(0x10111213, 32), Constant(0x14151617, 32), Constant(0x18191A1B, 32), Constant(0x1C1D1E1F, 32),\\n                    Constant(0x20212223, 32), Constant(0x24252627, 32), Constant(0x28292A2B, 32), Constant(0x2C2D2E2F, 32),\\n                    Constant(0x30313233, 32), Constant(0x34353637, 32), Constant(0x38393A3B, 32), Constant(0x3C3D3E3F, 32),\\n                ]\\n                pt = [\\n                    Constant(0x00112233, 32),\\n                    Constant(0x44556677, 32),\\n                    Constant(0x8899AABB, 32),\\n                    Constant(0xCCDDEEFF, 32),\\n                    Constant(0x10213243, 32),\\n                ]\\n                ct = [\\n                    Constant(0x213A4657, 32),\\n                    Constant(0x59CE3572, 32),\\n                    Constant(0xCA2A86D5, 32),\\n                    Constant(0xA680E948, 32),\\n                    Constant(0x45BEAA6B, 32)\\n                ]\\n                assert cls(pt, key) == tuple(ct)\\n\\n        cls.set_rounds(old_rounds)\"}, {\"identifier\":\"Shacal2Cipher\", \"path\":\"arxpy/primitives/shacal2.py\", \"snippet\":\"class Shacal2Cipher(Cipher):\\n    key_schedule = Shacal2KeySchedule\\n    encryption = Shacal2Encryption\\n    rounds = 64\\n\\n    @classmethod\\n    def set_rounds(cls, new_rounds):\\n        cls.rounds = new_rounds\\n        cls.key_schedule.set_rounds(new_rounds)\\n        cls.encryption.set_rounds(new_rounds)\\n\\n    # noinspection SpellCheckingInspection\\n    @classmethod\\n    def test(cls):\\n        # https://www.cosic.esat.kuleuven.be/nessie/testvectors/\\n        # key =\\n        # 80000000000000000000000000000000\\n        # 00000000000000000000000000000000\\n        # 00000000000000000000000000000000\\n        # 00000000000000000000000000000000\\n        # plain =\\n        # 00000000000000000000000000000000\\n        # 00000000000000000000000000000000\\n        # cipher =\\n        # 361AB632 2FA9E7A7 BB23818D 839E01BD\\n        # DAFDF473 05426EDD 297AEDB9 F6202BAE\\n\\n        old_rounds = cls.rounds\\n        cls.set_rounds(64)\\n\\n        global REFERENCE_VERSION\\n\\n        for ref_v in [True, False]:\\n            REFERENCE_VERSION = ref_v\\n\\n            key = [Constant(0x80000000, 32)]\\n            key.extend([Constant(0, 32) for _ in range(1, N)])\\n            pt = [Constant(0, 32) for _ in range(8)]\\n            ct = [\\n                Constant(0x361AB632, 32),\\n                Constant(0x2FA9E7A7, 32),\\n                Constant(0xBB23818D, 32),\\n                Constant(0x839E01BD, 32),\\n                Constant(0xDAFDF473, 32),\\n                Constant(0x05426EDD, 32),\\n                Constant(0x297AEDB9, 32),\\n                Constant(0xF6202BAE, 32),\\n            ]\\n            assert cls(pt, key) == tuple(ct)\\n\\n            if N == 16:\\n                # key =\\n                # 00010203 04050607 08090A0B 0C0D0E0F\\n                # 10111213 14151617 18191A1B 1C1D1E1F\\n                # 20212223 24252627 28292A2B 2C2D2E2F\\n                # 30313233 34353637 38393A3B 3C3D3E3F\\n                # plain =\\n                # 00112233 44556677 8899AABB CCDDEEFF\\n                # 10213243 54657687 98A9BACB DCEDFE0F\\n                # cipher =\\n                # 1A6B234A 20EAD408 C2D83B35 8AC81D7A\\n                # 648ED25D 01B7C9EC 9CC4C9E2 5CFA813E\\n\\n                key = [\\n                    Constant(0x00010203, 32), Constant(0x04050607, 32), Constant(0x08090A0B, 32), Constant(0x0C0D0E0F, 32),\\n                    Constant(0x10111213, 32), Constant(0x14151617, 32), Constant(0x18191A1B, 32), Constant(0x1C1D1E1F, 32),\\n                    Constant(0x20212223, 32), Constant(0x24252627, 32), Constant(0x28292A2B, 32), Constant(0x2C2D2E2F, 32),\\n                    Constant(0x30313233, 32), Constant(0x34353637, 32), Constant(0x38393A3B, 32), Constant(0x3C3D3E3F, 32),\\n                ]\\n                pt = [\\n                    Constant(0x00112233, 32),\\n                    Constant(0x44556677, 32),\\n                    Constant(0x8899AABB, 32),\\n                    Constant(0xCCDDEEFF, 32),\\n                    Constant(0x10213243, 32),\\n                    Constant(0x54657687, 32),\\n                    Constant(0x98A9BACB, 32),\\n                    Constant(0xDCEDFE0F, 32),\\n                ]\\n                ct = [\\n                    Constant(0x1A6B234A, 32),\\n                    Constant(0x20EAD408, 32),\\n                    Constant(0xC2D83B35, 32),\\n                    Constant(0x8AC81D7A, 32),\\n                    Constant(0x648ED25D, 32),\\n                    Constant(0x01B7C9EC, 32),\\n                    Constant(0x9CC4C9E2, 32),\\n                    Constant(0x5CFA813E, 32),\\n                ]\\n                assert cls(pt, key) == tuple(ct)\\n\\n        cls.set_rounds(old_rounds)\"}, {\"identifier\":\"FealCipher\", \"path\":\"arxpy/primitives/feal.py\", \"snippet\":\"class FealCipher(Cipher):\\n    key_schedule = FealXKeySchedule if FEALX else FealKeySchedule\\n    encryption = FealEncryption\\n    rounds = 8\\n\\n    @classmethod\\n    def set_rounds(cls, new_rounds):\\n        cls.rounds = new_rounds\\n        cls.encryption.set_rounds(new_rounds)\\n        cls.key_schedule.set_rounds(new_rounds)\\n\\n    @classmethod\\n    def test(cls):\\n        \\\"\\\"\\\"Test FEAL with official test vectors.\\\"\\\"\\\"\\n        # Extracted from \\\"The FEAL Cipher Family\\\"\\n        old_rounds = cls.rounds\\n        cls.set_rounds(8)\\n\\n        if not FEALX:\\n            plaintext = (0, 0, 0, 0, 0, 0, 0, 0)\\n            key = (0x01, 0x23, 0x45, 0x67, 0x89, 0xAB, 0xCD, 0xEF)\\n            ciphertext = (0xCE, 0xEF, 0x2C, 0x86, 0xF2, 0x49, 0x07, 0x52)\\n            assert cls(plaintext, key) == ciphertext\\n        else:\\n            plaintext = (0, 0, 0, 0, 0, 0, 0, 0)\\n            key = (0x01, 0x23, 0x45, 0x67, 0x89, 0xAB, 0xCD, 0xEF)\\n            key += key  # double the size\\n            ciphertext = (0x92, 0xBE, 0xB6, 0x5D, 0x0E, 0x93, 0x82, 0xFB)\\n            assert cls(plaintext, key) == ciphertext\\n\\n        cls.set_rounds(old_rounds)\"}, {\"identifier\":\"TeaCipher\", \"path\":\"arxpy/primitives/tea.py\", \"snippet\":\"class TeaCipher(Cipher):\\n    key_schedule = TeaKeySchedule\\n    encryption = TeaEncryption\\n    rounds = 32\\n\\n    @classmethod\\n    def set_rounds(cls, new_rounds):\\n        cls.rounds = new_rounds\\n        cls.encryption.set_rounds(new_rounds)\\n\\n    @classmethod\\n    def test(cls):\\n        \\\"\\\"\\\"Test tea with official test vectors.\\\"\\\"\\\"\\n        cls.set_rounds(32)\\n\\n        plaintext = (0, 0)\\n        key = (0, 0, 0, 0)\\n        assert cls(plaintext, key) == (0x41EA3A0A, 0x94BAA940)\\n\\n        plaintext = (0x01020304, 0x05060708)\\n        key = (0x00112233, 0x44556677, 0x8899AABB, 0xCCDDEEFF)\\n        assert cls(plaintext, key) == (0xDEB1C0A2, 0x7E745DB3)\"}, {\"identifier\":\"XteaCipher\", \"path\":\"arxpy/primitives/xtea.py\", \"snippet\":\"class XteaCipher(Cipher):\\n    key_schedule = XteaKeySchedule\\n    encryption = XteaEncryption\\n    rounds = 64\\n\\n    @classmethod\\n    def set_rounds(cls, new_rounds):\\n        # assert new_rounds >= 2\\n        cls.rounds = new_rounds\\n        cls.encryption.set_rounds(new_rounds)\\n        cls.key_schedule.set_rounds(new_rounds)\\n\\n    @classmethod\\n    def test(cls):\\n        \\\"\\\"\\\"Test Xtea with official test vectors.\\\"\\\"\\\"\\n        # https://go.googlesource.com/crypto/+/master/xtea/xtea_test.go\\n        plaintext = (0x41424344, 0x45464748)\\n        key = (0, 0, 0, 0)\\n        assert cls(plaintext, key) == (0xa0390589, 0xf8b8efa5)\\n\\n        plaintext = (0x41424344, 0x45464748)\\n        key = (0x00010203, 0x04050607, 0x08090A0B, 0x0C0D0E0F)\\n        assert cls(plaintext, key) == (0x497df3d0, 0x72612cb5)\"}, {\"identifier\":\"Multi2Cipher\", \"path\":\"arxpy/primitives/multi2.py\", \"snippet\":\"class Multi2Cipher(Cipher):\\n    key_schedule = Multi2KeySchedule\\n    encryption = Multi2Encryption\\n    rounds = 32\\n\\n    @classmethod\\n    def set_rounds(cls, new_rounds):\\n        cls.rounds = new_rounds\\n        cls.encryption.set_rounds(new_rounds)\\n        cls.key_schedule.set_rounds(new_rounds)\\n\\n    @classmethod\\n    def test(cls):\\n        old_rounds = cls.rounds\\n\\n        global REFERENCE_VERSION\\n\\n        for ref_v in [True, False]:\\n            REFERENCE_VERSION = ref_v\\n\\n            cls.set_rounds(32)\\n\\n            plaintext = [0, 0]\\n            key = [0 for _ in range(len(cls.key_schedule.input_widths))]\\n            ciphertext = (0x1d9dfa1e, 0x4d64bc67)\\n            assert cls(plaintext, key) == ciphertext\\n\\n            plaintext = [0x01, 0x23]\\n            key = [0x01, 0x23, 0x45, 0x67, 0x89, 0xAB, 0xCD, 0xEF, 0x01, 0x23]\\n            ciphertext = (0xd241e7c8, 0x74166979)\\n            assert cls(plaintext, key) == ciphertext\\n\\n        cls.set_rounds(old_rounds)\"}, {\"identifier\":\"cham\", \"path\":\"arxpy/primitives/cham.py\", \"snippet\":\"class ChamInstance(enum.Enum):\\n    class ChamKeySchedule(KeySchedule):\\n    class ChamEncryption(Encryption):\\n    class ChamCipher(Cipher):\\ndef get_Cham_instance(cham_instance):\\n        def set_rounds(cls, new_rounds):\\n        def eval(cls, *master_key):\\n        def set_rounds(cls, new_rounds):\\n        def eval(cls, p0, p1, p2, p3):\\n        def set_rounds(cls, new_rounds):\\n        def test(cls):\"}]", "import_statement": "import math\nimport collections\nimport functools\nimport os\nimport unittest\nfrom datetime import datetime\nfrom arxpy.bitvector.operation import Concat, BvComp\nfrom arxpy.differential.difference import XorDiff\nfrom arxpy.smt.search import (\n    DerMode, SkChSearchMode, RkChSearchMode,\n    round_based_search_SkCh, round_based_search_RkCh,\n    _get_smart_print\n)\nfrom arxpy.smt.tests.test_search import test_search_ch_skch, test_search_related_key_ch\nfrom arxpy.primitives import speck\nfrom arxpy.primitives import simon\nfrom arxpy.primitives import simeck\nfrom arxpy.primitives.hight import HightCipher\nfrom arxpy.primitives.lea import LeaCipher\nfrom arxpy.primitives.shacal1 import Shacal1Cipher\nfrom arxpy.primitives.shacal2 import Shacal2Cipher\nfrom arxpy.primitives.feal import FealCipher\nfrom arxpy.primitives.tea import TeaCipher\nfrom arxpy.primitives.xtea import XteaCipher\nfrom arxpy.primitives.multi2 import Multi2Cipher\nfrom arxpy.primitives import cham", "code": "\"\"\"Tests for cryptographic primitives.\"\"\"\n\n\n\n\n# from arxpy.primitives.threefish import ThreefishCipher\n\n\nOUTPUT_FILE = False\nVERBOSE_LEVEL = 0\n# 0: quiet\n# 1: basic info\n# 2: + ssa, model\n# 3: + hrepr\n# 4: + full hrepr\nCHECK = True\n\n\n# *_rounds should have trivial characteristics but *_rounds + 1 not (with ProbabilityOne)\nBlockCipher = collections.namedtuple('BlockCipher', ['cipher', 'sk_rounds', 'rk_rounds'])\n\n\nSpeck32 = speck.get_Speck_instance(speck.SpeckInstance.speck_32_64)\nSimon32 = simon.get_Simon_instance(simon.SimonInstance.simon_32_64)\nSimeck32 = simeck.get_Simeck_instance(simeck.SimeckInstance.simeck_32_64)\nCham64 = cham.get_Cham_instance(cham.ChamInstance.cham_64_128)\n\n\n\nBLOCK_CIPHERS = [\n    BlockCipher(Speck32, 1, 4),\n    BlockCipher(Simon32, 1, 4),\n    BlockCipher(Simeck32, 1, 4),\n    BlockCipher(Cham64, 4, 6),\n    BlockCipher(HightCipher, 2, None),  # for rk_rounds=7, ExactWeightError is raised\n    BlockCipher(LeaCipher, 2, 1),\n    BlockCipher(Shacal1Cipher, 1, 16),\n    BlockCipher(Shacal2Cipher, 1, None),  # for rk_rounds=18, ExactWeightError is raised\n    BlockCipher(FealCipher, 3, 1),\n    BlockCipher(TeaCipher, 1, TeaCipher.rounds - 1),\n    BlockCipher(XteaCipher, 1, 8),\n    BlockCipher(Multi2Cipher, 2, 2),\n    # BlockCipher(ThreefishCipher, ?, ?),  # huge block size\n]\n\nSKOption = collections.namedtuple('SingleKeyOptions', ['der_mode', 'search_mode'])\nRKOption = collections.namedtuple('RelatedKeyOptions', ['key_der_mode', 'enc_der_mode',\n                                                        'search_mode', 'initial_ew', 'initial_kw'])\n\nNoCheckModes = [SkChSearchMode.FirstCh, RkChSearchMode.FirstMinSum]\n\nSK_OPTIONS = [\n", "next_line": "    SKOption(DerMode.ProbabilityOne, SkChSearchMode.Optimal),", "gold_snippet_index": 3, "id": 11, "__internal_uuid__": "9e93c1e6-2355-4d38-b58b-dc30ef7ad7c2"}
{"repo_name": "DistrictDataLabs/partisan-discourse", "file_path": "arbiter/management/commands/train.py", "context": "[{\"identifier\":\"Estimator\", \"path\":\"arbiter/models.py\", \"snippet\":\"class Estimator(TimeStampedModel):\\n    \\\"\\\"\\\"\\n    Stores a Scikit-Learn Estimator object as a pickle in the database.\\n    \\\"\\\"\\\"\\n\\n    # Model types to help decide on evaluation criteria\\n    TYPES = Choices('classifier', 'regression', 'clusters', 'decomposition')\\n\\n    model_type  = models.CharField(choices=TYPES, max_length=32) # The type of the estimator\\n    model_class = models.CharField(max_length=255, **nullable)   # The class name of the estimator\\n    model_form  = models.CharField(max_length=512, **nullable)   # The repr of the estimator\\n    estimator   = PickledObjectField(**nullable)                 # The pickled object model\\n    build_time  = models.DurationField(**nullable)               # The amount of time it took to buld\\n    owner       = models.ForeignKey('auth.User', **nullable)     # The owner, if any, of the model\\n    corpus      = models.ForeignKey('corpus.Corpus', **nullable) # The corpus the estimator was trained on\\n\\n    class Meta:\\n        db_table = \\\"estimators\\\"\\n        get_latest_by = \\\"created\\\"\\n\\n    def __str__(self):\\n        s =  \\\"{} {} ({})\\\".format(\\n            self.model_class, self.model_type.title(), self.created.strftime('%Y-%m-%d')\\n        )\\n\\n        if self.owner:\\n            s += \\\" for {}\\\".format(self.owner)\\n\\n        return s\"}, {\"identifier\":\"Score\", \"path\":\"arbiter/models.py\", \"snippet\":\"class Score(TimeStampedModel):\\n    \\\"\\\"\\\"\\n    Stores an evaluation metric for an estimator.\\n    \\\"\\\"\\\"\\n\\n    # Metrics define how a specific estimator is scored\\n    METRICS = Choices(\\n        'accuracy', 'auc', 'brier', 'f1', 'fbeta', 'hamming', 'hinge',\\n        'jaccard', 'logloss', 'mcc', 'precision', 'recall', 'roc', 'support',\\n        'mae', 'mse', 'mdae', 'r2',\\n        'rand', 'completeness', 'homogeneity', 'mutual', 'silhouette', 'v',\\n        'time',\\n    )\\n\\n    metric    = models.CharField(choices=METRICS, max_length=32)    # The type of the score\\n    score     = models.FloatField(**nullable)                       # The actual value of the score\\n    label     = models.CharField(max_length=32, **nullable)         # The label, if any, of the score\\n    folds     = ArrayField(models.FloatField(), **nullable)         # Cross-validation scores\\n    estimator = models.ForeignKey(Estimator, related_name='scores') # The estimator being evaluated\\n\\n    class Meta:\\n        db_table = \\\"evaluations\\\"\\n        get_latest_by = \\\"created\\\"\\n\\n    def __str__(self):\\n        s = \\\"{} score for {} = {:0.3f}\\\".format(\\n            self.metric.title(), self.estimator, self.score\\n        )\\n\\n        if self.label:\\n            s = \\\"{} \\\".format(self.label.title()) + s\\n\\n        return s\"}, {\"identifier\":\"TranscriptCorpusReader\", \"path\":\"corpus/reader.py\", \"snippet\":\"class TranscriptCorpusReader(CategorizedPlaintextCorpusReader):\\n\\n    def __init__(self, root, **kwargs):\\n        CategorizedPlaintextCorpusReader.__init__(\\n        \\tself, root, DOC_PATTERN, cat_pattern=CAT_PATTERN\\n        )\\n\\n    def tagged(self, **kwargs):\\n        \\\"\\\"\\\"\\n        Returns part-of-speech tagged words in sentences in paragraphs.\\n        \\\"\\\"\\\"\\n        for para in self.paras(**kwargs):\\n            yield [\\n                nltk.pos_tag(sent) for sent in para\\n            ]\"}, {\"identifier\":\"Corpus\", \"path\":\"corpus/models.py\", \"snippet\":\"class Corpus(TimeStampedModel):\\n    \\\"\\\"\\\"\\n    A model that maintains a mapping of documents to estimators for use in\\n    tracking the training data that is used to fit a text classifier object.\\n    \\\"\\\"\\\"\\n\\n    title     = models.CharField(max_length=255, **nullable)\\n    slug      = AutoSlugField(populate_from='title', unique=True)\\n    documents = models.ManyToManyField('corpus.Document', through='LabeledDocument', related_name='corpora')\\n    user      = models.ForeignKey('auth.User', related_name='corpora', **nullable)\\n    labeled   = models.BooleanField(default=True)\\n\\n    objects   = CorpusManager()\\n\\n    class Meta:\\n        db_table = \\\"corpora\\\"\\n        get_latest_by = \\\"created\\\"\\n        ordering = [\\\"-created\\\"]\\n        verbose_name = \\\"corpus\\\"\\n        verbose_name_plural = \\\"corpora\\\"\\n\\n    def __str__(self):\\n        if self.title:\\n            return self.title\\n\\n        # Construct the descriptive string.\\n        s = \\\"{} document corpus created on {}\\\".format(\\n            self.documents.count(), self.created.strftime(\\\"%Y-%m-%d\\\")\\n        )\\n\\n        if self.user:\\n            s += \\\" by {}\\\".format(self.user)\\n\\n        return s\"}, {\"identifier\":\"Document\", \"path\":\"corpus/models.py\", \"snippet\":\"class Document(TimeStampedModel):\\n    \\\"\\\"\\\"\\n    Describes a document that is part of one or more corpora.\\n    \\\"\\\"\\\"\\n\\n    title     = models.CharField(max_length=255, **nullable)                 # The title of the document, extracted from HTML\\n    long_url  = models.URLField(max_length=2000, unique=True)                # The long url for the document\\n    short_url = models.URLField(max_length=30, **nullable)                   # The bit.ly shortened url\\n    raw_html  = models.TextField(**nullable)                                 # The html content fetched (hopefully)\\n    content   = PickledObjectField(**nullable)                               # The preprocessed NLP content in a parsable text representation\\n    signature = models.CharField(max_length=44, editable=False, **nullable)  # A base64 encoded hash of the content\\n    n_words   = models.SmallIntegerField(**nullable)                         # The word count of the document\\n    n_vocab   = models.SmallIntegerField(**nullable)                         # The size of the vocabulary used\\n\\n    # Users are associated with documents by downloading and annotating them.\\n    users     = models.ManyToManyField(\\n        'auth.User', through='corpus.Annotation', related_name='documents'\\n    )\\n\\n    class Meta:\\n        db_table = \\\"documents\\\"\\n        get_latest_by = \\\"created\\\"\\n        unique_together = (\\\"long_url\\\", \\\"short_url\\\")\\n\\n    def label(self, user=None):\\n        \\\"\\\"\\\"\\n        If a user is specified then returns the label for that user. Otherwise\\n        returns the majority voted label for the document in the corpus.\\n        \\\"\\\"\\\"\\n        # If a user is supplied get their annotation and return the label.\\n        if user is not None:\\n            annotation = self.annotations.filter(user=user).first()\\n            if annotation: return annotation.label\\n\\n        # Otherwise aggregate the annotations per document.\\n        # TODO: Add annotator aggreement logic here!\\n        else:\\n            labels = self.labels.annotate(votes=models.Count('id'))\\n            votes  = [(label, label.votes) for label in labels]\\n            if votes:\\n                # If we have more than one thing being voted for.\\n                if len(votes) > 1:\\n                    # Check if a tie between all labels\\n                    if all([v[1] == o[1] for o in votes for v in votes]):\\n                        return None\\n\\n                    # Select the label that has the most votes\\n                    vote = max(votes, key=itemgetter(1))\\n\\n                # Otherwise we've just got one thing being voted for\\n                else:\\n                    vote = votes[0]\\n\\n                # Make sure that there are enough votes for an article\\n                if vote[1] > 0:\\n                    return vote[0]\\n\\n        return None\\n\\n    def get_absolute_url(self):\\n        \\\"\\\"\\\"\\n        Returns the detail view url for the object\\n        \\\"\\\"\\\"\\n        return reverse('corpus:document-detail', args=(self.id,))\\n\\n    def __str__(self):\\n        if self.title: return self.title\\n        return self.short_url\"}, {\"identifier\":\"LabeledDocument\", \"path\":\"corpus/models.py\", \"snippet\":\"class LabeledDocument(TimeStampedModel):\\n    \\\"\\\"\\\"\\n    A model that tracks the relationship between documents and corpora and\\n    ensures that every document has a static label (or not) so that any model\\n    that has been generated is reproducible.\\n    \\\"\\\"\\\"\\n\\n    corpus   = models.ForeignKey('corpus.Corpus', related_name='labels')\\n    document = models.ForeignKey('corpus.Document', related_name='+')\\n    label    = models.ForeignKey('corpus.Label', **nullable)\\n\\n    class Meta:\\n        db_table = \\\"corpora_documents\\\"\\n\\n    def __str__(self):\\n        return \\\"{} ({})\\\".format(self.document, self.label)\"}, {\"identifier\":\"QueryCorpusReader\", \"path\":\"corpus/reader.py\", \"snippet\":\"class QueryCorpusReader(object):\\n    \\\"\\\"\\\"\\n    The query corpus reader takes in a query that yields a list of documents\\n    and modifies it such that it is only fetching the preprocessed content in\\n    a streaming fashion.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, query, user=None):\\n        \\\"\\\"\\\"\\n        Pass in a QuerySet or Query object for selecting a group of documents.\\n        Can also optionally pass in a user to determine labeling scheme.\\n        \\\"\\\"\\\"\\n        self.user  = user\\n        self.query = query\\n\\n    def fileids(self, categories=None):\\n        \\\"\\\"\\\"\\n        Returns a list of file primary keys for the files that make up this\\n        corpus or that make up the given category(s) if specified.\\n\\n        Categories can be either a single string or a list of strings.\\n        \\\"\\\"\\\"\\n        # If categories is None, return all fileids.\\n        if categories is None:\\n            return self.query.values_list('id', flat=True)\\n\\n        # Convert to a list if a singleton is passed\\n        if isinstance(categories, (str, Label)):\\n            categories = [categories,]\\n\\n        # Convert to a quick lookup data structure\\n        categories = set(categories)\\n\\n        # Manually loop through all documents (bummer)\\n        return [\\n            doc.id for doc in self.query\\n            if doc.label(self.user) in categories\\n        ]\\n\\n    def categories(self, fileids=None):\\n        \\\"\\\"\\\"\\n        Return a list of file identifiers of the categories defined for this\\n        corpus or the file(s) if it is given.\\n\\n        Fileids can be either a list of integers or a single integer.\\n        \\\"\\\"\\\"\\n        # If fileids is None, return all categories\\n        # HACK: use a unique query on the database\\n        if fileids is None:\\n            return list(set([\\n                str(doc.label(self.user)) for doc in self.query\\n            ]))\\n\\n        # Convert to a list if a singleton is passed\\n        if isinstance(fileids, int):\\n            fileids = [fileids,]\\n\\n        return list(set([\\n            str(doc.label(self.user))\\n            for doc in self.query.filter(id__in=fileids)\\n        ]))\\n\\n    def tagged(self, fileids=None, categories=None):\\n        \\\"\\\"\\\"\\n        Returns the content of each document.\\n        \\\"\\\"\\\"\\n        if fileids is None:\\n            fileids = self.fileids(categories)\\n\\n        if isinstance(fileids, int):\\n            fileids = [fileids,]\\n\\n        for doc in self.query.filter(id__in=fileids).values_list('content', flat=True):\\n            for para in doc:\\n                yield para\"}, {\"identifier\":\"CorpusModelReader\", \"path\":\"corpus/reader.py\", \"snippet\":\"class CorpusModelReader(QueryCorpusReader):\\n    \\\"\\\"\\\"\\n    Takes a corpus object and automatically references documents.\\n\\n    Note this class takes advantage of the LabeledDocument through model\\n    between documents and corpora in order to perform queries on the database.\\n    The QueryCorpusReader relies on the label() method of a document for\\n    label discovery and therefore cannot do filtering or querying based on\\n    data that is stored in the database.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, corpus):\\n        self.corpus = corpus\\n        super(CorpusModelReader, self).__init__(\\n            corpus.documents.all(), corpus.user\\n        )\\n\\n    def fileids(self, categories=None):\\n        \\\"\\\"\\\"\\n        Returns a list of file primary keys for the files that make up this\\n        corpus or that make up the given category(s) if specified.\\n\\n        Categories can be either a single string or a list of strings.\\n        \\\"\\\"\\\"\\n        # If categories is None, return all fileids.\\n        if categories is None:\\n            return self.query.values_list('id', flat=True)\\n\\n        # Convert to a list if a singleton is passed\\n        if isinstance(categories, (str, Label)):\\n            categories = [categories,]\\n\\n        # Convert to a quick lookup data structure\\n        categories = set(categories)\\n\\n        # Filter the labeled documents based on the label.\\n        query = self.corpus.labels.filter(label__in=categories)\\n        return query.values_list('document_id', flat=True)\\n\\n    def categories(self, fileids=None):\\n        \\\"\\\"\\\"\\n        Return a list of file identifiers of the categories defined for this\\n        corpus or the file(s) if it is given.\\n\\n        Fileids can be either a list of integers or a single integer.\\n        \\\"\\\"\\\"\\n        # If fileids is None, return all categories\\n        if fileids is None:\\n            labels = self.corpus.labels.values_list('label', flat=True).distinct()\\n            return Label.objects.filter(id__in=labels).values_list('slug', flat=True)\\n\\n        # Convert to a list if a singleton is passed\\n        if isinstance(fileids, int):\\n            fileids = [fileids,]\\n\\n        labels = self.corpus.labels.filter(document_id__in=fileids)\\n        labels = labels.values_list('label', flat=True).distinct()\\n        return Label.objects.filter(id__in=labels).values_list('slug', flat=True)\"}, {\"identifier\":\"CorpusLoader\", \"path\":\"corpus/learn.py\", \"snippet\":\"class CorpusLoader(object):\\n    \\\"\\\"\\\"\\n    The corpus loader knows how to deal with an NLTK corpus at the top of a\\n    pipeline by simply taking as input a corpus to read from. It exposes both\\n    the data and the labels and can be set up to do cross-validation.\\n\\n    If a number of folds is passed in for cross-validation, then the loader\\n    is smart about how to access data for train/test splits. Otherwise it will\\n    simply yield all documents in the corpus.\\n    \\\"\\\"\\\"\\n\\n    def __init__(self, corpus, folds=None, shuffle=True):\\n        self.n_docs = len(corpus.fileids())\\n        self.corpus = corpus\\n        self.folds  = folds\\n\\n        if folds is not None:\\n            # Generate the KFold cross validation for the loader.\\n            self.folds = KFold(self.n_docs, folds, shuffle)\\n\\n    @property\\n    def n_folds(self):\\n        \\\"\\\"\\\"\\n        Returns the number of folds if it exists; 0 otherwise.\\n        \\\"\\\"\\\"\\n        if self.folds is None: return 0\\n        return self.folds.n_folds\\n\\n    def fileids(self, fold=None, train=False, test=False):\\n        \\\"\\\"\\\"\\n        Returns a listing of the documents filtering to retreive specific\\n        data from the folds/splits. If no fold, train, or test is specified\\n        then the method will return all fileids.\\n\\n        If a fold is specified (should be an integer between 0 and folds),\\n        then the loader will return documents from that fold. Further, train\\n        or test must be specified to split the fold correctly.\\n        \\\"\\\"\\\"\\n        if fold is None:\\n            # If no fold is specified, return all the fileids.\\n            return self.corpus.fileids()\\n\\n        # Otherwise, identify the fold specifically and get the train/test idx\\n        for fold_idx, (train_idx, test_idx) in enumerate(self.folds):\\n            if fold_idx == fold: break\\n        else:\\n            # We have discovered no correct fold.\\n            raise ValueError(\\n                \\\"{} is not a fold, specify an integer less than {}\\\".format(\\n                    fold, self.folds.n_folds\\n                )\\n            )\\n\\n        # Now determine if we're in train or test mode.\\n        if not (test or train) or (test and train):\\n            raise ValueError(\\n                \\\"Please specify either train or test flag\\\"\\n            )\\n\\n        # Select only the indices to filter upon.\\n        indices = train_idx if train else test_idx\\n        return [\\n            fileid for doc_idx, fileid in enumerate(self.corpus.fileids())\\n            if doc_idx in indices\\n        ]\\n\\n    def labels(self, fold=None, train=False, test=False):\\n        \\\"\\\"\\\"\\n        Fit will load a list of the labels from the corpus categories.\\n\\n        If a fold is specified (should be an integer between 0 and folds),\\n        then the loader will return documents from that fold. Further, train\\n        or test must be specified to split the fold correctly.\\n        \\\"\\\"\\\"\\n        return [\\n            self.corpus.categories(fileids=fileid)[0]\\n            for fileid in self.fileids(fold, train, test)\\n        ]\\n\\n    def documents(self, fold=None, train=False, test=False):\\n        \\\"\\\"\\\"\\n        A generator of documents being streamed from disk. Each document is\\n        a list of paragraphs, which are a list of sentences, which in turn is\\n        a list of tuples of (token, tag) pairs. All preprocessing is done by\\n        NLTK and the CorpusReader object this object wraps.\\n\\n        If a fold is specified (should be an integer between 0 and folds),\\n        then the loader will return documents from that fold. Further, train\\n        or test must be specified to split the fold correctly. This method\\n        allows us to maintain the generator properties of document reads.\\n        \\\"\\\"\\\"\\n        for fileid in self.fileids(fold, train, test):\\n            yield list(self.corpus.tagged(fileids=fileid))\"}, {\"identifier\":\"build_model\", \"path\":\"corpus/learn.py\", \"snippet\":\"@timeit\\ndef build_model(loader, model, **kwargs):\\n    \\\"\\\"\\\"\\n    This function creates a pipeline from the feature extraction method in\\n    construct_pipeline and the passed in model and model keyword arguments,\\n    then trains the model with the given loader using all folds, then the\\n    complete dataset given by the loader object. It returns the fitted\\n    pipeline object along with scores and timing information.\\n    \\\"\\\"\\\"\\n\\n    # TODO: Add multiprocessing to parallelize build_inner_fold\\n    # TODO: Add verbosity to inform user on command line what is happening\\n    # TODO: Equip this method to be used by Celery workers\\n\\n    @timeit\\n    def build_inner_fold(loader, classifier, fold=None):\\n        \\\"\\\"\\\"\\n        A timed inner function that will return a set of evaluation scores\\n        if a fold is passed in, otherwise will build the model on the entire\\n        dataset and return the fitted model.\\n        \\\"\\\"\\\"\\n\\n        # Get the training data from the loader\\n        X_train = list(loader.documents(fold, train=True))\\n        y_train = list(loader.labels(fold, train=True))\\n\\n        # Construct the pipeline from the instantiated classifier\\n        model = construct_pipeline(classifier)\\n        model.fit(X_train, y_train)\\n\\n        # If folds is None, then return the fitted model.\\n        if fold is None: return model\\n\\n        # Otherwise get the test data from the fold to perform an evaluation.\\n        X_test  = list(loader.documents(fold, test=True))\\n        y_test  = list(loader.labels(fold, test=True))\\n        y_pred  = model.predict(X_test)\\n\\n        # Get the per-class scores as a well-structured object\\n        keys = ('precision', 'recall', 'f1', 'support')\\n        scores = precision_recall_fscore_support(y_test, y_pred, labels=model.classes_)\\n        scores = map(lambda s: dict(zip(model.classes_, s)), scores)\\n        scores = dict(zip(keys, scores))\\n\\n        # Get the weighted scores and add to the scores object\\n        weighted = precision_recall_fscore_support(y_test, y_pred, average='weighted', pos_label=None)\\n        for key, wscore in zip(keys, weighted):\\n            scores[key]['average'] = float(wscore) if wscore is not None else None\\n\\n        return scores\\n\\n\\n    # Now that the inner function works, let's run the model build process on\\n    # each fold for cross-validation and a final time to complete the model.\\n    scores = defaultdict(lambda: defaultdict(list))\\n    for fold in range(loader.n_folds):\\n\\n        classifier  = model(**kwargs)                            # Instantiate the classifier\\n        score, time = build_inner_fold(loader, classifier, fold) # Fit the model for this fold\\n\\n        # Update the scores as a list of scores for each run\\n        for name, values in score.items():\\n            for label, value in values.items():\\n                scores[name][label].append(value)\\n\\n        # Add the time to the scores listing\\n        scores['times']['folds'].append(time)\\n\\n    # Build the final model\\n    classifier = model(**kwargs)\\n    classifier, build_time = build_inner_fold(loader, classifier)\\n    scores['times']['final'] = build_time\\n\\n    # Return everything we've constructed (*whew)\\n    return classifier, scores\"}]", "import_statement": "import numpy as np\nfrom datetime import datetime\nfrom arbiter.models import Estimator, Score\nfrom django.contrib.auth.models import User\nfrom corpus.reader import TranscriptCorpusReader\nfrom corpus.models import Corpus, Document, LabeledDocument\nfrom corpus.reader import QueryCorpusReader, CorpusModelReader\nfrom corpus.learn import CorpusLoader, build_model\nfrom django.core.management.base import BaseCommand, CommandError\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.linear_model import LogisticRegression", "code": "        'nbayes': (MultinomialNB, {}),\n    }\n\n    # The minimum number of documents to train an estimator\n    min_docs = 12\n\n    def add_arguments(self, parser):\n        \"\"\"\n        Add command line argparse arguments.\n        \"\"\"\n        # Model selection argument\n        parser.add_argument(\n            '-m', '--model', choices=self.estimators, default='maxent',\n            help='specify the model form to fit on the given corpus',\n        )\n\n        # Number of folds for cross-validation\n        parser.add_argument(\n            '-f', '--folds', type=int, default=12,\n            help='number of folds to use in cross-validation',\n        )\n\n        # Optional ownership argument/build model for user\n        parser.add_argument(\n            '-u', '--username', default=None, metavar='NAME',\n            help='specify a user to build the model for or to assign ownership',\n        )\n\n        # Path on disk to build a corpus from transcripts\n        parser.add_argument(\n            '-t', '--transcripts', default=None, type=str, metavar='PATH',\n            help='specify a path on disk to the directory containing transcripts',\n        )\n\n        # Specify a corpus id to specifically build for\n        parser.add_argument(\n            '-c', '--corpus', type=int, default=None, metavar='ID',\n            help='specify the id of a corpus to build the model for',\n        )\n\n    def handle(self, *args, **options):\n        \"\"\"\n        Handles the model training process as follows:\n\n            1. If a transcript path is specified build that and assign to the\n               owner if given in the arguments (ignore other args)\n            2. If a corpus id is specified, build the model for that corpus and\n               assign to the owner if given in the arguments\n            3. If just a username is given, construct a user-specific corpus\n               and build a model for that corpus\n            4. If none of those arguments are given, construct a corpus that\n               utilizes the entire state of the current database, and build\n               a model for that corpus.\n\n        Note that items 1 and 2 do not create a corpus, whereas 3 and 4 do.\n        \"\"\"\n\n        # Get the owner from the options\n        owner  = self.get_user(options['username'])\n\n        # Create the reader from the options\n        if options['transcripts']:\n            # Get the transcripts reader\n            reader = TranscriptCorpusReader(options['transcripts'])\n            corpus = None\n            description = \"transcripts located at {}\".format(options['transcripts'])\n        else:\n            # Get or create the corpus object\n            reader, corpus = self.get_corpus(owner=owner, **options)\n            if corpus:\n                description = str(reader.corpus)\n            else:\n                description = \"Corpus read by {}\".format(\n                    reader.__class__.__name__\n                )\n\n        # Build the model from the corpus and owner.\n        estimator = self.build_model(reader, owner, description, **options)\n\n        # If corpus, assign it to the estimator and save\n        if corpus:\n            estimator.corpus = corpus\n            estimator.save()\n\n    def build_model(self, reader, owner, description, **options):\n        \"\"\"\n        Once the reader has been\n        \"\"\"\n        # Get the details from the command line arguments\n        model, kwargs = self.estimators[options['model']]\n\n        # Construct the loader from the passed in reader object.\n        loader = CorpusLoader(reader, options['folds'])\n\n        # Inform the user that the training process is beginning\n        self.stdout.write((\n            \"Starting training of {} {} models on {}\\n\"\n            \"This may take quite a bit of time, please be patient!\\n\"\n        ).format(\n            loader.n_folds + 1, model.__name__, description\n        ))\n\n        # GO! Build the model forever! Whooo!!!\n        (clf, scores), total_time = build_model(loader, model, **kwargs)\n\n        # Save the estimator model\n        estimator = Estimator.objects.create(\n            model_type  = Estimator.TYPES.classifier,\n            model_class = model.__name__,\n            model_form  = repr(clf),\n            estimator   = clf,\n            build_time  = total_time,\n            owner       = owner,\n        )\n\n        # Save the scores objects.\n        for metric, values in scores.items():\n\n            # Handle the time key in particular.\n            if metric == 'times':\n", "next_line": "                Score.objects.create(", "gold_snippet_index": 1, "id": 12, "__internal_uuid__": "6f2b978a-05db-47f5-8ff3-e95ba50e8d2d"}
{"repo_name": "steveniemitz/scales", "file_path": "scales/dispatch.py", "context": "[{\"identifier\":\"AsyncResult\", \"path\":\"scales/asynchronous.py\", \"snippet\":\"class AsyncResult(g_AsyncResult):\\n  @staticmethod\\n  def WhenAll(ars):\\n    \\\"\\\"\\\"Returns an AsyncResult representing the state of all AsyncResults passed.\\n\\n    Args:\\n      ars - An enumerable of AsyncResults.\\n    Returns:\\n      An AsyncResult representing the completion of all ars passed in.  When all\\n      complete, the AsyncResult will be set to an array of the results of each\\n      AsyncResult, in the order they were enumerated in.\\n      If any AsyncResult fails, the return result will fail.\\n    \\\"\\\"\\\"\\n\\n    ret = AsyncResult()\\n    num_ars = len(ars)\\n    total = [num_ars]\\n    results = [None] * num_ars\\n    def complete(_n, _ar):\\n      if _ar.exception:\\n        ret.set_exception(_ar.exception)\\n      elif not ret.ready():\\n        total[0] -= 1\\n        results[_n] = _ar.value\\n        if total[0] == 0:\\n          ret.set(results)\\n\\n    for n, ar in enumerate(ars):\\n      ar.rawlink(functools.partial(complete, n))\\n    return ret\\n\\n  @staticmethod\\n  def WhenAny(ars):\\n    \\\"\\\"\\\"Returns an AsyncResult representing the state of any AsyncResult passed in.\\n    The return value represents the state of the first AsyncResult to complete, or,\\n    if all fail, the last to fail.\\n\\n    Args:\\n      ars - An enumerable of AsyncResults.\\n    Returns:\\n      An AsyncResult representing the state of the first AsyncResult to complete.\\n      The AsyncResult's value will be set to the value of the first result to\\n      complete, or, if all fail, the exception thrown by the last to fail.\\n    \\\"\\\"\\\"\\n    ready_ars = [ar for ar in ars if ar.ready()]\\n    if ready_ars:\\n      return ready_ars[0]\\n\\n    ret = AsyncResult()\\n    total = [len(ars)]\\n    def complete(_ar):\\n      total[0] -= 1\\n      if total[0] == 0 and _ar.exception:\\n        ret.set_exception(_ar.exception)\\n      elif not ret.ready() and _ar.successful():\\n        ret.set(_ar.value)\\n\\n    for ar in ars:\\n      ar.rawlink(complete)\\n    return ret\\n\\n  @staticmethod\\n  def FromValue(val):\\n    if val is None:\\n      return AsyncResult.Complete()\\n    else:\\n      ar = AsyncResult()\\n      ar.set(val)\\n      return ar\\n\\n  @staticmethod\\n  def Complete():\\n    \\\"\\\"\\\"Return an AsyncResult that has completed.\\\"\\\"\\\"\\n    return _COMPLETE\\n\\n  @staticmethod\\n  def CompleteIn(n):\\n    \\\"\\\"\\\"Returns an AsyncResult that completes in <n> seconds\\n\\n    Args:\\n      n - The number of seconds to wait before completing.\\n    \\\"\\\"\\\"\\n    ar = AsyncResult()\\n    def helper():\\n      ar.set()\\n    g = Greenlet(helper)\\n    g.start_later(float(n))\\n    return ar\\n\\n  def _SafeLinkHelper(self, fn):\\n    try:\\n      self.set(fn())\\n    except:\\n      self.set_exception(sys.exc_info()[1])\\n\\n  def SafeLink(self, fn):\\n    \\\"\\\"\\\"Propagate the result of calling fn() on a new greenlet to ar\\n\\n    Args:\\n      ar - An AsyncResult.\\n      fn - The function to execute.\\n    \\\"\\\"\\\"\\n    gevent.spawn(self._SafeLinkHelper, fn)\\n\\n  def ContinueWith(self, fn, on_hub=True):\\n    cw_ar = AsyncResult()\\n    def continue_with_callback(_ar):\\n      def run():\\n        try:\\n          val = fn(_ar)\\n          cw_ar.set(val)\\n        except:\\n          cw_ar.set_exception(sys.exc_info()[1])\\n      if on_hub:\\n        run()\\n      else:\\n        gevent.spawn(run)\\n    self.rawlink(continue_with_callback)\\n    return cw_ar\\n\\n  def Map(self, fn):\\n    def mapper(_):\\n      if self.exception:\\n        return self\\n      else:\\n        return fn(self.value)\\n    return self.ContinueWith(mapper).Unwrap()\\n\\n  def _UnwrapHelper(self, target):\\n    if self.ready():\\n      # We're ready, propagate the result\\n      if self.exception:\\n        target.set_exception(self.exception)\\n      else:\\n        if isinstance(self.value, AsyncResult):\\n          self.value._UnwrapHelper(target)\\n        else:\\n          target.set(self.value)\\n    else:\\n      self.rawlink(\\n        functools.partial(AsyncResult._UnwrapHelper, target=target))\\n\\n  def Unwrap(self):\\n    unwrapped_ar = AsyncResult()\\n    self._UnwrapHelper(unwrapped_ar)\\n    return unwrapped_ar\\n\\n  @staticmethod\\n  def TryGet(val):\\n    if isinstance(val, AsyncResult):\\n      return val.get()\\n    else:\\n      return val\\n\\n  @staticmethod\\n  def Run(fn):\\n    ar = AsyncResult()\\n    ar.SafeLink(fn)\\n    return ar\\n\\n  @staticmethod\\n  def RunInline(fn):\\n    ar = AsyncResult()\\n    ar._SafeLinkHelper(fn)\\n    return ar\"}, {\"identifier\":\"MessageProperties\", \"path\":\"scales/constants.py\", \"snippet\":\"class MessageProperties(object):\\r\\n  Endpoint = '__Endpoint'\\r\"}, {\"identifier\":\"SinkProperties\", \"path\":\"scales/constants.py\", \"snippet\":\"class SinkProperties(object):\\r\\n  Endpoint = 'endpoint'\\r\\n  ServiceInterface = 'service_iface'\\r\\n  Label = 'label'\\r\"}, {\"identifier\":\"Deadline\", \"path\":\"scales/message.py\", \"snippet\":\"class Deadline(object):\\r\\n  KEY = \\\"__Deadline\\\"\\r\\n  EVENT_KEY = \\\"__Deadline_Event\\\"\\r\\n\\r\\n  def __init__(self, timeout):\\r\\n    \\\"\\\"\\\"\\r\\n    Args:\\r\\n      timeout - The timeout in seconds\\r\\n    \\\"\\\"\\\"\\r\\n    import  time\\r\\n    self._ts = Long(time.time()) * 1000000000 # Nanoseconds\\r\\n    self._timeout = Long(timeout * 1000000000)\\r\"}, {\"identifier\":\"MethodCallMessage\", \"path\":\"scales/message.py\", \"snippet\":\"class MethodCallMessage(Message):\\r\\n  \\\"\\\"\\\"A MethodCallMessage represents a method being invoked on a service.\\\"\\\"\\\"\\r\\n  __slots__ = ('service', 'method', 'args', 'kwargs')\\r\\n\\r\\n  def __init__(self, service, method, args, kwargs):\\r\\n    \\\"\\\"\\\"\\r\\n    Args:\\r\\n      service - The service this method call is intended for.\\r\\n      method - The method on the service.\\r\\n      args - The args passed to the method call.\\r\\n      kwargs-  The kwargs passed to the method call.\\r\\n    \\\"\\\"\\\"\\r\\n    super(MethodCallMessage, self).__init__()\\r\\n    self.service = service\\r\\n    self.method = method\\r\\n    self.args = args\\r\\n    self.kwargs = kwargs\\r\"}, {\"identifier\":\"MethodReturnMessage\", \"path\":\"scales/message.py\", \"snippet\":\"class MethodReturnMessage(Message):\\r\\n  \\\"\\\"\\\"A message representing the return value from a service call.\\\"\\\"\\\"\\r\\n  __slots__ = ('return_value', 'error', 'stack')\\r\\n\\r\\n  def __init__(self, return_value=None, error=None):\\r\\n    \\\"\\\"\\\"\\r\\n    Args:\\r\\n      return_value - The return value of the call, or None\\r\\n      error - The error that occurred during processing, or None.\\r\\n              If not None, the current stack will be captured and included.\\r\\n    \\\"\\\"\\\"\\r\\n    super(MethodReturnMessage, self).__init__()\\r\\n    self.return_value = return_value\\r\\n    self.error = error\\r\\n    if error:\\r\\n      exc_info = sys.exc_info()\\r\\n      if len(exc_info) != 3 or exc_info[2] is None:\\r\\n        try:\\r\\n          raise ZeroDivisionError\\r\\n        except ZeroDivisionError:\\r\\n          tb = sys.exc_info()[2]\\r\\n          frame = tb.tb_frame.f_back\\r\\n      else:\\r\\n        tb = exc_info[2]\\r\\n        while tb.tb_next is not None:\\r\\n          tb = tb.tb_next\\r\\n        frame = tb.tb_frame\\r\\n\\r\\n      stack = traceback.format_list(traceback.extract_stack(frame))\\r\\n      stack = stack + traceback.format_exception_only(error.__class__, error)\\r\\n      self.stack = stack\\r\\n      # Prevent circular references\\r\\n      del frame\\r\\n      del tb\\r\\n    else:\\r\\n      self.stack = None\\r\"}, {\"identifier\":\"TimeoutError\", \"path\":\"scales/message.py\", \"snippet\":\"class TimeoutError(Exception):\\r\\n  def __init__(self):\\r\\n    super(TimeoutError, self).__init__(\\r\\n      'The call did not complete within the specified timeout '\\r\\n      'and has been aborted.')\\r\"}, {\"identifier\":\"ClientMessageSink\", \"path\":\"scales/sink.py\", \"snippet\":\"class ClientMessageSink(MessageSink):\\n  \\\"\\\"\\\"ClientMessageSinks take a message, stream, and headers and perform\\n  processing on them.\\n  \\\"\\\"\\\"\\n  __slots__ = '_on_faulted',\\n  Role = None\\n  Builder = None\\n\\n  def __init__(self):\\n    self._on_faulted = Observable()\\n    super(ClientMessageSink, self).__init__()\\n\\n  @property\\n  def state(self):\\n    return self.next_sink.state\\n\\n  @property\\n  def is_open(self):\\n    \\\"\\\"\\\"Returns True if the sink is Idle, Open, or Busy\\\"\\\"\\\"\\n    return self.state <= ChannelState.Busy\\n\\n  @property\\n  def is_closed(self):\\n    \\\"\\\"\\\"Returns True if the sink is Closed.\\\"\\\"\\\"\\n    return self.state == ChannelState.Closed\\n\\n  @property\\n  def is_ready(self):\\n    \\\"\\\"\\\"Returns True if the channel is open, eg ready to process messages.\\\"\\\"\\\"\\n    return self.state == ChannelState.Open\\n\\n  @property\\n  def on_faulted(self):\\n    return self._on_faulted\\n\\n  def Open(self):\\n    if self.next_sink:\\n      return self.next_sink.Open()\\n    else:\\n      return AsyncResult.Complete()\\n\\n  def Close(self):\\n    if self.next_sink:\\n      self.next_sink.Close()\\n\\n  @abstractmethod\\n  def AsyncProcessRequest(self, sink_stack, msg, stream, headers):\\n    \\\"\\\"\\\"Process a request message, stream, and headers.\\n\\n    Args:\\n      sink_stack - The SinkStack representing the processing state of the message.\\n                   Implementors should push their sink onto this stack before\\n                   forwarding the message in order to participate in processing\\n                   the response.\\n      msg - The message being processed.\\n      stream - A serialized version of the message.\\n      headers - Any additional headers to be sent.\\n    \\\"\\\"\\\"\\n    raise NotImplementedError()\\n\\n  @abstractmethod\\n  def AsyncProcessResponse(self, sink_stack, context, stream, msg):\\n    \\\"\\\"\\\"Process a response stream.\\n\\n    Args:\\n      sink_stack - The SinkStack representing the processing state of the message.\\n                   Implementors should call sink_stack.AsyncProcessMessage(...)\\n                   to forward the message to the next sink.\\n      context - The context that was pushed onto the stack in AsyncProcessRequest.\\n      stream - The stream representing the serialized response.\\n    \\\"\\\"\\\"\\n    raise NotImplementedError()\"}, {\"identifier\":\"ClientMessageSinkStack\", \"path\":\"scales/sink.py\", \"snippet\":\"class ClientMessageSinkStack(SinkStack):\\n  \\\"\\\"\\\"A SinkStack of ClientMessageSinks.\\n\\n  The ClientMessageSinkStack forwards AsyncProcessResponse to the next sink\\n  on the stack.\\n  \\\"\\\"\\\"\\n\\n  def __init__(self):\\n    \\\"\\\"\\\"\\n    Args:\\n      reply_sink - An optional ReplySink.\\n    \\\"\\\"\\\"\\n    super(ClientMessageSinkStack, self).__init__()\\n\\n  def AsyncProcessResponse(self, stream, msg):\\n    \\\"\\\"\\\"Pop the next sink off the stack and call AsyncProcessResponse on it.\\\"\\\"\\\"\\n    if self.Any():\\n      next_sink, next_ctx = self.Pop()\\n      next_sink.AsyncProcessResponse(self, next_ctx, stream, msg)\\n\\n  def AsyncProcessResponseStream(self, stream):\\n    self.AsyncProcessResponse(stream, None)\\n\\n  def AsyncProcessResponseMessage(self, msg):\\n    self.AsyncProcessResponse(None, msg)\"}, {\"identifier\":\"Rate\", \"path\":\"scales/varz.py\", \"snippet\":\"class VarzType(object):\\nclass Source(object):\\nclass VarzMetric(object):\\nclass Gauge(VarzMetric): VARZ_TYPE = VarzType.Gauge\\nclass Rate(VarzMetric): VARZ_TYPE = VarzType.Rate\\nclass AverageRate(VarzMetric): VARZ_TYPE = VarzType.AverageRate\\nclass Counter(Rate): VARZ_TYPE = VarzType.Counter\\nclass VarzTimerBase(VarzMetric):\\nclass AverageTimer(VarzTimerBase): VARZ_TYPE = VarzType.AverageTimer\\nclass AggregateTimer(VarzTimerBase): VARZ_TYPE = VarzType.AggregateTimer\\nclass VarzMeta(type):\\nclass _VarzBase(object):\\nclass _SampleSet(object):\\nclass VarzReceiver(object):\\nclass VarzAggregator(object):\\n  class _Agg(object):\\nclass VarzSocketWrapper(object):\\n  class Varz(VarzBase):\\nclass MonoClock(object):\\nclass Ema(object):\\n  def __init__(self, method=None, service=None, endpoint=None, client_id=None):\\n  def to_tuple(self):\\n  def to_dict(self):\\n  def __cmp__(self, other):\\n  def __hash__(self):\\n  def _Adapt(fn):\\n    def __Adapt(metric, source, amount=1):\\n  def __init__(self, metric, source):\\n  def __call__(self, *args):\\n  def ForSource(self, source):\\n  def Measure(self, source=None):\\n  def __new__(mcs, name, bases, dct):\\ndef VerifySource(source):\\n  def __init__(self, source):\\n  def __getattr__(self, item):\\n  def __init__(self, max_size, data=None, p=.1):\\n  def Sample(self, value):\\n  def RegisterMetric(metric, varz_type):\\n  def IncrementVarz(source, metric, amount=1):\\n  def SetVarz(source, metric, value):\\n  def RecordPercentileSample(cls, source, metric, value):\\ndef DefaultKeySelector(k):\\n    def __init__(self):\\n  def CalculatePercentile(values, pct):\\n  def _Downsample(lst, target_size):\\n  def Aggregate(varz, metrics, key_selector=None):\\n  def __init__(self, socket, varz_tag):\\n  def host(self):\\n  def port(self):\\n  def isOpen(self):\\n  def read(self, sz):\\n  def recv_into(self, buf, sz):\\n  def flush(self):\\n  def write(self, buff):\\n  def open(self):\\n  def close(self):\\n  def readAll(self, sz):\\n  def __init__(self):\\n  def Sample(self):\\n  def __init__(self, window):\\n  def Update(self, ts, sample):\\n  VARZ_TYPE = None\\nclass Gauge(VarzMetric): VARZ_TYPE = VarzType.Gauge\\nclass Rate(VarzMetric): VARZ_TYPE = VarzType.Rate\\nclass AverageRate(VarzMetric): VARZ_TYPE = VarzType.AverageRate\\nclass Counter(Rate): VARZ_TYPE = VarzType.Counter\\nclass AverageTimer(VarzTimerBase): VARZ_TYPE = VarzType.AverageTimer\\nclass AggregateTimer(VarzTimerBase): VARZ_TYPE = VarzType.AggregateTimer\\n  _VARZ = {}\\n  _VARZ_BASE_NAME = None\\n  VARZ_METRICS = {}\\n  VARZ_DATA = defaultdict(lambda: defaultdict(int))\\n  VARZ_PERCENTILES = [.5, .90, .99, .999, .9999]\\n  _MAX_PERCENTILE_SIZE = 1000\\n  MAX_AGG_AGE = 5 * 60\\n    _VARZ_BASE_NAME = 'scales.socket'\\n    _VARZ = {\\n      'bytes_recv': Rate,\\n      'bytes_sent': Rate,\\n      'num_connections': Counter,\\n      'tests_failed': Counter,\\n      'connects': Rate,\\n      'open_latency': AverageTimer\\n    }\"}]", "import_statement": "import time\r\nimport gevent\r\nfrom .asynchronous import AsyncResult\r\nfrom .constants import MessageProperties, SinkProperties\r\nfrom .message import (\r\n  Deadline,\r\n  MethodCallMessage,\r\n  MethodReturnMessage,\r\n  TimeoutError\r\n)\r\nfrom .sink import (\r\n  ClientMessageSink,\r\n  ClientMessageSinkStack\r\n)\r\nfrom .varz import (\r\n  Rate,\r\n  Source,\r\n  AverageTimer,\r\n  VarzBase\r\n)\r", "code": "\"\"\"Core classes for dispatching messages from a Scales proxy to a message sink stack.\"\"\"\r\n\r\n\r\n\r\n\r\n\r\nclass InternalError(Exception):\r\n  pass\r\n\r\n\r\nclass ScalesError(Exception):\r\n  def __init__(self, ex, msg):\r\n    self.inner_exception = ex\r\n    super(ScalesError, self).__init__(msg)\r\n\r\n\r\nclass ServiceClosedError(Exception):\r\n  pass\r\n\r\n\r\nclass _AsyncResponseSink(ClientMessageSink):\r\n  @staticmethod\r\n  def _WrapException(msg):\r\n    \"\"\"Creates an exception object that contains the inner exception\r\n    from a SystemErrorMessage.  This allows the actual failure stack\r\n    to propagate to the waiting greenlet.\r\n\r\n    Args:\r\n      msg - The MethodReturnMessage that has an active error.\r\n\r\n    Returns:\r\n      An exception object wrapping the error in the MethodCallMessage.\r\n    \"\"\"\r\n    # Don't wrap timeouts.\r\n    if isinstance(msg.error, TimeoutError):\r\n      return msg.error\r\n\r\n    stack = getattr(msg, 'stack', None)\r\n    if stack:\r\n      ex_msg = \"\"\"An error occurred while processing the request\r\n[Inner Exception: --------------------]\r\n%s[End of Inner Exception---------------]\r\n\"\"\" % ''.join(stack)\r\n      return ScalesError(msg.error, ex_msg)\r\n    else:\r\n      return msg.error\r\n\r\n  def AsyncProcessRequest(self, sink_stack, msg, stream, headers):\r\n    raise NotImplementedError(\"This should never be called\")\r\n\r\n  def AsyncProcessResponse(self, sink_stack, context, stream, msg):\r\n    \"\"\"Propagate the results from a message onto an AsyncResult.\r\n\r\n    Args:\r\n      msg - The reply message (a MethodReturnMessage).\r\n    \"\"\"\r\n    source, start_time, ar, msg_props = context\r\n", "next_line": "    endpoint = msg_props.get(MessageProperties.Endpoint, None)\r", "gold_snippet_index": 1, "id": 13, "__internal_uuid__": "39e8aebc-bc15-4894-b7f5-5fc8c3bc23c2"}
{"repo_name": "ratoaq2/knowit", "file_path": "knowit/providers/mkvmerge.py", "context": "[{\"identifier\":\"Property\", \"path\":\"knowit/core.py\", \"snippet\":\"class Property(Reportable[T]):\\n    \\\"\\\"\\\"Property class.\\\"\\\"\\\"\\n\\n    def __init__(\\n            self,\\n            *args: str,\\n            default: typing.Optional[T] = None,\\n            private: bool = False,\\n            description: typing.Optional[str] = None,\\n            delimiter: str = ' / ',\\n            **kwargs,\\n    ):\\n        \\\"\\\"\\\"Init method.\\\"\\\"\\\"\\n        super().__init__(*args, description=description, **kwargs)\\n        self.default = default\\n        self.private = private\\n        # Used to detect duplicated values. e.g.: en / en or High@L4.0 / High@L4.0 or Progressive / Progressive\\n        self.delimiter = delimiter\\n\\n    def extract_value(\\n            self,\\n            track: typing.Mapping,\\n            context: typing.MutableMapping,\\n    ) -> typing.Optional[T]:\\n        \\\"\\\"\\\"Extract the property value from a given track.\\\"\\\"\\\"\\n        for name in self.names:\\n            names = name.split('.')\\n            value = track.get(names[0], {}).get(names[1]) if len(names) == 2 else track.get(name)\\n            if value is None:\\n                if self.default is None:\\n                    continue\\n\\n                value = self.default\\n\\n            if isinstance(value, bytes):\\n                value = value.decode()\\n\\n            if isinstance(value, str):\\n                value = value.translate(_visible_chars_table).strip()\\n                if _is_unknown(value):\\n                    continue\\n                value = self._deduplicate(value)\\n\\n            result = self.handle(value, context)\\n            if result is not None and not _is_unknown(result):\\n                return result\\n\\n        return None\\n\\n    @classmethod\\n    def _deduplicate(cls, value: str) -> str:\\n        values = value.split(' / ')\\n        if len(values) == 2 and values[0] == values[1]:\\n            return values[0]\\n        return value\\n\\n    def handle(self, value: T, context: typing.MutableMapping) -> typing.Optional[T]:\\n        \\\"\\\"\\\"Return the value without any modification.\\\"\\\"\\\"\\n        return value\"}, {\"identifier\":\"AudioCodec\", \"path\":\"knowit/properties/audio.py\", \"snippet\":\"class AudioCodec(Configurable[str]):\\n    \\\"\\\"\\\"Audio codec property.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def _extract_key(cls, value) -> str:\\n        key = str(value).upper()\\n        if key.startswith('A_'):\\n            key = key[2:]\\n\\n        # only the first part of the word. E.g.: 'AAC LC' => 'AAC'\\n        return key.split(' ')[0]\\n\\n    @classmethod\\n    def _extract_fallback_key(cls, value, key) -> typing.Optional[str]:\\n        if '/' in key:\\n            return key.split('/')[0]\\n        else:\\n            return None\"}, {\"identifier\":\"Basic\", \"path\":\"knowit/properties/general.py\", \"snippet\":\"class Basic(Property[T]):\\n    \\\"\\\"\\\"Basic property to handle int, Decimal and other basic types.\\\"\\\"\\\"\\n\\n    def __init__(self, *args: str, data_type: typing.Type,\\n                 processor: typing.Optional[typing.Callable[[T], T]] = None,\\n                 allow_fallback: bool = False, **kwargs):\\n        \\\"\\\"\\\"Init method.\\\"\\\"\\\"\\n        super().__init__(*args, **kwargs)\\n        self.data_type = data_type\\n        self.processor = processor or (lambda x: x)\\n        self.allow_fallback = allow_fallback\\n\\n    def handle(self, value, context: typing.MutableMapping):\\n        \\\"\\\"\\\"Handle value.\\\"\\\"\\\"\\n        if isinstance(value, self.data_type):\\n            return self.processor(value)\\n\\n        try:\\n            return self.processor(self.data_type(value))\\n        except ValueError:\\n            if not self.allow_fallback:\\n                self.report(value, context)\"}, {\"identifier\":\"Duration\", \"path\":\"knowit/properties/general.py\", \"snippet\":\"class Duration(Property[timedelta]):\\n    \\\"\\\"\\\"Duration property.\\\"\\\"\\\"\\n\\n    duration_re = re.compile(r'(?P<hours>\\\\d{1,2}):'\\n                             r'(?P<minutes>\\\\d{1,2}):'\\n                             r'(?P<seconds>\\\\d{1,2})(?:\\\\.'\\n                             r'(?P<milliseconds>\\\\d{3})'\\n                             r'(?P<microseconds>\\\\d{3})?\\\\d*)?')\\n\\n    def __init__(self, *args: str, resolution: typing.Union[int, Decimal] = 1, **kwargs):\\n        \\\"\\\"\\\"Initialize a Duration.\\\"\\\"\\\"\\n        super().__init__(*args, **kwargs)\\n        self.resolution = resolution\\n\\n    def handle(self, value, context: typing.MutableMapping):\\n        \\\"\\\"\\\"Return duration as timedelta.\\\"\\\"\\\"\\n        if isinstance(value, timedelta):\\n            return value\\n        elif isinstance(value, int):\\n            return timedelta(milliseconds=int(value * self.resolution))\\n        try:\\n            return timedelta(\\n                milliseconds=int(Decimal(value) * self.resolution))\\n        except (ValueError, InvalidOperation):\\n            pass\\n\\n        match = self.duration_re.match(value)\\n        if not match:\\n            self.report(value, context)\\n            return None\\n\\n        params = {\\n            key: int(value)\\n            for key, value in match.groupdict().items()\\n            if value\\n        }\\n        return timedelta(**params)\"}, {\"identifier\":\"Language\", \"path\":\"knowit/properties/general.py\", \"snippet\":\"class Language(Property[babelfish.Language]):\\n    \\\"\\\"\\\"Language property.\\\"\\\"\\\"\\n\\n    def handle(self, value, context: typing.MutableMapping):\\n        \\\"\\\"\\\"Handle languages.\\\"\\\"\\\"\\n        try:\\n            if len(value) == 3:\\n                return babelfish.Language.fromalpha3b(value)\\n\\n            return babelfish.Language.fromietf(value)\\n        except (babelfish.Error, ValueError):\\n            pass\\n\\n        try:\\n            return babelfish.Language.fromname(value)\\n        except babelfish.Error:\\n            pass\\n\\n        self.report(value, context)\\n        return babelfish.Language('und')\"}, {\"identifier\":\"Quantity\", \"path\":\"knowit/properties/general.py\", \"snippet\":\"class Quantity(Property):\\n    \\\"\\\"\\\"Quantity is a property with unit.\\\"\\\"\\\"\\n\\n    def __init__(self, *args: str, unit, data_type=int, **kwargs):\\n        \\\"\\\"\\\"Init method.\\\"\\\"\\\"\\n        super().__init__(*args, **kwargs)\\n        self.unit = unit\\n        self.data_type = data_type\\n\\n    def handle(self, value, context):\\n        \\\"\\\"\\\"Handle value with unit.\\\"\\\"\\\"\\n        if not isinstance(value, self.data_type):\\n            try:\\n                value = self.data_type(value)\\n            except ValueError:\\n                self.report(value, context)\\n                return\\n        if isinstance(value, Decimal):\\n            value = round_decimal(value, min_digits=1, max_digits=3)\\n\\n        return value if context.get('no_units') else value * self.unit\"}, {\"identifier\":\"YesNo\", \"path\":\"knowit/properties/general.py\", \"snippet\":\"class YesNo(Configurable[str]):\\n    \\\"\\\"\\\"Yes or No handler.\\\"\\\"\\\"\\n\\n    yes_values = ('yes', 'true', '1')\\n\\n    def __init__(self, *args: str, yes=True, no=False, hide_value=None,\\n                 config: typing.Optional[\\n                     typing.Mapping[str, typing.Mapping]] = None,\\n                 config_key: typing.Optional[str] = None,\\n                 **kwargs):\\n        \\\"\\\"\\\"Init method.\\\"\\\"\\\"\\n        super().__init__(config or {}, config_key=config_key, *args, **kwargs)\\n        self.yes = yes\\n        self.no = no\\n        self.hide_value = hide_value\\n\\n    def handle(self, value, context):\\n        \\\"\\\"\\\"Handle boolean values.\\\"\\\"\\\"\\n        result = self.yes if str(value).lower() in self.yes_values else self.no\\n        if result == self.hide_value:\\n            return None\\n\\n        return super().handle(result, context) if self.mapping else result\"}, {\"identifier\":\"VideoCodec\", \"path\":\"knowit/properties/video.py\", \"snippet\":\"class VideoCodec(Configurable[str]):\\n    \\\"\\\"\\\"Video Codec handler.\\\"\\\"\\\"\\n\\n    @classmethod\\n    def _extract_key(cls, value) -> str:\\n        key = value.upper().split('/')[-1]\\n        if key.startswith('V_'):\\n            key = key[2:]\\n\\n        return key.split(' ')[-1]\"}, {\"identifier\":\"VideoDimensions\", \"path\":\"knowit/properties/video.py\", \"snippet\":\"class VideoDimensions(Property[int]):\\n    \\\"\\\"\\\"Dimensions property.\\\"\\\"\\\"\\n\\n    def __init__(self, *args: str, dimension='width' or 'height', **kwargs):\\n        \\\"\\\"\\\"Initialize the object.\\\"\\\"\\\"\\n        super().__init__(*args, **kwargs)\\n        self.dimension = dimension\\n\\n    dimensions_re = re.compile(r'(?P<width>\\\\d+)x(?P<height>\\\\d+)')\\n\\n    def handle(self, value, context) -> typing.Optional[int]:\\n        \\\"\\\"\\\"Handle ratio.\\\"\\\"\\\"\\n        match = self.dimensions_re.match(value)\\n        if match:\\n            match_dict = match.groupdict()\\n            try:\\n                value = match_dict[self.dimension]\\n            except KeyError:\\n                pass\\n            else:\\n                return int(value)\\n\\n        self.report(value, context)\\n        return None\"}, {\"identifier\":\"MalformedFileError\", \"path\":\"knowit/provider.py\", \"snippet\":\"class MalformedFileError(ProviderError):\\n    \\\"\\\"\\\"Malformed File error.\\\"\\\"\\\"\\n\\n    pass\"}, {\"identifier\":\"Provider\", \"path\":\"knowit/provider.py\", \"snippet\":\"class Provider:\\n    \\\"\\\"\\\"Base class for all providers.\\\"\\\"\\\"\\n\\n    min_fps = 10\\n    max_fps = 200\\n\\n    def __init__(\\n            self,\\n            config: knowit.config.Config,\\n            mapping: PropertyConfig,\\n            rules: typing.Optional[RuleConfig] = None,\\n    ):\\n        \\\"\\\"\\\"Init method.\\\"\\\"\\\"\\n        self.config = config\\n        self.mapping = mapping\\n        self.rules = rules or {}\\n\\n    def accepts(self, target):\\n        \\\"\\\"\\\"Whether or not the video is supported by this provider.\\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    def describe(self, target, context):\\n        \\\"\\\"\\\"Read video metadata information.\\\"\\\"\\\"\\n        raise NotImplementedError\\n\\n    def _describe_tracks(self, video_path, general_track, video_tracks, audio_tracks, subtitle_tracks, context):\\n        logger.debug('Handling general track')\\n        props = self._describe_track(general_track, 'general', context)\\n\\n        if 'path' not in props:\\n            props['path'] = video_path\\n        if 'container' not in props:\\n            props['container'] = os.path.splitext(video_path)[1][1:]\\n        if 'size' not in props and os.path.isfile(video_path):\\n            props['size'] = size_property.handle(os.path.getsize(video_path), context)\\n\\n        for track_type, tracks, in (('video', video_tracks),\\n                                    ('audio', audio_tracks),\\n                                    ('subtitle', subtitle_tracks)):\\n            results = []\\n            for track in tracks or []:\\n                logger.debug('Handling %s track', track_type)\\n                t = self._validate_track(track_type, self._describe_track(track, track_type, context))\\n                if t:\\n                    results.append(t)\\n\\n            if results:\\n                props[track_type] = results\\n\\n        return props\\n\\n    @classmethod\\n    def _validate_track(cls, track_type, track):\\n        if track_type != 'video' or 'frame_rate' not in track:\\n            return track\\n\\n        frame_rate = track['frame_rate']\\n        try:\\n            frame_rate = frame_rate.magnitude\\n        except AttributeError:\\n            pass\\n\\n        if cls.min_fps < frame_rate < cls.max_fps:\\n            return track\\n\\n    def _describe_track(self, track, track_type, context):\\n        \\\"\\\"\\\"Describe track to a dict.\\n\\n        :param track:\\n        :param track_type:\\n        :rtype: dict\\n        \\\"\\\"\\\"\\n        props = {}\\n        pv_props = {}\\n        for name, prop in self.mapping[track_type].items():\\n            if not prop:\\n                # placeholder to be populated by rules. It keeps the order\\n                props[name] = None\\n                continue\\n\\n            value = prop.extract_value(track, context)\\n            if value is not None:\\n                if not prop.private:\\n                    which = props\\n                else:\\n                    which = pv_props\\n                which[name] = value\\n\\n        for name, rule in self.rules.get(track_type, {}).items():\\n            if props.get(name) is not None and not rule.override:\\n                logger.debug('Skipping rule %s since property is already present: %r', name, props[name])\\n                continue\\n\\n            value = rule.execute(props, pv_props, context)\\n            if value is not None:\\n                props[name] = value\\n            elif name in props and not rule.override:\\n                del props[name]\\n\\n        return props\\n\\n    @property\\n    def version(self):\\n        \\\"\\\"\\\"Return provider version information.\\\"\\\"\\\"\\n        raise NotImplementedError\"}, {\"identifier\":\"AudioChannelsRule\", \"path\":\"knowit/rules/audio.py\", \"snippet\":\"class AudioChannelsRule(Rule):\\n    \\\"\\\"\\\"Audio Channel rule.\\\"\\\"\\\"\\n\\n    mapping = {\\n        1: '1.0',\\n        2: '2.0',\\n        6: '5.1',\\n        8: '7.1',\\n    }\\n\\n    def execute(self, props, pv_props, context):\\n        \\\"\\\"\\\"Execute the rule against properties.\\\"\\\"\\\"\\n        count = props.get('channels_count')\\n        if count is None:\\n            return\\n\\n        channels = self.mapping.get(count) if isinstance(count, int) else None\\n        positions = pv_props.get('channel_positions') or []\\n        positions = positions if isinstance(positions, list) else [positions]\\n        candidate = 0\\n        for position in positions:\\n            if not position:\\n                continue\\n\\n            c = Decimal('0.0')\\n            for i in position.split('/'):\\n                try:\\n                    c += Decimal(i)\\n                except ValueError:\\n                    logger.debug('Invalid %s: %s', self.description, i)\\n                    pass\\n\\n            c_count = int(c) + int(round((c - int(c)) * 10))\\n            if c_count == count:\\n                return str(c)\\n\\n            candidate = max(candidate, c)\\n\\n        if channels:\\n            return channels\\n\\n        if candidate:\\n            return candidate\\n\\n        self.report(positions, context)\"}, {\"identifier\":\"LanguageRule\", \"path\":\"knowit/rules/general.py\", \"snippet\":\"class LanguageRule(Rule):\\n    \\\"\\\"\\\"Language rules.\\\"\\\"\\\"\\n\\n    name_re = re.compile(r'(?P<name>\\\\w+)\\\\b', re.IGNORECASE)\\n\\n    def execute(self, props, pv_props, context):\\n        \\\"\\\"\\\"Language detection using name.\\\"\\\"\\\"\\n        if 'language' in props:\\n            return\\n\\n        if 'name' in props:\\n            name = props.get('name', '')\\n            match = self.name_re.match(name)\\n            if match:\\n                try:\\n                    return babelfish.Language.fromname(match.group('name'))\\n                except babelfish.Error:\\n                    pass\\n            logger.info('Invalid %s: %r', self.description, name)\"}, {\"identifier\":\"ClosedCaptionRule\", \"path\":\"knowit/rules/subtitle.py\", \"snippet\":\"class ClosedCaptionRule(Rule):\\n    \\\"\\\"\\\"Closed caption rule.\\\"\\\"\\\"\\n\\n    cc_re = re.compile(r'(\\\\bcc\\\\d\\\\b)', re.IGNORECASE)\\n\\n    def execute(self, props, pv_props, context):\\n        \\\"\\\"\\\"Execute closed caption rule.\\\"\\\"\\\"\\n        for name in (pv_props.get('_closed_caption'), props.get('name')):\\n            if name and self.cc_re.search(name):\\n                return True\"}, {\"identifier\":\"HearingImpairedRule\", \"path\":\"knowit/rules/subtitle.py\", \"snippet\":\"class HearingImpairedRule(Rule):\\n    \\\"\\\"\\\"Hearing Impaired rule.\\\"\\\"\\\"\\n\\n    hi_re = re.compile(r'(\\\\bsdh\\\\b)', re.IGNORECASE)\\n\\n    def execute(self, props, pv_props, context):\\n        \\\"\\\"\\\"Hearing Impaired.\\\"\\\"\\\"\\n        name = props.get('name')\\n        if name and self.hi_re.search(name):\\n            return True\"}, {\"identifier\":\"ResolutionRule\", \"path\":\"knowit/rules/video.py\", \"snippet\":\"class ResolutionRule(Rule):\\n    \\\"\\\"\\\"Resolution rule.\\\"\\\"\\\"\\n\\n    standard_resolutions = (\\n        480,\\n        720,\\n        1080,\\n        2160,\\n        4320,\\n    )\\n    uncommon_resolutions = (\\n        240,\\n        288,\\n        360,\\n        576,\\n    )\\n    resolutions = list(sorted(standard_resolutions + uncommon_resolutions))\\n    square = 4. / 3\\n    wide = 16. / 9\\n\\n    def execute(self, props, pv_props, context):\\n        \\\"\\\"\\\"Return the resolution for the video.\\n\\n        The resolution is based on a widescreen TV (16:9)\\n        1920x800 will be considered 1080p since the TV will use 1920x1080 with vertical black bars\\n        1426x1080 is considered 1080p since the TV will use 1920x1080 with horizontal black bars\\n\\n        The calculation considers the display aspect ratio and the pixel aspect ratio (not only width and height).\\n        The upper resolution is selected if there's no perfect match with the following list of resolutions:\\n            240, 288, 360, 480, 576, 720, 1080, 2160, 4320\\n        If no interlaced information is available, resolution will be considered Progressive.\\n        \\\"\\\"\\\"\\n        width = props.get('width')\\n        height = props.get('height')\\n        if not width or not height:\\n            return\\n\\n        try:\\n            width = width.magnitude\\n            height = height.magnitude\\n        except AttributeError:\\n            pass\\n\\n        dar = props.get('aspect_ratio', Decimal(width) / height)\\n        par = props.get('pixel_aspect_ratio', 1)\\n        scan_type = props.get('scan_type', 'p')[0].lower()\\n\\n        # selected DAR must be between 4:3 and 16:9\\n        selected_dar = max(min(dar, self.wide), self.square)\\n\\n        # mod-16\\n        stretched_width = int(round(width * par / 16)) * 16\\n\\n        # mod-8\\n        calculated_height = int(round(stretched_width / selected_dar / 8)) * 8\\n\\n        selected_resolution = None\\n        for r in reversed(self.resolutions):\\n            if r < calculated_height:\\n                break\\n\\n            selected_resolution = r\\n\\n        if selected_resolution:\\n            return f'{selected_resolution}{scan_type}'\\n\\n        msg = f'{width}x{height} - scan_type: {scan_type}, aspect_ratio: {dar}, pixel_aspect_ratio: {par}'\\n        self.report(msg, context)\"}, {\"identifier\":\"get_json_encoder\", \"path\":\"knowit/serializer.py\", \"snippet\":\"def get_json_encoder(context):\\n    \\\"\\\"\\\"Return json encoder that handles all needed object types.\\\"\\\"\\\"\\n    class StringEncoder(json.JSONEncoder):\\n        \\\"\\\"\\\"String json encoder.\\\"\\\"\\\"\\n\\n        def default(self, o):\\n            return format_property(context['profile'], o)\\n\\n    return StringEncoder\"}, {\"identifier\":\"units\", \"path\":\"knowit/units.py\", \"snippet\":\"class NullRegistry:\\n    def __init__(self):\\n    def __getattr__(self, item: typing.Any) -> int:\\n    def __bool__(self):\\n    def define(self, *args, **kwargs):\\ndef _build_unit_registry():\"}, {\"identifier\":\"define_candidate\", \"path\":\"knowit/utils.py\", \"snippet\":\"def define_candidate(\\n        locations: OPTION_MAP,\\n        names: OPTION_MAP,\\n        os_family: typing.Optional[OS_FAMILY] = None,\\n        suggested_path: typing.Optional[str] = None,\\n) -> typing.Generator[str, None, None]:\\n    \\\"\\\"\\\"Select family-specific options and generate possible candidates.\\\"\\\"\\\"\\n    os_family = os_family or detect_os()\\n    family_names = names[os_family]\\n    all_locations = (suggested_path, ) + locations[os_family]\\n    yield from build_candidates(all_locations, family_names)\"}, {\"identifier\":\"detect_os\", \"path\":\"knowit/utils.py\", \"snippet\":\"def detect_os() -> OS_FAMILY:\\n    \\\"\\\"\\\"Detect os family: windows, macos or unix.\\\"\\\"\\\"\\n    if os.name in ('nt', 'dos', 'os2', 'ce'):\\n        return 'windows'\\n    if sys.platform == 'darwin':\\n        return 'macos'\\n    return 'unix'\"}]", "import_statement": "import json\nimport logging\nimport re\nfrom decimal import Decimal\nfrom logging import NullHandler, getLogger\nfrom subprocess import check_output\nfrom knowit.core import Property\nfrom knowit.properties import (\n    AudioCodec,\n    Basic,\n    Duration,\n    Language,\n    Quantity,\n    VideoCodec,\n    VideoDimensions,\n    YesNo,\n)\nfrom knowit.provider import (\n    MalformedFileError,\n    Provider,\n)\nfrom knowit.rules import (\n    AudioChannelsRule,\n    ClosedCaptionRule,\n    HearingImpairedRule,\n    LanguageRule,\n    ResolutionRule,\n)\nfrom knowit.serializer import get_json_encoder\nfrom knowit.units import units\nfrom knowit.utils import define_candidate, detect_os", "code": "\n\n\nlogger = getLogger(__name__)\nlogger.addHandler(NullHandler())\n\nWARN_MSG = r'''\n=========================================================================================\nmkvmerge not found on your system or could not be loaded.\nVisit https://mkvtoolnix.download to download it.\nIf you still have problems, please check if the downloaded version matches your system.\nTo load mkvmerge from a specific location, please define the location as follow:\n  knowit --mkvmerge /usr/local/mkvmerge/bin <video_path>\n  knowit --mkvmerge /usr/local/mkvmerge/bin/ffprobe <video_path>\n  knowit --mkvmerge \"C:\\Program Files\\mkvmerge\" <video_path>\n  knowit --mkvmerge C:\\Software\\mkvmerge.exe <video_path>\n=========================================================================================\n'''\n\n\nclass MkvMergeExecutor:\n    \"\"\"Executor that knows how to execute mkvmerge.\"\"\"\n\n    version_re = re.compile(r'\\bv(?P<version>[^\\b\\s]+)')\n    locations = {\n        'unix': ('/usr/local/mkvmerge/lib', '/usr/local/mkvmerge/bin', '__PATH__'),\n        'windows': ('__PATH__', ),\n        'macos': ('__PATH__', ),\n    }\n\n    def __init__(self, location, version):\n        \"\"\"Initialize the object.\"\"\"\n        self.location = location\n        self.version = version\n\n    def extract_info(self, filename):\n        \"\"\"Extract media info.\"\"\"\n        json_dump = self._execute(filename)\n        return json.loads(json_dump)\n\n    def _execute(self, filename):\n        raise NotImplementedError\n\n    @classmethod\n    def _get_version(cls, output):\n        match = cls.version_re.search(output)\n        if match:\n            version = match.groupdict()['version']\n            return version\n\n    @classmethod\n    def get_executor_instance(cls, suggested_path=None):\n        \"\"\"Return executor instance.\"\"\"\n        os_family = detect_os()\n        logger.debug('Detected os: %s', os_family)\n        for exec_cls in (MkvMergeCliExecutor, ):\n            executor = exec_cls.create(os_family, suggested_path)\n            if executor:\n                return executor\n\n\nclass MkvMergeCliExecutor(MkvMergeExecutor):\n    \"\"\"Executor that uses mkvmerge cli.\"\"\"\n\n    names = {\n        'unix': ('mkvmerge', ),\n        'windows': ('mkvmerge.exe', ),\n        'macos': ('mkvmerge', ),\n    }\n\n    def _execute(self, filename):\n        return check_output([self.location, '-i', '-F', 'json', filename]).decode()\n\n    @classmethod\n    def create(cls, os_family=None, suggested_path=None):\n        \"\"\"Create the executor instance.\"\"\"\n        for candidate in define_candidate(cls.locations, cls.names, os_family, suggested_path):\n            try:\n                output = check_output([candidate, '--version']).decode()\n                version = cls._get_version(output)\n                if version:\n                    logger.debug('MkvMerge cli detected: %s v%s', candidate, version)\n                    return MkvMergeCliExecutor(candidate, version.split('.'))\n            except OSError:\n                pass\n\n\nclass MkvMergeProvider(Provider):\n    \"\"\"MkvMerge Provider.\"\"\"\n\n    def __init__(self, config, suggested_path=None, *args, **kwargs):\n        \"\"\"Init method.\"\"\"\n        super().__init__(config, {\n            'general': {\n                'title': Property('title', description='media title'),\n", "next_line": "                'duration': Duration('duration', resolution=Decimal('0.000001'), description='media duration'),", "gold_snippet_index": 3, "id": 14, "__internal_uuid__": "204d5f72-36a2-4559-bd3b-16a5a76e8e35"}
{"repo_name": "steveniemitz/scales", "file_path": "scales/thrift/sink.py", "context": "[{\"identifier\":\"AsyncResult\", \"path\":\"scales/asynchronous.py\", \"snippet\":\"class AsyncResult(g_AsyncResult):\\n  @staticmethod\\n  def WhenAll(ars):\\n    \\\"\\\"\\\"Returns an AsyncResult representing the state of all AsyncResults passed.\\n\\n    Args:\\n      ars - An enumerable of AsyncResults.\\n    Returns:\\n      An AsyncResult representing the completion of all ars passed in.  When all\\n      complete, the AsyncResult will be set to an array of the results of each\\n      AsyncResult, in the order they were enumerated in.\\n      If any AsyncResult fails, the return result will fail.\\n    \\\"\\\"\\\"\\n\\n    ret = AsyncResult()\\n    num_ars = len(ars)\\n    total = [num_ars]\\n    results = [None] * num_ars\\n    def complete(_n, _ar):\\n      if _ar.exception:\\n        ret.set_exception(_ar.exception)\\n      elif not ret.ready():\\n        total[0] -= 1\\n        results[_n] = _ar.value\\n        if total[0] == 0:\\n          ret.set(results)\\n\\n    for n, ar in enumerate(ars):\\n      ar.rawlink(functools.partial(complete, n))\\n    return ret\\n\\n  @staticmethod\\n  def WhenAny(ars):\\n    \\\"\\\"\\\"Returns an AsyncResult representing the state of any AsyncResult passed in.\\n    The return value represents the state of the first AsyncResult to complete, or,\\n    if all fail, the last to fail.\\n\\n    Args:\\n      ars - An enumerable of AsyncResults.\\n    Returns:\\n      An AsyncResult representing the state of the first AsyncResult to complete.\\n      The AsyncResult's value will be set to the value of the first result to\\n      complete, or, if all fail, the exception thrown by the last to fail.\\n    \\\"\\\"\\\"\\n    ready_ars = [ar for ar in ars if ar.ready()]\\n    if ready_ars:\\n      return ready_ars[0]\\n\\n    ret = AsyncResult()\\n    total = [len(ars)]\\n    def complete(_ar):\\n      total[0] -= 1\\n      if total[0] == 0 and _ar.exception:\\n        ret.set_exception(_ar.exception)\\n      elif not ret.ready() and _ar.successful():\\n        ret.set(_ar.value)\\n\\n    for ar in ars:\\n      ar.rawlink(complete)\\n    return ret\\n\\n  @staticmethod\\n  def FromValue(val):\\n    if val is None:\\n      return AsyncResult.Complete()\\n    else:\\n      ar = AsyncResult()\\n      ar.set(val)\\n      return ar\\n\\n  @staticmethod\\n  def Complete():\\n    \\\"\\\"\\\"Return an AsyncResult that has completed.\\\"\\\"\\\"\\n    return _COMPLETE\\n\\n  @staticmethod\\n  def CompleteIn(n):\\n    \\\"\\\"\\\"Returns an AsyncResult that completes in <n> seconds\\n\\n    Args:\\n      n - The number of seconds to wait before completing.\\n    \\\"\\\"\\\"\\n    ar = AsyncResult()\\n    def helper():\\n      ar.set()\\n    g = Greenlet(helper)\\n    g.start_later(float(n))\\n    return ar\\n\\n  def _SafeLinkHelper(self, fn):\\n    try:\\n      self.set(fn())\\n    except:\\n      self.set_exception(sys.exc_info()[1])\\n\\n  def SafeLink(self, fn):\\n    \\\"\\\"\\\"Propagate the result of calling fn() on a new greenlet to ar\\n\\n    Args:\\n      ar - An AsyncResult.\\n      fn - The function to execute.\\n    \\\"\\\"\\\"\\n    gevent.spawn(self._SafeLinkHelper, fn)\\n\\n  def ContinueWith(self, fn, on_hub=True):\\n    cw_ar = AsyncResult()\\n    def continue_with_callback(_ar):\\n      def run():\\n        try:\\n          val = fn(_ar)\\n          cw_ar.set(val)\\n        except:\\n          cw_ar.set_exception(sys.exc_info()[1])\\n      if on_hub:\\n        run()\\n      else:\\n        gevent.spawn(run)\\n    self.rawlink(continue_with_callback)\\n    return cw_ar\\n\\n  def Map(self, fn):\\n    def mapper(_):\\n      if self.exception:\\n        return self\\n      else:\\n        return fn(self.value)\\n    return self.ContinueWith(mapper).Unwrap()\\n\\n  def _UnwrapHelper(self, target):\\n    if self.ready():\\n      # We're ready, propagate the result\\n      if self.exception:\\n        target.set_exception(self.exception)\\n      else:\\n        if isinstance(self.value, AsyncResult):\\n          self.value._UnwrapHelper(target)\\n        else:\\n          target.set(self.value)\\n    else:\\n      self.rawlink(\\n        functools.partial(AsyncResult._UnwrapHelper, target=target))\\n\\n  def Unwrap(self):\\n    unwrapped_ar = AsyncResult()\\n    self._UnwrapHelper(unwrapped_ar)\\n    return unwrapped_ar\\n\\n  @staticmethod\\n  def TryGet(val):\\n    if isinstance(val, AsyncResult):\\n      return val.get()\\n    else:\\n      return val\\n\\n  @staticmethod\\n  def Run(fn):\\n    ar = AsyncResult()\\n    ar.SafeLink(fn)\\n    return ar\\n\\n  @staticmethod\\n  def RunInline(fn):\\n    ar = AsyncResult()\\n    ar._SafeLinkHelper(fn)\\n    return ar\"}, {\"identifier\":\"NoopTimeout\", \"path\":\"scales/asynchronous.py\", \"snippet\":\"class NoopTimeout(object):\\n  def start(self): pass\\n  def cancel(self): pass\"}, {\"identifier\":\"BytesIO\", \"path\":\"scales/compat.py\", \"snippet\":\"\"}, {\"identifier\":\"ChannelState\", \"path\":\"scales/constants.py\", \"snippet\":\"class ChannelState(object):\\r\\n  Idle = 1\\r\\n  Open = 2\\r\\n  Busy = 3\\r\\n  Closed = 4\\r\"}, {\"identifier\":\"SinkProperties\", \"path\":\"scales/constants.py\", \"snippet\":\"class SinkProperties(object):\\r\\n  Endpoint = 'endpoint'\\r\\n  ServiceInterface = 'service_iface'\\r\\n  Label = 'label'\\r\"}, {\"identifier\":\"SinkRole\", \"path\":\"scales/constants.py\", \"snippet\":\"class SinkRole(object):\\r\\n  Transport = 'transport'\\r\\n  Pool = 'pool'\\r\\n  LoadBalancer = 'loadbalancer'\\r\\n  Formatter = 'formatter'\\r\"}, {\"identifier\":\"ChannelConcurrencyError\", \"path\":\"scales/message.py\", \"snippet\":\"class ChannelConcurrencyError(Exception): pass\\r\"}, {\"identifier\":\"ClientError\", \"path\":\"scales/message.py\", \"snippet\":\"class ClientError(Exception): pass\\r\"}, {\"identifier\":\"Deadline\", \"path\":\"scales/message.py\", \"snippet\":\"class Deadline(object):\\r\\n  KEY = \\\"__Deadline\\\"\\r\\n  EVENT_KEY = \\\"__Deadline_Event\\\"\\r\\n\\r\\n  def __init__(self, timeout):\\r\\n    \\\"\\\"\\\"\\r\\n    Args:\\r\\n      timeout - The timeout in seconds\\r\\n    \\\"\\\"\\\"\\r\\n    import  time\\r\\n    self._ts = Long(time.time()) * 1000000000 # Nanoseconds\\r\\n    self._timeout = Long(timeout * 1000000000)\\r\"}, {\"identifier\":\"MethodCallMessage\", \"path\":\"scales/message.py\", \"snippet\":\"class MethodCallMessage(Message):\\r\\n  \\\"\\\"\\\"A MethodCallMessage represents a method being invoked on a service.\\\"\\\"\\\"\\r\\n  __slots__ = ('service', 'method', 'args', 'kwargs')\\r\\n\\r\\n  def __init__(self, service, method, args, kwargs):\\r\\n    \\\"\\\"\\\"\\r\\n    Args:\\r\\n      service - The service this method call is intended for.\\r\\n      method - The method on the service.\\r\\n      args - The args passed to the method call.\\r\\n      kwargs-  The kwargs passed to the method call.\\r\\n    \\\"\\\"\\\"\\r\\n    super(MethodCallMessage, self).__init__()\\r\\n    self.service = service\\r\\n    self.method = method\\r\\n    self.args = args\\r\\n    self.kwargs = kwargs\\r\"}, {\"identifier\":\"MethodReturnMessage\", \"path\":\"scales/message.py\", \"snippet\":\"class MethodReturnMessage(Message):\\r\\n  \\\"\\\"\\\"A message representing the return value from a service call.\\\"\\\"\\\"\\r\\n  __slots__ = ('return_value', 'error', 'stack')\\r\\n\\r\\n  def __init__(self, return_value=None, error=None):\\r\\n    \\\"\\\"\\\"\\r\\n    Args:\\r\\n      return_value - The return value of the call, or None\\r\\n      error - The error that occurred during processing, or None.\\r\\n              If not None, the current stack will be captured and included.\\r\\n    \\\"\\\"\\\"\\r\\n    super(MethodReturnMessage, self).__init__()\\r\\n    self.return_value = return_value\\r\\n    self.error = error\\r\\n    if error:\\r\\n      exc_info = sys.exc_info()\\r\\n      if len(exc_info) != 3 or exc_info[2] is None:\\r\\n        try:\\r\\n          raise ZeroDivisionError\\r\\n        except ZeroDivisionError:\\r\\n          tb = sys.exc_info()[2]\\r\\n          frame = tb.tb_frame.f_back\\r\\n      else:\\r\\n        tb = exc_info[2]\\r\\n        while tb.tb_next is not None:\\r\\n          tb = tb.tb_next\\r\\n        frame = tb.tb_frame\\r\\n\\r\\n      stack = traceback.format_list(traceback.extract_stack(frame))\\r\\n      stack = stack + traceback.format_exception_only(error.__class__, error)\\r\\n      self.stack = stack\\r\\n      # Prevent circular references\\r\\n      del frame\\r\\n      del tb\\r\\n    else:\\r\\n      self.stack = None\\r\"}, {\"identifier\":\"TimeoutError\", \"path\":\"scales/message.py\", \"snippet\":\"class TimeoutError(Exception):\\r\\n  def __init__(self):\\r\\n    super(TimeoutError, self).__init__(\\r\\n      'The call did not complete within the specified timeout '\\r\\n      'and has been aborted.')\\r\"}, {\"identifier\":\"SinkProvider\", \"path\":\"scales/sink.py\", \"snippet\":\"def SinkProvider(sink_cls, role=None, **defaults):\\n  \\\"\\\"\\\"Factory for creating simple sink providers.\\n\\n  Args:\\n    sink_cls - The type of sink to provide.\\n  Returns:\\n    A SinkProvider that provides sinks of type 'sink_cls'.\\n  \\\"\\\"\\\"\\n  field_names = ' '.join(defaults.keys())\\n  params_cls = namedtuple('Params', field_names)\\n\\n  def CreateSink(self, properties):\\n    return self.SINK_CLASS(self.next_provider, self.sink_properties, properties)\\n\\n  def sink_class(self):\\n    return self.SINK_CLASS\\n\\n  provider = type(\\n    sink_cls.__name__ + 'Provider',\\n    (SinkProviderBase, ),\\n    {\\n      'SINK_CLASS': sink_cls,\\n      'PARAMS_CLASS': params_cls,\\n      'Role': role,\\n      'CreateSink': CreateSink,\\n      'sink_class': property(sink_class),\\n      '_defaults': defaults\\n    }\\n  )\\n  return provider\"}, {\"identifier\":\"SocketTransportSinkProvider\", \"path\":\"scales/sink.py\", \"snippet\":\"def SocketTransportSinkProvider(sink_cls):\\n  class _SocketTransportSinkProvider(SinkProviderBase):\\n    SINK_CLS = sink_cls\\n    Role = SinkRole.Transport\\n\\n    def CreateSink(self, properties):\\n      server = properties[SinkProperties.Endpoint]\\n      service = properties[SinkProperties.Label]\\n      sock = ScalesSocket(server.host, server.port)\\n      healthy_sock = VarzSocketWrapper(sock, service)\\n      sink = self.SINK_CLS(healthy_sock, service)\\n      return sink\\n\\n    @property\\n    def sink_class(self):\\n      return self.SINK_CLS\\n\\n  return _SocketTransportSinkProvider\"}, {\"identifier\":\"ClientMessageSink\", \"path\":\"scales/sink.py\", \"snippet\":\"class ClientMessageSink(MessageSink):\\n  \\\"\\\"\\\"ClientMessageSinks take a message, stream, and headers and perform\\n  processing on them.\\n  \\\"\\\"\\\"\\n  __slots__ = '_on_faulted',\\n  Role = None\\n  Builder = None\\n\\n  def __init__(self):\\n    self._on_faulted = Observable()\\n    super(ClientMessageSink, self).__init__()\\n\\n  @property\\n  def state(self):\\n    return self.next_sink.state\\n\\n  @property\\n  def is_open(self):\\n    \\\"\\\"\\\"Returns True if the sink is Idle, Open, or Busy\\\"\\\"\\\"\\n    return self.state <= ChannelState.Busy\\n\\n  @property\\n  def is_closed(self):\\n    \\\"\\\"\\\"Returns True if the sink is Closed.\\\"\\\"\\\"\\n    return self.state == ChannelState.Closed\\n\\n  @property\\n  def is_ready(self):\\n    \\\"\\\"\\\"Returns True if the channel is open, eg ready to process messages.\\\"\\\"\\\"\\n    return self.state == ChannelState.Open\\n\\n  @property\\n  def on_faulted(self):\\n    return self._on_faulted\\n\\n  def Open(self):\\n    if self.next_sink:\\n      return self.next_sink.Open()\\n    else:\\n      return AsyncResult.Complete()\\n\\n  def Close(self):\\n    if self.next_sink:\\n      self.next_sink.Close()\\n\\n  @abstractmethod\\n  def AsyncProcessRequest(self, sink_stack, msg, stream, headers):\\n    \\\"\\\"\\\"Process a request message, stream, and headers.\\n\\n    Args:\\n      sink_stack - The SinkStack representing the processing state of the message.\\n                   Implementors should push their sink onto this stack before\\n                   forwarding the message in order to participate in processing\\n                   the response.\\n      msg - The message being processed.\\n      stream - A serialized version of the message.\\n      headers - Any additional headers to be sent.\\n    \\\"\\\"\\\"\\n    raise NotImplementedError()\\n\\n  @abstractmethod\\n  def AsyncProcessResponse(self, sink_stack, context, stream, msg):\\n    \\\"\\\"\\\"Process a response stream.\\n\\n    Args:\\n      sink_stack - The SinkStack representing the processing state of the message.\\n                   Implementors should call sink_stack.AsyncProcessMessage(...)\\n                   to forward the message to the next sink.\\n      context - The context that was pushed onto the stack in AsyncProcessRequest.\\n      stream - The stream representing the serialized response.\\n    \\\"\\\"\\\"\\n    raise NotImplementedError()\"}, {\"identifier\":\"AggregateTimer\", \"path\":\"scales/varz.py\", \"snippet\":\"class VarzType(object):\\nclass Source(object):\\nclass VarzMetric(object):\\nclass Gauge(VarzMetric): VARZ_TYPE = VarzType.Gauge\\nclass Rate(VarzMetric): VARZ_TYPE = VarzType.Rate\\nclass AverageRate(VarzMetric): VARZ_TYPE = VarzType.AverageRate\\nclass Counter(Rate): VARZ_TYPE = VarzType.Counter\\nclass VarzTimerBase(VarzMetric):\\nclass AverageTimer(VarzTimerBase): VARZ_TYPE = VarzType.AverageTimer\\nclass AggregateTimer(VarzTimerBase): VARZ_TYPE = VarzType.AggregateTimer\\nclass VarzMeta(type):\\nclass _VarzBase(object):\\nclass _SampleSet(object):\\nclass VarzReceiver(object):\\nclass VarzAggregator(object):\\n  class _Agg(object):\\nclass VarzSocketWrapper(object):\\n  class Varz(VarzBase):\\nclass MonoClock(object):\\nclass Ema(object):\\n  def __init__(self, method=None, service=None, endpoint=None, client_id=None):\\n  def to_tuple(self):\\n  def to_dict(self):\\n  def __cmp__(self, other):\\n  def __hash__(self):\\n  def _Adapt(fn):\\n    def __Adapt(metric, source, amount=1):\\n  def __init__(self, metric, source):\\n  def __call__(self, *args):\\n  def ForSource(self, source):\\n  def Measure(self, source=None):\\n  def __new__(mcs, name, bases, dct):\\ndef VerifySource(source):\\n  def __init__(self, source):\\n  def __getattr__(self, item):\\n  def __init__(self, max_size, data=None, p=.1):\\n  def Sample(self, value):\\n  def RegisterMetric(metric, varz_type):\\n  def IncrementVarz(source, metric, amount=1):\\n  def SetVarz(source, metric, value):\\n  def RecordPercentileSample(cls, source, metric, value):\\ndef DefaultKeySelector(k):\\n    def __init__(self):\\n  def CalculatePercentile(values, pct):\\n  def _Downsample(lst, target_size):\\n  def Aggregate(varz, metrics, key_selector=None):\\n  def __init__(self, socket, varz_tag):\\n  def host(self):\\n  def port(self):\\n  def isOpen(self):\\n  def read(self, sz):\\n  def recv_into(self, buf, sz):\\n  def flush(self):\\n  def write(self, buff):\\n  def open(self):\\n  def close(self):\\n  def readAll(self, sz):\\n  def __init__(self):\\n  def Sample(self):\\n  def __init__(self, window):\\n  def Update(self, ts, sample):\\n  VARZ_TYPE = None\\nclass Gauge(VarzMetric): VARZ_TYPE = VarzType.Gauge\\nclass Rate(VarzMetric): VARZ_TYPE = VarzType.Rate\\nclass AverageRate(VarzMetric): VARZ_TYPE = VarzType.AverageRate\\nclass Counter(Rate): VARZ_TYPE = VarzType.Counter\\nclass AverageTimer(VarzTimerBase): VARZ_TYPE = VarzType.AverageTimer\\nclass AggregateTimer(VarzTimerBase): VARZ_TYPE = VarzType.AggregateTimer\\n  _VARZ = {}\\n  _VARZ_BASE_NAME = None\\n  VARZ_METRICS = {}\\n  VARZ_DATA = defaultdict(lambda: defaultdict(int))\\n  VARZ_PERCENTILES = [.5, .90, .99, .999, .9999]\\n  _MAX_PERCENTILE_SIZE = 1000\\n  MAX_AGG_AGE = 5 * 60\\n    _VARZ_BASE_NAME = 'scales.socket'\\n    _VARZ = {\\n      'bytes_recv': Rate,\\n      'bytes_sent': Rate,\\n      'num_connections': Counter,\\n      'tests_failed': Counter,\\n      'connects': Rate,\\n      'open_latency': AverageTimer\\n    }\"}, {\"identifier\":\"MessageSerializer\", \"path\":\"scales/thrift/serializer.py\", \"snippet\":\"class MessageSerializer(object):\\n  \\\"\\\"\\\"A serializer that can serialize and deserialize thrift method calls.\\n\\n  This relies on the generated thrift args and return value classes created\\n  by the thrift compiler to do the serialization/deserialization.\\n  \\\"\\\"\\\"\\n  def __init__(\\n      self,\\n      service_cls,\\n      protocol_factory=TBinaryProtocolAcceleratedFactory()):\\n    \\\"\\\"\\\"\\n    Args:\\n      service_cls - The thrift generated interface class.\\n      protocol_factory - A class implementing getProtocol(...).  By default,\\n       TBinaryProtocolAcceleratedFactory is used.\\n    \\\"\\\"\\\"\\n    self._protocol_factory = protocol_factory\\n    self._seq_id = 0\\n    self._service_modules = [sys.modules[c.__module__]\\n                             for c in inspect.getmro(service_cls)\\n                             if not c is object]\\n    if len(self._service_modules) == 1:\\n      self._FindClass = self._FindClassNoInheritance\\n    else:\\n      self._attr_cache = {}\\n      self._FindClass = self._FindClassInheritance\\n\\n  def _FindClassInheritance(self, name):\\n    cls = self._attr_cache.get(name)\\n    if cls:\\n      return cls\\n\\n    for m in self._service_modules:\\n      cls = getattr(m, name, None)\\n      if cls:\\n        self._attr_cache[name] = cls\\n        return cls\\n    return None\\n\\n  def _FindClassNoInheritance(self, name):\\n    return getattr(self._service_modules[0], name, None)\\n\\n  def SerializeThriftCall(self, msg, buf):\\n    \\\"\\\"\\\"Serialize a MethodCallMessage to a stream\\n\\n    Args:\\n      msg - The MethodCallMessage to serialize.\\n      buf - The buffer to serialize into.\\n    \\\"\\\"\\\"\\n    thrift_buffer = TMemoryBuffer()\\n    thrift_buffer._buffer = buf\\n    protocol = self._protocol_factory.getProtocol(thrift_buffer)\\n    method, args, kwargs = msg.method, msg.args, msg.kwargs\\n    is_one_way = self._FindClass('%s_result' % method) is None\\n    args_cls = self._FindClass('%s_args' % method)\\n    if not args_cls:\\n      raise AttributeError('Unable to find args class for method %s' % method)\\n\\n    protocol.writeMessageBegin(\\n        msg.method,\\n        TMessageType.ONEWAY if is_one_way else TMessageType.CALL,\\n        self._seq_id)\\n    thrift_args = args_cls(*args, **kwargs)\\n    thrift_args.write(protocol)\\n    protocol.writeMessageEnd()\\n\\n  def DeserializeThriftCall(self, buf):\\n    \\\"\\\"\\\"Deserialize a stream and context to a MethodReturnMessage.\\n\\n    Args:\\n      buf - The buffer.\\n      ctx - The context from serialization.\\n\\n    Returns:\\n      A MethodCallMessage.\\n    \\\"\\\"\\\"\\n\\n    thrift_buffer = TMemoryBuffer()\\n    thrift_buffer._buffer = buf\\n    protocol = self._protocol_factory.getProtocol(thrift_buffer)\\n\\n    (fn_name, msg_type, seq_id) = protocol.readMessageBegin()\\n    if msg_type == TMessageType.EXCEPTION:\\n      x = TApplicationException()\\n      x.read(protocol)\\n      protocol.readMessageEnd()\\n      return MethodReturnMessage(error=x)\\n\\n    result_cls = self._FindClass('%s_result' % fn_name)\\n    if result_cls:\\n      result = result_cls()\\n      result.read(protocol)\\n    else:\\n      result = None\\n    protocol.readMessageEnd()\\n\\n    if not result:\\n      return MethodReturnMessage()\\n    if getattr(result, 'success', None) is not None:\\n      return MethodReturnMessage(return_value=result.success)\\n\\n    result_spec = getattr(result_cls, 'thrift_spec', None)\\n    if result_spec:\\n      exceptions = result_spec[1:]\\n      for e in exceptions:\\n        attr_val = getattr(result, e[2], None)\\n        if attr_val is not None:\\n          return MethodReturnMessage(error=attr_val)\\n\\n    return MethodReturnMessage(TApplicationException(\\n      TApplicationException.MISSING_RESULT, \\\"%s failed: unknown result\\\" % fn_name))\"}]", "import_statement": "from struct import (pack, unpack)\nfrom thrift.protocol.TBinaryProtocol import TBinaryProtocolAcceleratedFactory\nfrom ..asynchronous import (\n  AsyncResult,\n  NoopTimeout\n)\nfrom ..compat import BytesIO\nfrom ..constants import (\n  ChannelState,\n  SinkProperties,\n  SinkRole\n)\nfrom ..message import (\n  ChannelConcurrencyError,\n  ClientError,\n  Deadline,\n  MethodCallMessage,\n  MethodReturnMessage,\n  TimeoutError\n)\nfrom ..sink import (\n  SinkProvider,\n  SocketTransportSinkProvider,\n  ClientMessageSink,\n)\nfrom ..varz import (\n  AggregateTimer,\n  AverageTimer,\n  Rate,\n  Source,\n  VarzBase,\n)\nfrom .serializer import MessageSerializer\nimport time\nimport gevent", "code": "\n\n\n\nclass SocketTransportSink(ClientMessageSink):\n  \"\"\"A sink to transport thrift method calls over a socket.\n\n  This sink does not support processing multiple messages in parallel and will\n  raise an exception if it detects it is about to.\n  \"\"\"\n  class Varz(VarzBase):\n    \"\"\"\n    messages_sent - The number of messages sent over this sink.\n    messages_recv - The number of messages received over this sink.\n    send_time - The aggregate amount of time spent sending data.\n    recv_time - The aggregate amount of time spend receiving data.\n    send_latency - The average amount of time taken to send a message.\n    recv_latency - The average amount of time taken to receive a message\n                   (once a response has reached the client).\n    transport_latency - The average amount of time taken to perform a full\n                        method call transaction (send data, wait for response,\n                        read response).\n    \"\"\"\n\n    _VARZ_BASE_NAME = 'scales.thrift.SocketTransportSink'\n    _VARZ = {\n      'messages_sent': Rate,\n      'messages_recv': Rate,\n      'send_time': AggregateTimer,\n      'recv_time': AggregateTimer,\n      'send_latency': AverageTimer,\n      'recv_latency': AverageTimer,\n      'transport_latency': AverageTimer\n    }\n\n  def __init__(self, socket, source):\n    super(SocketTransportSink, self).__init__()\n    self._socket = socket\n    self._state = ChannelState.Idle\n    socket_source = '%s:%d' % (self._socket.host, self._socket.port)\n    self._varz = self.Varz(Source(service=source, endpoint=socket_source))\n    self._processing = None\n    self._open_result = None\n\n  def Open(self):\n    if not self._open_result:\n", "next_line": "      self._open_result = AsyncResult()", "gold_snippet_index": 0, "id": 15, "__internal_uuid__": "17f8412b-9333-4a93-84c0-efcd9848b110"}
{"repo_name": "charityscience/csh-sms", "file_path": "tests/modules/test_text_processor.py", "context": "[{\"identifier\":\"Contact\", \"path\":\"management/models.py\", \"snippet\":\"class Contact(models.Model):\\n    # Vitals\\n    name = models.CharField(max_length=50)\\n    phone_regex = RegexValidator(regex=r'^\\\\+?91?\\\\d{9,15}$',\\n        message=\\\"Phone number must be entered in the format: '+9199999999'. Up to 15 digits allowed.\\\",\\n        code=\\\"Invalid phone_number\\\")\\n    phone_number = models.CharField(validators=[phone_regex], blank=False,\\n        max_length=20, default=\\\"012345\\\") # validators should be a list\\n    alt_phone_number = models.CharField(validators=[phone_regex], blank=False,\\n        max_length=20, default=\\\"012345\\\")\\n    date_of_birth = models.DateField(auto_now=False, auto_now_add=False,\\n        default=datetime.date.today)\\n    date_of_sign_up = models.DateField(auto_now=False, auto_now_add=False,\\n        default=datetime.date.today)\\n    delay_in_days = models.SmallIntegerField(default=0, blank=True)\\n    functional_date_of_birth = models.DateField(blank=True,auto_now=False,\\n        auto_now_add=False, default=datetime.date.today)\\n    cancelled = models.BooleanField(default=False, blank=False)\\n\\n    # Personal Info\\n    gender = models.CharField(max_length=6, blank=True)\\n    mother_tongue = models.CharField(max_length=50, blank=True)\\n    state = models.CharField(max_length=50, blank=True)\\n    division = models.CharField(max_length=50, blank=True)\\n    district = models.CharField(max_length=50, blank=True)\\n    city = models.CharField(max_length=50, blank=True)\\n    monthly_income_rupees = models.IntegerField(blank=True, default=999999)\\n    religion = models.CharField(max_length=50, blank=True)\\n    children_previously_vaccinated = models.NullBooleanField()\\n    not_vaccinated_why = models.CharField(max_length=500, blank=True)\\n    mother_first_name = models.CharField(max_length=50, blank=True)\\n    mother_last_name = models.CharField(max_length=50, blank=True)\\n    \\n    # Type of Sign Up\\n    method_of_sign_up = models.CharField(max_length=50, blank=True)\\n    org_sign_up = models.CharField(max_length=40, blank=True)\\n    hospital_name = models.CharField(max_length=50, blank=True)\\n    doctor_name = models.CharField(max_length=30, blank=True)\\n    preg_signup = models.NullBooleanField(default=False)\\n    preg_update = models.NullBooleanField(default=False)\\n\\n    def has_been_born(self):\\n        today = datetime.date.today()\\n        diff = today - self.date_of_birth\\n        return  diff >= datetime.timedelta(0)\\n\\n\\n    # System Identification\\n    telerivet_contact_id = models.CharField(max_length=50, blank=True)\\n    trial_id = models.CharField(max_length=40, blank=True)\\n    trial_group = models.CharField(max_length=40, blank=True)    \\n\\n\\n    language_preference = models.CharField(max_length=20,\\n        default=\\\"English\\\", blank=False, null=False)\\n\\n    # Message References\\n    preferred_time = models.CharField(max_length=50, blank=True)\\n    script_selection = models.CharField(max_length=20, blank=True)\\n    telerivet_sender_phone = models.CharField(max_length=100, blank=True)\\n    time_created = models.DateField(auto_now=False, auto_now_add=False,\\n        default=datetime.date.today)\\n    last_heard_from = models.DateTimeField(auto_now=False, auto_now_add=False, blank=True,\\n        null=True)\\n    last_contacted = models.DateTimeField(auto_now=False, auto_now_add=False, blank=True,\\n        null=True)\\n\\n    def __str__(self):\\n        return \\\"%s, %s, %s\\\" % (self.name, self.phone_number, self.date_of_birth)\\n\\n    class Meta:\\n        ordering = ('name',)\"}, {\"identifier\":\"Message\", \"path\":\"management/models.py\", \"snippet\":\"class Message(models.Model):\\n    contact = models.ForeignKey(Contact, on_delete=models.CASCADE, null=True)\\n    body = models.CharField(max_length=300)\\n\\n    # Message direction is Incoming or Outgoing\\n    direction = models.CharField(max_length=10)\\n    \\n    is_processed = models.BooleanField(default=False, blank=False)\\n    created_at = models.DateTimeField(auto_now_add=True)\\n    received_at = models.DateTimeField(auto_now=False,\\n                                        auto_now_add=False,\\n                                        blank=True,\\n                                        null=True)\\n    sent_at = models.DateTimeField(auto_now=False,\\n                                    auto_now_add=False,\\n                                    blank=True,\\n                                    null=True)\\n\\n    def __str__(self):\\n        return self.body\"}, {\"identifier\":\"quote\", \"path\":\"modules/utils.py\", \"snippet\":\"def quote(word):\\n    return u\\\"`{}`\\\".format(word)\"}, {\"identifier\":\"TextProcessor\", \"path\":\"modules/text_processor.py\", \"snippet\":\"class TextProcessor(object):\\n    def __init__(self, phone_number):\\n        self.phone_number = phone_number\\n        self.set_language(default=None)\\n\\n    def set_language(self, default):\\n        if self.get_contacts().exists():\\n            self.language = self.contacts.first().language_preference or default\\n        else:\\n            self.language = None\\n\\n    def get_language(self, language, inferred_language, keyword):\\n        # Returns inferred language if the keyword is in the subscribe keywords of the inferred langauge,\\n        #  ignoring \\\"born\\\"\\n        subscribe_keys_without_born = keywords_without_word(language=inferred_language, word=\\\"born\\\")\\n        if keyword in subscribe_keys_without_born:\\n            return inferred_language\\n        return language\\n\\n    # self.get_contacts() is preferred to self.contact due to triggering a Django DB reload.\\n    def get_contacts(self):\\n        self.contacts = Contact.objects.filter(phone_number=self.phone_number)\\n        return self.contacts\\n\\n\\n    def create_contact(self, child_name, phone_number, date_of_birth, language, preg_update=False):\\n        contact = Contact.objects.filter(name=child_name, phone_number=self.phone_number).first()\\n        if contact:\\n            if contact.cancelled or preg_update:\\n                # Update and resubscribe\\n                contact.cancelled = False\\n                contact.language_preference = language\\n                contact.date_of_birth = date_of_birth\\n                contact.functional_date_of_birth = date_of_birth\\n                contact.preg_update = preg_update\\n                contact.save()\\n                return True\\n            elif Message.objects.filter(contact=contact,\\n                                        direction=\\\"Outgoing\\\",\\n                                        body=msg_subscribe(language).format(name=contact.name)).exists():\\n                # Already exists (error)\\n                logging.error(\\\"Contact for {name} at {phone} was subscribed but already exists!\\\".format(name=child_name, phone=self.phone_number))\\n                return False\\n\\n        # Otherwise, create\\n        update_dict = {\\\"delay_in_days\\\": 0,\\n                       \\\"language_preference\\\": self.language,\\n                       \\\"date_of_birth\\\": date_of_birth,\\n                       \\\"functional_date_of_birth\\\": date_of_birth,\\n                       \\\"method_of_sign_up\\\": \\\"Text\\\"}\\n        contact, _ = Contact.objects.update_or_create(name=child_name,\\n                                                      phone_number=phone_number,\\n                                                      defaults=update_dict)\\n        for group_name in [\\\"Text Sign Ups\\\",\\n                           \\\"Text Sign Ups - \\\" + self.language.title(),\\n                           \\\"Everyone - \\\" + self.language.title()]:\\n            add_contact_to_group(contact, group_name)\\n        self.get_contacts()\\n        return True\\n\\n\\n    def cancel_contacts(self):\\n        for contact in self.contacts:\\n            contact.cancelled = True\\n            contact.save()\\n        return True\\n\\n\\n    def process_subscribe(self, child_name, date_of_birth, preg_update):        \\n        if self.create_contact(child_name=child_name,\\n                               phone_number=self.phone_number,\\n                               date_of_birth=date_of_birth,\\n                               language=self.language,\\n                               preg_update=preg_update):\\n            return msg_subscribe(self.language).format(name=child_name)\\n        else:\\n            return msg_already_sub(self.language)\\n\\n\\n    def process_unsubscribe(self, child_name, date_of_birth, preg_update=False):\\n        if self.contacts.exists():\\n            contact = self.get_contacts().first()\\n            if contact.name is None or contact.date_of_birth is None or contact.language_preference is None:\\n                logging.error(quote(self.phone_number) + \\\" asked to be unsubscribed but some data is missing on the existing contact object.\\\")\\n            self.cancel_contacts()\\n        else:\\n            logging.error(quote(self.phone_number) + \\\" asked to be unsubscribed but does not exist.\\\")\\n        return msg_unsubscribe(self.language or \\\"English\\\")\\n\\n\\n    def process_failure(self, child_name, date_of_birth, preg_update=False):\\n        return msg_failure(self.language)\\n\\n\\n    def process_failed_date(self, child_name, date_of_birth, preg_update=False):\\n        return msg_failed_date(self.language)\\n\\n\\n    def get_data_from_message(self, message):\\n        \\\"\\\"\\\"Get the keyword, child name, and the date from the message.\\n            A text will look like `<KEYWORD> <CHILD> <DATE OF BIRTH>`, like\\n            `REMIND NATHAN 25/11/2015`. Sometimes the child name is omitted.\\\"\\\"\\\"\\n        message = message.lower().split(\\\" \\\")\\n        if len(message) == 1:\\n            keyword = message[0]\\n            date = None\\n            child_name = None\\n        elif len(message) == 2:\\n            keyword, date = message\\n            child_name = None\\n        else:\\n            keyword, child_name, date = message[0:3]\\n        date = date_string_to_date(date) if date and date_is_valid(date) else None\\n        return (keyword, child_name, date)\\n\\n    def write_to_database(self, message, date):\\n        keyword, child_name, date_entered = self.get_data_from_message(message)\\n        inferred_language = \\\"Hindi\\\" if keyword and keyword[0] not in string.ascii_lowercase else \\\"English\\\"\\n        language = self.language or inferred_language\\n\\n        if language != inferred_language:\\n            language = self.get_language(language=language,\\n                                            inferred_language=inferred_language,\\n                                            keyword=keyword)\\n\\n        if not child_name and self.get_contacts():\\n            contact = self.get_contacts().first()\\n            child_name = contact.name\\n\\n        if child_name:\\n            child_name = child_name.title()\\n\\n        incoming = self.create_message_object(child_name=child_name,\\n                                              phone_number=self.phone_number,\\n                                              language=language,\\n                                              body=message,\\n                                              direction=\\\"Incoming\\\")\\n\\n        contact = Contact.objects.get(pk=incoming.contact.id)\\n        contact.last_heard_from = incoming.created_at\\n        incoming.received_at = date\\n        incoming.save()\\n        contact.save()\\n        self.get_contacts()\\n        return incoming\\n\\n    def process(self, message):\\n        \\\"\\\"\\\"This is the main function that is run on an message to process it.\\\"\\\"\\\"\\n        contact = Contact.objects.get(pk=message.contact.id)\\n        keyword, child_name, date = self.get_data_from_message(message.body)\\n        preg_update = False\\n        if keyword in subscribe_keywords(\\\"English\\\"):\\n            self.set_language(default=\\\"English\\\")\\n            if keyword == \\\"born\\\":\\n                preg_update = True\\n            action = self.process_subscribe\\n        elif keyword in subscribe_keywords(\\\"Hindi\\\"):\\n            self.set_language(default=\\\"Hindi\\\")\\n            if keyword == hindi_born():\\n                preg_update = True\\n            action = self.process_subscribe\\n        elif keyword == \\\"end\\\":\\n            self.set_language(default=\\\"English\\\")\\n            action = self.process_unsubscribe\\n        else:\\n            self.set_language(default=\\\"English\\\")\\n            logging.error(\\\"Keyword \\\" + quote(keyword) + \\\" in message \\\" + quote(message.body) +\\n                          \\\" was not understood by the system.\\\")\\n            action = self.process_failure\\n\\n        if action == self.process_subscribe:\\n            if child_name is None:\\n                # If a child name is not found, we call them \\\"your child\\\".\\n                child_name = msg_placeholder_child(self.language)\\n            else:\\n                child_name = child_name.title()\\n\\n            if len(child_name) > 50:\\n                action = self.process_failure\\n\\n            if date is None:\\n                logging.error(\\\"Date in message \\\" + quote(message.body) + \\\" is invalid.\\\")\\n                action = self.process_failed_date\\n\\n        if action == self.process_subscribe:\\n            logging.info(\\\"Subscribing \\\" + quote(message.body) + \\\"...\\\")\\n        elif action == self.process_unsubscribe:\\n            logging.info(\\\"Unsubscribing \\\" + quote(contact.phone_number) + \\\"...\\\")\\n\\n        response_text_message = action(child_name=child_name,\\n                                       date_of_birth=date,\\n                                       preg_update=preg_update)\\n\\n        outgoing = self.create_message_object(child_name=contact.name,\\n                                              phone_number=contact.phone_number,\\n                                              language=self.language,\\n                                              body=response_text_message,\\n                                              direction=\\\"Outgoing\\\")\\n        contact = Contact.objects.get(pk=outgoing.contact.id)\\n        contact.last_contacted = outgoing.created_at\\n        contact.save()\\n        self.get_contacts()\\n        Texter().send(message=response_text_message,\\n                      phone_number=self.phone_number)\\n        outgoing.is_processed = True\\n        outgoing.sent_at = datetime.now().replace(tzinfo=timezone.get_default_timezone())\\n        outgoing.save()\\n        message.is_processed = True\\n        message.save()\\n        return response_text_message\\n\\n    def create_message_object(self, child_name, phone_number, language, body, direction):\\n        if not child_name or len(child_name) > 50:\\n            if not language:\\n                language = \\\"English\\\"\\n            child_name = msg_placeholder_child(language)\\n        try:\\n            contact, _ = Contact.objects.get_or_create(name=child_name,\\n                                                       phone_number=phone_number)\\n        except MultipleObjectsReturned:\\n            contact = Contact.objects.filter(name=child_name,\\n                                             phone_number=phone_number).first()\\n\\n        contact.language_preference = language\\n        contact.save()\\n        return Message.objects.create(contact=contact, direction=direction, body=body)\"}, {\"identifier\":\"hindi_remind\", \"path\":\"modules/i18n.py\", \"snippet\":\"def hindi_remind():\\n    return u'\\\\u0938\\\\u094d\\\\u092e\\\\u0930\\\\u0923'\"}, {\"identifier\":\"hindi_information\", \"path\":\"modules/i18n.py\", \"snippet\":\"def hindi_information():\\n    return u'\\\\u0907\\\\u0924\\\\u094d\\\\u0924\\\\u093f\\\\u0932\\\\u093e'\"}, {\"identifier\":\"msg_placeholder_child\", \"path\":\"modules/i18n.py\", \"snippet\":\"def msg_placeholder_child(language):\\n    if language == \\\"English\\\":\\n        return \\\"Your child\\\"\\n    elif language == \\\"Hindi\\\":\\n        return hindi_placeholder_name()\\n    elif language == \\\"Gujarati\\\":\\n        return gujarati_placeholder_name()\"}, {\"identifier\":\"msg_subscribe\", \"path\":\"modules/i18n.py\", \"snippet\":\"def msg_subscribe(language):\\n    if language == \\\"English\\\":\\n        return \\\"{name} has been subscribed to CSH health reminders. Text END to \\\" + TEXTLOCAL_PHONENUMBER + \\\" to unsubscribe.\\\"\\n    elif language == \\\"Hindi\\\":\\n        return u'{name} \\\\u0938\\\\u0940 \\\\u090f\\\\u0938 \\\\u090f\\\\u091a \\\\u0939\\\\u0947\\\\u0932\\\\u094d\\\\u0925 \\\\u0905\\\\u0928\\\\u0941\\\\u0938\\\\u094d\\\\u092e\\\\u0930\\\\u0928 \\\\u0915\\\\u0947 \\\\u0938\\\\u0926\\\\u0938\\\\u094d\\\\u092f \\\\u0939\\\\u0948\\\\u0902. \\\\u0938\\\\u0926\\\\u0938\\\\u094d\\\\u092f\\\\u0924\\\\u093e \\\\u0930\\\\u0926\\\\u094d\\\\u0926 \\\\u0915\\\\u0930\\\\u0928\\\\u0947 \\\\u0915\\\\u0947 \\\\u0932\\\\u093f\\\\u090f \\\\u0932\\\\u093f\\\\u0916\\\\u0947\\\\u0902 \\\"END\\\" \\\\u0914\\\\u0930 \\\\u092d\\\\u0947\\\\u0902\\\\u091c \\\\u0926\\\\u0947 ' + TEXTLOCAL_PHONENUMBER + u' \\\\u092a\\\\u0930.'\"}, {\"identifier\":\"msg_unsubscribe\", \"path\":\"modules/i18n.py\", \"snippet\":\"def msg_unsubscribe(language):\\n    if language == \\\"English\\\":\\n        return \\\"You have been unsubscribed from CSH health reminders.\\\"\\n    elif language == \\\"Hindi\\\":\\n        return u'\\\\u0906\\\\u092a\\\\u0915\\\\u0940 \\\\u0938\\\\u0926\\\\u0938\\\\u094d\\\\u092f\\\\u0924\\\\u093e \\\\u0938\\\\u092e\\\\u093e\\\\u092a\\\\u094d\\\\u0924 \\\\u0915\\\\u0930 \\\\u0926\\\\u0940 \\\\u0917\\\\u092f\\\\u0940 \\\\u0939\\\\u0948.'\"}, {\"identifier\":\"msg_failure\", \"path\":\"modules/i18n.py\", \"snippet\":\"def msg_failure(language):\\n    if language == \\\"English\\\":\\n        return \\\"Sorry, we didn't understand that message. Text END to \\\" + TEXTLOCAL_PHONENUMBER + \\\" to unsubscribe.\\\"\\n    elif language == \\\"Hindi\\\":\\n        return u'\\\\u0915\\\\u094d\\\\u0937\\\\u092e\\\\u093e \\\\u0915\\\\u0930\\\\u0947\\\\u0902, \\\\u0939\\\\u092e\\\\u0928\\\\u0947 \\\\u0909\\\\u0938 \\\\u0938\\\\u0902\\\\u0926\\\\u0947\\\\u0936 \\\\u0915\\\\u094b \\\\u0928\\\\u0939\\\\u0940\\\\u0902 \\\\u0938\\\\u092e\\\\u091d\\\\u093e. \\\\u0938\\\\u0926\\\\u0938\\\\u094d\\\\u092f\\\\u0924\\\\u093e \\\\u0930\\\\u0926\\\\u094d\\\\u0926 \\\\u0915\\\\u0930\\\\u0928\\\\u0947 \\\\u0915\\\\u0947 \\\\u0932\\\\u093f\\\\u090f \\\\u0932\\\\u093f\\\\u0916\\\\u0947\\\\u0902 \\\"END\\\" \\\\u0914\\\\u0930 \\\\u092d\\\\u0947\\\\u0902\\\\u091c \\\\u0926\\\\u0947 ' + TEXTLOCAL_PHONENUMBER + ' \\\\u092a\\\\u0930.'\"}, {\"identifier\":\"msg_failed_date\", \"path\":\"modules/i18n.py\", \"snippet\":\"def msg_failed_date(language):\\n    if language == \\\"English\\\":\\n        return \\\"Sorry, the date format was incorrect. An example message is 'Remind Sai 14-01-17' where 'Sai' is your child's first name and '14-01-17'' is their birthday.\\\"\\n    elif language == \\\"Hindi\\\":\\n        return u'\\\\u0915\\\\u094d\\\\u0937\\\\u092e\\\\u093e \\\\u0915\\\\u0940\\\\u091c\\\\u093f\\\\u092f\\\\u0947, \\\\u0924\\\\u093e\\\\u0930\\\\u0940\\\\u0916 \\\\u0915\\\\u093e \\\\u092a\\\\u094d\\\\u0930\\\\u093e\\\\u0930\\\\u0942\\\\u092a \\\\u0917\\\\u0932\\\\u0924 \\\\u0939\\\\u0948.'\"}, {\"identifier\":\"msg_already_sub\", \"path\":\"modules/i18n.py\", \"snippet\":\"def msg_already_sub(language):\\n    if language == \\\"English\\\":\\n        return \\\"You are already registered to receive CSH health reminders.\\\"\\n    elif language == \\\"Hindi\\\":\\n        return u'\\\\u0906\\\\u092a \\\\u092a\\\\u0939\\\\u0932\\\\u0947 \\\\u0938\\\\u0947 \\\\u0939\\\\u0940 \\\\u0938\\\\u0940.\\\\u090f\\\\u0938.\\\\u091a \\\\u0938\\\\u094d\\\\u0935\\\\u093e\\\\u0938\\\\u094d\\\\u0925\\\\u094d\\\\u092f \\\\u0938\\\\u094d\\\\u092e\\\\u0930\\\\u0923 \\\\u092a\\\\u094d\\\\u0930\\\\u093e\\\\u092a\\\\u094d\\\\u0924 \\\\u0915\\\\u0930\\\\u0928\\\\u0947 \\\\u0915\\\\u0947 \\\\u0932\\\\u093f\\\\u090f \\\\u092a\\\\u0902\\\\u091c\\\\u0940\\\\u0915\\\\u0943\\\\u0924 \\\\u0939\\\\u0948\\\\u0902.'\"}, {\"identifier\":\"hindi_born\", \"path\":\"modules/i18n.py\", \"snippet\":\"def hindi_born():\\n    return u'\\\\u091c\\\\u0928\\\\u094d\\\\u092e'\"}]", "import_statement": "from mock import patch\nfrom datetime import datetime\nfrom django.test import TestCase\nfrom django.utils import timezone\nfrom freezegun import freeze_time\nfrom management.models import Contact, Message\nfrom modules.utils import quote\nfrom modules.text_processor import TextProcessor\nfrom modules.i18n import hindi_remind, hindi_information, msg_placeholder_child, \\\n                         msg_subscribe, msg_unsubscribe, msg_failure, msg_failed_date, \\\n                         msg_already_sub, hindi_born", "code": "\n\nFAKE_NOW = datetime(2017, 7, 17, 0, 0)\n\nclass TextProcessorGetDataTests(TestCase):\n    def test_all_caps(self):\n        t = TextProcessor(phone_number=\"1-111-1111\")\n        keyword, child_name, date = t.get_data_from_message(\"REMIND NATHAN 25/11/2015\")\n        self.assertEqual(keyword, \"remind\")\n        self.assertEqual(child_name, \"nathan\")\n        self.assertEqual(date, datetime(2015, 11, 25, 0, 0).date())\n\n    def test_all_lowercase(self):\n        t = TextProcessor(phone_number=\"1-111-1111\")\n        keyword, child_name, date = t.get_data_from_message(\"remind nathan 25/11/2015\")\n        self.assertEqual(keyword, \"remind\")\n        self.assertEqual(child_name, \"nathan\")\n        self.assertEqual(date, datetime(2015, 11, 25, 0, 0).date())\n\n    def test_normalcase(self):\n        t = TextProcessor(phone_number=\"1-111-1111\")\n        keyword, child_name, date = t.get_data_from_message(\"Remind Nathan 25/11/2015\")\n        self.assertEqual(keyword, \"remind\")\n        self.assertEqual(child_name, \"nathan\")\n        self.assertEqual(date, datetime(2015, 11, 25, 0, 0).date())\n\n    def test_date_short_year(self):\n        t = TextProcessor(phone_number=\"1-111-1111\")\n        keyword, child_name, date = t.get_data_from_message(\"REMIND NATHAN 25/11/15\")\n        self.assertEqual(keyword, \"remind\")\n        self.assertEqual(child_name, \"nathan\")\n        self.assertEqual(date, datetime(2015, 11, 25, 0, 0).date())\n\n    def test_date_hypen_separation(self):\n        t = TextProcessor(phone_number=\"1-111-1111\")\n        keyword, child_name, date = t.get_data_from_message(\"REMIND NATHAN 25-11-2015\")\n        self.assertEqual(keyword, \"remind\")\n        self.assertEqual(child_name, \"nathan\")\n        self.assertEqual(date, datetime(2015, 11, 25, 0, 0).date())\n\n    def test_join_keyword(self):\n        t = TextProcessor(phone_number=\"1-111-1111\")\n        keyword, child_name, date = t.get_data_from_message(\"Join Charles 19/12/2012\")\n        self.assertEqual(keyword, \"join\")\n        self.assertEqual(child_name, \"charles\")\n        self.assertEqual(date, datetime(2012, 12, 19, 0, 0).date())\n\n    def test_born_keyword(self):\n        t = TextProcessor(phone_number=\"1-111-1111\")\n        keyword, child_name, date = t.get_data_from_message(\"Born Charles 19/12/2012\")\n        self.assertEqual(keyword, \"born\")\n        self.assertEqual(child_name, \"charles\")\n        self.assertEqual(date, datetime(2012, 12, 19, 0, 0).date())\n\n    def test_hindi_remind(self):\n        t = TextProcessor(phone_number=\"1-111-1111\")\n        keyword, child_name, date = t.get_data_from_message(hindi_remind() + \" Sai 11/09/2013\")\n        self.assertEqual(keyword, hindi_remind())\n        self.assertEqual(child_name, \"sai\")\n        self.assertEqual(date, datetime(2013, 9, 11, 0, 0).date())\n\n    def test_hindi_remind_with_hindi_name(self):\n        t = TextProcessor(phone_number=\"1-111-1111\")\n        keyword, child_name, date = t.get_data_from_message(hindi_remind() + u' \\u0906\\u0930\\u0935 11/09/2013')\n        self.assertEqual(keyword, hindi_remind())\n        self.assertEqual(child_name, u'\\u0906\\u0930\\u0935')\n        self.assertEqual(date, datetime(2013, 9, 11, 0, 0).date())\n\n    def test_hindi_information(self):\n        t = TextProcessor(phone_number=\"1-111-1111\")\n        keyword, child_name, date = t.get_data_from_message(hindi_information() + \" Sai 11/09/2013\")\n        self.assertEqual(keyword, hindi_information())\n        self.assertEqual(child_name, \"sai\")\n        self.assertEqual(date, datetime(2013, 9, 11, 0, 0).date())\n\n    def test_hindi_born(self):\n        t = TextProcessor(phone_number=\"1-111-1111\")\n        keyword, child_name, date = t.get_data_from_message(hindi_born() + \" Sai 11/09/2013\")\n        self.assertEqual(keyword, hindi_born())\n        self.assertEqual(child_name, \"sai\")\n        self.assertEqual(date, datetime(2013, 9, 11, 0, 0).date())\n\n    def test_no_child_name(self):\n        t = TextProcessor(phone_number=\"1-111-1111\")\n        keyword, child_name, date = t.get_data_from_message(\"Remind 25/11/2015\")\n        self.assertEqual(keyword, \"remind\")\n        self.assertEqual(child_name, None)\n        self.assertEqual(date, datetime(2015, 11, 25, 0, 0).date())\n\n    def test_end(self):\n        t = TextProcessor(phone_number=\"1-111-1111\")\n        keyword, child_name, date = t.get_data_from_message(\"END\")\n        self.assertEqual(keyword, \"end\")\n        self.assertEqual(child_name, None)\n        self.assertEqual(date, None)\n\n\nclass TextProcessorProcessTests(TestCase):\n    def create_contact(self, name, phone_number, delay_in_days, language_preference, method_of_sign_up):\n", "next_line": "        return Contact.objects.create(name=name,", "gold_snippet_index": 0, "id": 16, "__internal_uuid__": "5aea8d91-cb4d-4aca-b753-f08bd5c0d906"}
{"repo_name": "opalmer/pywincffi", "file_path": "tests/test_core/test_dist.py", "context": "[{\"identifier\":\"dist\", \"path\":\"pywincffi/core/dist.py\", \"snippet\":\"MODULE_NAME = \\\"_pywincffi\\\"\\nHEADER_FILES = (\\n    resource_filename(\\n        \\\"pywincffi\\\", join(\\\"core\\\", \\\"cdefs\\\", \\\"headers\\\", \\\"typedefs.h\\\")),\\n    resource_filename(\\n        \\\"pywincffi\\\", join(\\\"core\\\", \\\"cdefs\\\", \\\"headers\\\", \\\"constants.h\\\")),\\n    resource_filename(\\n        \\\"pywincffi\\\", join(\\\"core\\\", \\\"cdefs\\\", \\\"headers\\\", \\\"structs.h\\\")),\\n    resource_filename(\\n        \\\"pywincffi\\\", join(\\\"core\\\", \\\"cdefs\\\", \\\"headers\\\", \\\"functions.h\\\")))\\nSOURCE_FILES = (\\n    resource_filename(\\n        \\\"pywincffi\\\", join(\\\"core\\\", \\\"cdefs\\\", \\\"sources\\\", \\\"main.c\\\")), )\\nLIBRARIES = (\\\"kernel32\\\", \\\"user32\\\", \\\"Ws2_32\\\")\\nREGEX_SAL_ANNOTATION = re.compile(\\n    r\\\"\\\\b(_In_|_Inout_|_Out_|_Outptr_|_Reserved_)(opt_)?\\\\b\\\")\\n    _RUNTIME_CONSTANTS = dict(\\n        # Defined here because cffi can't handle negative values\\n        # in constants yet.\\n        INVALID_HANDLE_VALUE=-1,\\n\\n        # The maximum length of the lpCommandLine input to a CreateProcess\\n        # call:\\n        #  https://msdn.microsoft.com/en-us/library/ms682425\\n        # Technically, this is not a Windows constant.  It's something that\\n        # pywincffi defines for ease of use and to limit the possibility of\\n        # typos.\\n        MAX_COMMAND_LINE=32768\\n    )\\nclass LibraryWrapper(object):  # pylint: disable=too-few-public-methods\\nclass Loader(object):\\n    def __init__(self, library):\\n    def __dir__(self):\\n    def __getattribute__(self, item):\\n    def __getattr__(self, item):\\n    def __repr__(self):  # pragma: no cover\\n    def set(cls, ffi, library):\\n    def get(cls):\\ndef _import_path(path, module_name=MODULE_NAME):\\ndef _read(*paths):\\ndef _ffi(\\n        module_name=MODULE_NAME, headers=HEADER_FILES, sources=SOURCE_FILES,\\n        libraries=LIBRARIES):\\ndef _compile(ffi, tmpdir=None, module_name=MODULE_NAME):\\ndef load():\"}, {\"identifier\":\"MODULE_NAME\", \"path\":\"pywincffi/core/dist.py\", \"snippet\":\"MODULE_NAME = \\\"_pywincffi\\\"\"}, {\"identifier\":\"HEADER_FILES\", \"path\":\"pywincffi/core/dist.py\", \"snippet\":\"HEADER_FILES = (\\n    resource_filename(\\n        \\\"pywincffi\\\", join(\\\"core\\\", \\\"cdefs\\\", \\\"headers\\\", \\\"typedefs.h\\\")),\\n    resource_filename(\\n        \\\"pywincffi\\\", join(\\\"core\\\", \\\"cdefs\\\", \\\"headers\\\", \\\"constants.h\\\")),\\n    resource_filename(\\n        \\\"pywincffi\\\", join(\\\"core\\\", \\\"cdefs\\\", \\\"headers\\\", \\\"structs.h\\\")),\\n    resource_filename(\\n        \\\"pywincffi\\\", join(\\\"core\\\", \\\"cdefs\\\", \\\"headers\\\", \\\"functions.h\\\")))\"}, {\"identifier\":\"SOURCE_FILES\", \"path\":\"pywincffi/core/dist.py\", \"snippet\":\"SOURCE_FILES = (\\n    resource_filename(\\n        \\\"pywincffi\\\", join(\\\"core\\\", \\\"cdefs\\\", \\\"sources\\\", \\\"main.c\\\")), )\"}, {\"identifier\":\"LIBRARIES\", \"path\":\"pywincffi/core/dist.py\", \"snippet\":\"LIBRARIES = (\\\"kernel32\\\", \\\"user32\\\", \\\"Ws2_32\\\")\"}, {\"identifier\":\"LibraryWrapper\", \"path\":\"pywincffi/core/dist.py\", \"snippet\":\"class LibraryWrapper(object):  # pylint: disable=too-few-public-methods\\n    \\\"\\\"\\\"\\n    Because of differences between Windows versions and some issues with cffi\\n    we need to wrap the library that cffi produces.  Without this certain\\n    constants can't be included in the lib, such as INVALID_HANDLE_VALUE which\\n    has a negative value.  Other constants, such as FILE_FLAG_SESSION_AWARE,\\n    are not available on all Windows versions so this class helps to provide\\n    a uniform interface.\\n\\n    .. warning::\\n\\n        Please do not define constants here unless absolutely necessary.  By\\n        default, constants should be defined in\\n        :blob:`pywincffi/core/cdefs/headers/constants.h` unless some conditions\\n        are met:\\n            * cffi cannot compile the requested constant.\\n            * The constant is only defined in a few Windows SDK versions and\\n              it can't be conditionally defined in main.c.\\n    \\\"\\\"\\\"\\n    _RUNTIME_CONSTANTS = dict(\\n        # Defined here because cffi can't handle negative values\\n        # in constants yet.\\n        INVALID_HANDLE_VALUE=-1,\\n\\n        # The maximum length of the lpCommandLine input to a CreateProcess\\n        # call:\\n        #  https://msdn.microsoft.com/en-us/library/ms682425\\n        # Technically, this is not a Windows constant.  It's something that\\n        # pywincffi defines for ease of use and to limit the possibility of\\n        # typos.\\n        MAX_COMMAND_LINE=32768\\n    )\\n\\n    def __init__(self, library):\\n        self._library = library\\n\\n    def __dir__(self):\\n        \\\"\\\"\\\"\\n        Overrides the default ``__dir__`` function so functions such as\\n        :func:`dir` return the attributes of the underlying library plus\\n        the runtime constants.\\n        \\\"\\\"\\\"\\n        return dir(self._library) + list(self._RUNTIME_CONSTANTS.keys())\\n\\n    def __getattribute__(self, item):\\n        \\\"\\\"\\\"\\n        Overrides the default ``__getattribute__`` function so that we\\n        can provide more useful results for certain attributes.\\n        \\\"\\\"\\\"\\n        if item == \\\"__dict__\\\":\\n            library_dict = self._library.__dict__.copy()\\n            library_dict.update(self._RUNTIME_CONSTANTS)\\n            return library_dict\\n\\n        return object.__getattribute__(self, item)\\n\\n    def __getattr__(self, item):\\n        \\\"\\\"\\\"\\n        Attempts to retrieve the requested attribute.  This will first look\\n        for the attribute on the library we're wrapping then try to look\\n        for a runtime constant defined on this class.\\n        \\\"\\\"\\\"\\n        # Most likely we're looking for an attribute on the\\n        # compiled library.\\n        try:\\n            return getattr(self._library, item)\\n        except AttributeError as initial_exception:\\n            # Maybe it's a predefined constant?\\n            try:\\n                return self._RUNTIME_CONSTANTS[item]\\n            except KeyError:\\n                pass\\n\\n            # It's not an attribute in either the library or the\\n            # runtime constants so it shouldn't exist.\\n            raise initial_exception\\n\\n    def __repr__(self):  # pragma: no cover\\n        return \\\"%s(%r)\\\" % (self.__class__.__name__, self._library)\"}, {\"identifier\":\"Loader\", \"path\":\"pywincffi/core/dist.py\", \"snippet\":\"class Loader(object):\\n    \\\"\\\"\\\"\\n    A class which provides a cache for :func:`load`.\\n    \\\"\\\"\\\"\\n    cache = None\\n\\n    @classmethod\\n    def set(cls, ffi, library):\\n        \\\"\\\"\\\"\\n        Establishes the cache.\\n\\n        :raises pywincffi.exceptions.InternalError:\\n            Raised if the cache was already setup once.\\n        \\\"\\\"\\\"\\n        if cls.cache is not None:\\n            # Setting up the cache multiple times is an indication of a\\n            # possible bug.\\n            raise InternalError(\\\"The cache has already been established\\\")\\n\\n        cls.cache = (ffi, library)\\n\\n    @classmethod\\n    def get(cls):\\n        \\\"\\\"\\\"\\n        Retrieves the current cache.\\n\\n        :raises pywincffi.exceptions.InternalError:\\n            Raised if an attempt is made to retrieve the cache when it\\n            has not been setup yet.\\n        \\\"\\\"\\\"\\n        if cls.cache is None:\\n            raise InternalError(\\\"The cache has not been established yet\\\")\\n        return cls.cache\"}, {\"identifier\":\"_import_path\", \"path\":\"pywincffi/core/dist.py\", \"snippet\":\"def _import_path(path, module_name=MODULE_NAME):\\n    \\\"\\\"\\\"\\n    Function which imports ``path`` and returns it as a module.  This is\\n    meant to import pyd files produced by :meth:`Distribution._build` in\\n    a Python 2/3 agnostic fashion.\\n\\n    :param str path:\\n        The path to the file to import\\n\\n    :keyword str module_name:\\n        Optional name of the module being imported.  By default\\n        this will use ``_pywincffi`` if no value is provided.\\n\\n    :raises ResourceNotFoundError:\\n        Raised if ``path`` does not exist.\\n    \\\"\\\"\\\"\\n    if not isfile(path):\\n        raise ResourceNotFoundError(\\\"Module path %r does not exist\\\" % path)\\n\\n    elif ExtensionFileLoader is not None:\\n        loader = ExtensionFileLoader(module_name, path)\\n        # pylint: disable=deprecated-method\\n        return loader.load_module(module_name)\\n\\n    elif imp is not None:  # pragma: no cover\\n        return imp.load_dynamic(module_name, path)\\n\\n    else:  # pragma: no cover\\n        raise NotImplementedError(\\n            \\\"Neither `imp` or `ExtensionFileLoader` were imported\\\")\"}, {\"identifier\":\"_ffi\", \"path\":\"pywincffi/core/dist.py\", \"snippet\":\"def _ffi(\\n        module_name=MODULE_NAME, headers=HEADER_FILES, sources=SOURCE_FILES,\\n        libraries=LIBRARIES):\\n    \\\"\\\"\\\"\\n    Returns an instance of :class:`FFI` without compiling\\n    the module.  This function is used internally but also\\n    as an entrypoint in the setup.py for `cffi_modules`.\\n\\n    :keyword str module_name:\\n        Optional module name to use when setting the source.\\n\\n    :keyword tuple headers:\\n        Optional path(s) to the header files.\\n\\n    :keyword tuple sources:\\n        Optional path(s) to the source files.\\n    \\\"\\\"\\\"\\n    header = _read(*headers)\\n    source = _read(*sources)\\n\\n    ffi = FFI()\\n    ffi.set_unicode(True)\\n    ffi.set_source(module_name, source, libraries=libraries)\\n\\n    # Windows uses SAL annotations which can provide some helpful information\\n    # about the inputs and outputs to a function.  Rather than require these\\n    # to be stripped out manually we should strip them out programmatically.\\n    ffi.cdef(REGEX_SAL_ANNOTATION.sub(\\\" \\\", header))\\n\\n    return ffi\"}, {\"identifier\":\"_compile\", \"path\":\"pywincffi/core/dist.py\", \"snippet\":\"def _compile(ffi, tmpdir=None, module_name=MODULE_NAME):\\n    \\\"\\\"\\\"\\n    Performs the compile step, loads the resulting module and then\\n    return it.\\n\\n    :param cffi.FFI ffi:\\n        An instance of :class:`FFI` which you wish to compile and load\\n        the resulting module for.\\n\\n    :keyword str tmpdir:\\n        The path to compile the module to.  By default this will be\\n        constructed using ``tempfile.mkdtemp(prefix=\\\"pywincffi-\\\")``.\\n\\n    :keyword str module_name:\\n        Optional name of the module to be imported.\\n\\n    :returns:\\n        Returns the module built by compiling the ``ffi`` object.\\n    \\\"\\\"\\\"\\n    if tmpdir is None:\\n        tmpdir = tempfile.mkdtemp(prefix=\\\"pywincffi-\\\")\\n\\n    pyd_path = ffi.compile(tmpdir=tmpdir)\\n    module = _import_path(pyd_path, module_name=module_name)\\n\\n    # Try to cleanup the temp directory that was created\\n    # for compiling the module.  In most cases this will\\n    # remove everything but the built .pyd file.\\n    shutil.rmtree(tmpdir, ignore_errors=True)\\n\\n    return module\"}, {\"identifier\":\"_read\", \"path\":\"pywincffi/core/dist.py\", \"snippet\":\"def _read(*paths):\\n    \\\"\\\"\\\"\\n    Iterates over ``files`` and produces string which combines all inputs\\n    into a single string.\\n\\n    :raises ResourceNotFoundError:\\n        Raised if one of the files in ``files`` is missing.\\n    \\\"\\\"\\\"\\n    output = \\\"\\\"\\n    for path in paths:\\n        try:\\n            with open(path, \\\"r\\\") as file_:\\n                output += file_.read()\\n        except (OSError, IOError, WindowsError) as error:\\n            if error.errno == ENOENT:\\n                raise ResourceNotFoundError(\\\"Failed to locate %s\\\" % path)\\n            raise  # pragma: no cover\\n\\n    return output\"}, {\"identifier\":\"load\", \"path\":\"pywincffi/core/dist.py\", \"snippet\":\"def load():\\n    \\\"\\\"\\\"\\n    The main function used by pywincffi to load an instance of\\n    :class:`FFI` and the underlying library.\\n    \\\"\\\"\\\"\\n    try:\\n        return Loader.get()\\n    except InternalError:\\n        try:\\n            import _pywincffi as pywincffi\\n        except ImportError:\\n            pywincffi = _compile(_ffi())\\n\\n        # pylint: disable=no-member\\n        Loader.set(pywincffi.ffi, LibraryWrapper(pywincffi.lib))\\n\\n    return Loader.get()\"}, {\"identifier\":\"TestCase\", \"path\":\"pywincffi/dev/testutil.py\", \"snippet\":\"class TestCase(_TestCase):  # pylint: disable=too-many-public-methods\\n    \\\"\\\"\\\"\\n    A base class for all test cases.  By default the\\n    core test case just provides some extra functionality.\\n    \\\"\\\"\\\"\\n    # A list of hosts and port to check for internet access on.  If we fail\\n    # to reach any of the hosts in `INTERNET_HOSTS` on `INTERNET_PORT` then\\n    # `HAS_INTERNET` will be set to False.\\n    INTERNET_PORT = 80\\n    INTERNET_HOSTS = (\\\"github.com\\\", \\\"readthedocs.org\\\", \\\"example.com\\\")\\n    REQUIRES_INTERNET = False\\n    HAS_INTERNET = None\\n\\n    # Class level attributes used to access some specific Windows API\\n    # functions when testing.  This is kept separate from what `dist.load()`\\n    # produces so problems in the build don't break parts of the base TestCase.\\n    ffi = None\\n    kernel32 = None\\n    ws2_32 = None\\n\\n    @classmethod\\n    def setUpClass(cls):\\n        # Reset everything back to the default values first.\\n        cls.ffi = None\\n        cls.kernel32 = None\\n        cls.ws2_32 = None\\n        cls.HAS_INTERNET = None\\n\\n        # First run and this test case requires internet access.  Determine\\n        # if we have access to the internet then cache the value.\\n        if cls.REQUIRES_INTERNET and SharedState.HAS_INTERNET is None:\\n            original_timeout = socket.getdefaulttimeout()\\n            socket.setdefaulttimeout(1)\\n\\n            try:\\n                for hostname in cls.INTERNET_HOSTS:\\n                    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\\n                    try:\\n                        sock.connect((hostname, cls.INTERNET_PORT))\\n                        SharedState.HAS_INTERNET = True\\n                        break\\n\\n                    # pylint: disable=broad-except\\n                    except Exception:  # pragma: no cover\\n                        pass\\n\\n                    finally:\\n                        sock.close()\\n                else:  # pragma: no cover\\n                    SharedState.HAS_INTERNET = False\\n            finally:\\n                socket.setdefaulttimeout(original_timeout)\\n\\n        if os.name == \\\"nt\\\" and SharedState.ffi is None:\\n            try:\\n                ffi = FFI()\\n                ffi.set_unicode(True)\\n                ffi.cdef(dedent(\\\"\\\"\\\"\\n                // kernel32 functions\\n                DWORD GetLastError(void);\\n                void SetLastError(DWORD);\\n\\n                // ws2_32 functions\\n                void WSASetLastError(int);\\n                int WSAGetLastError(void);\\n                \\\"\\\"\\\"))\\n                SharedState.ffi = ffi\\n                SharedState.kernel32 = ffi.dlopen(\\\"kernel32\\\")\\n                SharedState.ws2_32 = ffi.dlopen(\\\"ws2_32\\\")\\n\\n            # pylint: disable=broad-except\\n            except Exception as error:  # pragma: no cover\\n                if os.name == \\\"nt\\\":\\n                    SharedState.ffi = error\\n\\n        cls.HAS_INTERNET = SharedState.HAS_INTERNET\\n        cls.ffi = SharedState.ffi\\n        cls.kernel32 = SharedState.kernel32\\n        cls.ws2_32 = SharedState.ws2_32\\n\\n    def setUp(self):  # pragma: no cover\\n        if self.REQUIRES_INTERNET and not self.HAS_INTERNET:\\n            if os.environ.get(\\\"CI\\\"):\\n                self.fail(\\n                    \\\"%s requires internet but we do not seem to be \\\"\\n                    \\\"connected.\\\" % self.__class__.__name__)\\n\\n            self.skipTest(\\\"Internet unavailable\\\")\\n\\n        if os.name != \\\"nt\\\":\\n            return\\n\\n        if isinstance(self.ffi, Exception):\\n            self.fail(\\\"FFI module setup failed: %s\\\" % self.ffi)\\n\\n        self.assertIsNotNone(\\n            self.kernel32, \\\"setUp() failed: missing kernel32\\\")\\n        self.assertIsNotNone(\\n            self.ws2_32, \\\"setUp() failed: missing ws2_32\\\")\\n\\n        self.addCleanup(self.unhandled_error_check)\\n\\n        # Always reset the last error to 0 between tests.  This ensures\\n        # that if an unhandled API error occurs it won't impact the\\n        # currently running test.  The cleanup step above will ensure that\\n        # tests that do not exit cleanly will cause a failure.\\n        self.kernel32.SetLastError(0)\\n        self.ws2_32.WSASetLastError(0)\\n\\n    def GetLastError(self):  # pylint: disable=invalid-name\\n        \\\"\\\"\\\"\\n        Returns a tuple containing output from the Windows GetLastError\\n        function and the associated error message.  The error message will\\n        be None if GetLastError() returns 0.\\n        \\\"\\\"\\\"\\n        errno = self.kernel32.GetLastError()\\n        return errno, self.ffi.getwinerror(errno) if errno != 0 else None\\n\\n    def WSAGetLastError(self):  # pylint: disable=invalid-name\\n        \\\"\\\"\\\"\\n        Returns a tuple containing output from the Windows WSAGetLastError\\n        function and the associated error message.  The error message will\\n        be None if WSAGetLastError() returns 0.\\n        \\\"\\\"\\\"\\n        errno = self.ws2_32.WSAGetLastError()\\n        return errno, self.ffi.getwinerror(errno) if errno != 0 else None\\n\\n    def WSASetLastError(self, errno):  # pylint: disable=invalid-name\\n        \\\"\\\"\\\"Wrapper for WSASetLastError()\\\"\\\"\\\"\\n        self.ws2_32.WSASetLastError(errno)\\n\\n    def SetLastError(self, errno):  # pylint: disable=invalid-name\\n        \\\"\\\"\\\"Wrapper for SetLastError()\\\"\\\"\\\"\\n        self.kernel32.SetLastError(errno)\\n\\n    def unhandled_error_check(self):\\n        \\\"\\\"\\\"\\n        A cleanup step which ensures that there are not any uncaught API\\n        errors left over.  Unhandled errors could be a sign of an unhandled\\n        testing artifact, improper API usage or other problem.  In any case,\\n        unhandled errors are often a source of test flake.\\n        \\\"\\\"\\\"\\n        # Check for kernel32 errors.\\n        k32_errno, k32_message = self.GetLastError()\\n        self.assertEqual(\\n            k32_errno, 0,\\n            msg=\\\"Unhandled kernel32 error. Errno: %r. Message: %r\\\" % (\\n                k32_errno, k32_message))\\n\\n        # Check for ws2_32 errors.\\n        ws2_errno, ws2_message = self.WSAGetLastError()\\n        self.assertEqual(\\n            ws2_errno, 0,\\n            msg=\\\"Unhandled ws2_32 error. Errno: %r. Message: %r\\\" % (\\n                ws2_errno, ws2_message))\\n\\n    def _terminate_process(self, process):  # pylint: disable=no-self-use\\n        \\\"\\\"\\\"\\n        Calls terminnate() on ``process`` and ignores any errors produced.\\n        \\\"\\\"\\\"\\n        try:\\n            process.terminate()\\n\\n        # pylint: disable=broad-except\\n        except Exception:  # pragma: no cover\\n            pass\\n\\n    def create_python_process(self, command):\\n        \\\"\\\"\\\"Creates a Python process that run ``command``\\\"\\\"\\\"\\n        process = subprocess.Popen(\\n            [sys.executable, \\\"-c\\\", command],\\n            stdout=subprocess.PIPE, stderr=subprocess.PIPE)\\n        self.addCleanup(self._terminate_process, process)\\n        return process\\n\\n    def random_string(self, length):\\n        \\\"\\\"\\\"\\n        Returns a random string as long as ``length``.  The first character\\n        will always be a letter.  All other characters will be A-F,\\n        A-F or 0-9.\\n        \\\"\\\"\\\"\\n        if length < 1:  # pragma: no cover\\n            self.fail(\\\"Length must be at least 1.\\\")\\n\\n        # First character should always be a letter so the string\\n        # can be used in object names.\\n        output = choice(ascii_lowercase)\\n        length -= 1\\n\\n        while length:\\n            length -= 1\\n            output += choice(ascii_lowercase + ascii_uppercase + \\\"0123456789\\\")\\n\\n        return output\\n\\n    def assert_last_error(self, errno):\\n        \\\"\\\"\\\"\\n        This function will assert that the last unhandled error\\n        was ``errno``.  After the check the last error will be reset to\\n        zero.\\n\\n        :param int errno:\\n            The expected value from GetLastError.\\n        \\\"\\\"\\\"\\n        last_error, _ = self.GetLastError()\\n        self.assertEqual(last_error, errno)\\n        self.SetLastError(0)\\n\\n    def maybe_assert_last_error(self, errno):\\n        \\\"\\\"\\\"\\n        This function is similar to :meth:`assert_last_error` except\\n        it won't fail if the current error number is already 0.\\n        \\\"\\\"\\\"\\n        last_error, _ = self.GetLastError()\\n        self.assertIn(last_error, (0, errno))\\n        self.SetLastError(0)\"}, {\"identifier\":\"ResourceNotFoundError\", \"path\":\"pywincffi/exceptions.py\", \"snippet\":\"class ResourceNotFoundError(InternalError):\\n    \\\"\\\"\\\"Raised when we fail to locate a specific resource\\\"\\\"\\\"\"}, {\"identifier\":\"InternalError\", \"path\":\"pywincffi/exceptions.py\", \"snippet\":\"class InternalError(PyWinCFFIError):\\n    \\\"\\\"\\\"\\n    Raised if we encounter an internal error.  Most likely this is an\\n    indication of a bug in pywincffi but it could also be a problem caused by\\n    an unexpected use case.\\n    \\\"\\\"\\\"\"}]", "import_statement": "import os\nimport shutil\nimport sys\nimport tempfile\nfrom os.path import isfile, dirname\nfrom cffi import FFI\nfrom mock import patch\nfrom pywincffi.core import dist\nfrom pywincffi.core.dist import (\n    MODULE_NAME, HEADER_FILES, SOURCE_FILES, LIBRARIES, LibraryWrapper, Loader,\n    _import_path, _ffi, _compile, _read, load)\nfrom pywincffi.dev.testutil import TestCase\nfrom pywincffi.exceptions import ResourceNotFoundError, InternalError", "code": "        with self.assertRaises(ResourceNotFoundError):\n            _read(path)\n\n\nclass TestFFI(TestCase):\n    \"\"\"Tests for :func:`pywincffi.core.dist._ffi`\"\"\"\n    def setUp(self):\n        self.module_name = self.random_string(16)\n        self.addCleanup(sys.modules.pop, self.module_name, None)\n\n    def test_calls_set_unicode(self):\n        # Certain types require set_unicode to be called so\n        # this test will fail if ffi.set_unicode(True) is never\n        # called in our core library.\n        fd, header_path = tempfile.mkstemp(suffix=\".h\")\n        with os.fdopen(fd, \"w\") as file_:\n            file_.write(\"BOOL Foobar(LPTSTR);\")\n\n        _ffi(module_name=self.module_name, sources=[], headers=[header_path])\n\n    def test_default_source_files(self):\n        with patch.object(FFI, \"set_source\") as mocked_set_source:\n            _ffi(module_name=self.module_name)\n\n        mocked_set_source.assert_called_once_with(\n            self.module_name, _read(*SOURCE_FILES), libraries=LIBRARIES)\n\n    def test_alternate_source_files(self):\n        _, path = tempfile.mkstemp(suffix=\".h\")\n\n        with patch.object(FFI, \"set_source\") as mocked_set_source:\n            _ffi(module_name=self.module_name, sources=[path])\n\n        mocked_set_source.assert_called_once_with(\n            self.module_name, _read(*[path]), libraries=LIBRARIES)\n\n\nclass TestCompile(TestCase):\n    \"\"\"Tests for :func:`pywincffi.core.dist._compile`\"\"\"\n    def setUp(self):\n        super(TestCompile, self).setUp()\n        self.module_name = self.random_string(16)\n        self.addCleanup(sys.modules.pop, self.module_name, None)\n        # self.addCleanup(setattr, Module, \"cache\", None)\n\n    def test_compile(self):\n        # Create fake header\n        fd, header = tempfile.mkstemp(suffix=\".h\")\n        self.addCleanup(os.remove, header)\n        with os.fdopen(fd, \"w\") as file_:\n            file_.write(\"int add(int, int);\")\n\n        # Create fake source\n        fd, source = tempfile.mkstemp(suffix=\".c\")\n        self.addCleanup(os.remove, source)\n        with os.fdopen(fd, \"w\") as file_:\n            file_.write(\n                \"int add(int a, int b) {return a + b;}\")\n\n        ffi = _ffi(\n            module_name=self.module_name, sources=[source], headers=[header])\n        module = _compile(ffi, module_name=self.module_name)\n        self.assertEqual(module.lib.add(1, 2), 3)\n\n    def test_compile_uses_provided_tempdir(self):\n        # Create fake header\n        fd, header = tempfile.mkstemp(suffix=\".h\")\n        self.addCleanup(os.remove, header)\n        with os.fdopen(fd, \"w\") as file_:\n            file_.write(\"int add(int, int);\")\n\n        # Create fake source\n        fd, source = tempfile.mkstemp(suffix=\".c\")\n        self.addCleanup(os.remove, source)\n        with os.fdopen(fd, \"w\") as file_:\n            file_.write(\"int add(int a, int b) {return a + b;}\")\n\n        tmpdir = tempfile.mkdtemp()\n        self.addCleanup(shutil.rmtree, tmpdir, ignore_errors=True)\n\n        ffi = _ffi(\n            module_name=self.module_name, sources=[source], headers=[header])\n        module = _compile(ffi, tmpdir=tmpdir, module_name=self.module_name)\n        self.assertEqual(dirname(module.__file__), tmpdir)\n\n\nclass TestLoad(TestCase):\n    \"\"\"Tests for :func:`pywincffi.core.dist.load`\"\"\"\n    def setUp(self):\n        super(TestLoad, self).setUp()\n        mock = patch.object(Loader, \"cache\", None)\n        mock.start()\n        self.addCleanup(sys.modules.pop, MODULE_NAME, None)\n        self.addCleanup(mock.stop)\n\n    def test_prebuilt(self):\n        class FakeModule(object):\n            ffi = None\n\n            class lib(object):\n                a, b = self.random_string(6), self.random_string(6)\n\n        sys.modules[MODULE_NAME] = FakeModule\n        _, library = load()\n\n        self.assertEqual(library.a, FakeModule.lib.a)\n        self.assertEqual(library.b, FakeModule.lib.b)\n\n    def test_compiled(self):\n        # Python 3.5 changes the behavior of None in sys.modules. So\n        # long as other Python versions pass, skipping this should\n        # be ok.\n        if sys.version_info[0:2] >= (3, 5):\n            self.skipTest(\"Python 3.5 not suppoted in this test\")\n\n        # Setting _pywincffi to None in sys.modules will force\n        # 'import _pywincffi' to fail forcing load() to\n        # compile the module.\n        sys.modules[MODULE_NAME] = None\n\n", "next_line": "        with patch.object(dist, \"_compile\") as mocked:", "gold_snippet_index": 0, "id": 17, "__internal_uuid__": "e56edaf3-4dac-4d13-9835-e3771c5bdd9f"}
{"repo_name": "leapcode/leap_mx", "file_path": "src/leap/mx/vendor/pgpy/packet/packets.py", "context": "[{\"identifier\":\"DSAPriv\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class DSAPriv(PrivKey, DSAPub):\\n    __privfields__ = ('x',)\\n\\n    def __privkey__(self):\\n        params = dsa.DSAParameterNumbers(self.p, self.q, self.g)\\n        pn = dsa.DSAPublicNumbers(self.y, params)\\n        return dsa.DSAPrivateNumbers(self.x, pn).private_key(default_backend())\\n\\n    def _generate(self, key_size):\\n        if any(c != 0 for c in self):  # pragma: no cover\\n            raise PGPError(\\\"key is already populated\\\")\\n\\n        # generate some big numbers!\\n        pk = dsa.generate_private_key(key_size, default_backend())\\n        pkn = pk.private_numbers()\\n\\n        self.p = MPI(pkn.public_numbers.parameter_numbers.p)\\n        self.q = MPI(pkn.public_numbers.parameter_numbers.q)\\n        self.g = MPI(pkn.public_numbers.parameter_numbers.g)\\n        self.y = MPI(pkn.public_numbers.y)\\n        self.x = MPI(pkn.x)\\n\\n        del pkn\\n        del pk\\n\\n        self._compute_chksum()\\n\\n    def parse(self, packet):\\n        super(DSAPriv, self).parse(packet)\\n        self.s2k.parse(packet)\\n\\n        if not self.s2k:\\n            self.x = MPI(packet)\\n\\n        else:\\n            self.encbytes = packet\\n\\n        if self.s2k.usage in [0, 255]:\\n            self.chksum = packet[:2]\\n            del packet[:2]\\n\\n    def decrypt_keyblob(self, passphrase):\\n        kb = super(DSAPriv, self).decrypt_keyblob(passphrase)\\n        del passphrase\\n\\n        self.x = MPI(kb)\\n\\n        if self.s2k.usage in [254, 255]:\\n            self.chksum = kb\\n            del kb\\n\\n    def sign(self, sigdata, hash_alg):\\n        signer = self.__privkey__().signer(hash_alg)\\n        signer.update(sigdata)\\n        return signer.finalize()\"}, {\"identifier\":\"DSAPub\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class DSAPub(PubKey):\\n    __pubfields__ = ('p', 'q', 'g', 'y')\\n\\n    def __pubkey__(self):\\n        params = dsa.DSAParameterNumbers(self.p, self.q, self.g)\\n        return dsa.DSAPublicNumbers(self.y, params).public_key(default_backend())\\n\\n    def verify(self, subj, sigbytes, hash_alg):\\n        verifier = self.__pubkey__().verifier(sigbytes, hash_alg)\\n        verifier.update(subj)\\n\\n        try:\\n            verifier.verify()\\n\\n        except InvalidSignature:\\n            return False\\n\\n        return True\\n\\n    def parse(self, packet):\\n        self.p = MPI(packet)\\n        self.q = MPI(packet)\\n        self.g = MPI(packet)\\n        self.y = MPI(packet)\"}, {\"identifier\":\"DSASignature\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class DSASignature(Signature):\\n    __mpis__ = ('r', 's')\\n\\n    def __sig__(self):\\n        # return the signature data into an ASN.1 sequence of integers in DER format\\n        seq = Sequence()\\n        for i in self:\\n            seq.setComponentByPosition(len(seq), Integer(i))\\n\\n        return encoder.encode(seq)\\n\\n    def from_signer(self, sig):\\n        ##TODO: just use pyasn1 for this\\n        def _der_intf(_asn):\\n            if _asn[0] != 0x02:  # pragma: no cover\\n                raise ValueError(\\\"Expected: Integer (0x02). Got: 0x{:02X}\\\".format(_asn[0]))\\n            del _asn[0]\\n\\n            if _asn[0] & 0x80:  # pragma: no cover\\n                llen = _asn[0] & 0x7F\\n                del _asn[0]\\n\\n                flen = self.bytes_to_int(_asn[:llen])\\n                del _asn[:llen]\\n\\n            else:\\n                flen = _asn[0] & 0x7F\\n                del _asn[0]\\n\\n            i = self.bytes_to_int(_asn[:flen])\\n            del _asn[:flen]\\n            return i\\n\\n        if isinstance(sig, bytes):\\n            sig = bytearray(sig)\\n\\n        # this is a very limited asn1 decoder - it is only intended to decode a DER encoded sequence of integers\\n        if not sig[0] == 0x30:\\n            raise NotImplementedError(\\\"Expected: Sequence (0x30). Got: 0x{:02X}\\\".format(sig[0]))\\n        del sig[0]\\n\\n        # skip the sequence length field\\n        if sig[0] & 0x80:  # pragma: no cover\\n            llen = sig[0] & 0x7F\\n            del sig[:llen + 1]\\n\\n        else:\\n            del sig[0]\\n\\n        self.r = MPI(_der_intf(sig))\\n        self.s = MPI(_der_intf(sig))\\n\\n    def parse(self, packet):\\n        self.r = MPI(packet)\\n        self.s = MPI(packet)\"}, {\"identifier\":\"ECDSAPub\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class ECDSAPub(PubKey):\\n    __pubfields__ = ('x', 'y')\\n\\n    def __init__(self):\\n        super(ECDSAPub, self).__init__()\\n        self.oid = None\\n\\n    def __len__(self):\\n        return sum([len(getattr(self, i)) - 2 for i in self.__pubfields__] +\\n                   [3, len(encoder.encode(self.oid.value)) - 1])\\n\\n    def __pubkey__(self):\\n        return ec.EllipticCurvePublicNumbers(self.x, self.y, self.oid.curve()).public_key(default_backend())\\n\\n    def __bytearray__(self):\\n        _b = bytearray()\\n        _b += encoder.encode(self.oid.value)[1:]\\n        # 0x04 || x || y\\n        # where x and y are the same length\\n        _xy = b'\\\\x04' + self.x.to_mpibytes()[2:] + self.y.to_mpibytes()[2:]\\n        _b += MPI(self.bytes_to_int(_xy, 'big')).to_mpibytes()\\n\\n        return _b\\n\\n    def __copy__(self):\\n        pkt = super(ECDSAPub, self).__copy__()\\n        pkt.oid = self.oid\\n        return pkt\\n\\n    def verify(self, subj, sigbytes, hash_alg):\\n        verifier = self.__pubkey__().verifier(sigbytes, ec.ECDSA(hash_alg))\\n        verifier.update(subj)\\n\\n        try:\\n            verifier.verify()\\n\\n        except InvalidSignature:\\n            return False\\n\\n        return True\\n\\n    def parse(self, packet):\\n        oidlen = packet[0]\\n        del packet[0]\\n        _oid = bytearray(b'\\\\x06')\\n        _oid.append(oidlen)\\n        _oid += bytearray(packet[:oidlen])\\n        # try:\\n        oid, _  = decoder.decode(bytes(_oid))\\n\\n        # except:\\n        #     raise PGPError(\\\"Bad OID octet stream: b'{:s}'\\\".format(''.join(['\\\\\\\\x{:02X}'.format(c) for c in _oid])))\\n        self.oid = EllipticCurveOID(oid)\\n        del packet[:oidlen]\\n\\n        # flen = (self.oid.bit_length // 8)\\n        xy = bytearray(MPI(packet).to_mpibytes()[2:])\\n        # xy = bytearray(MPI(packet).to_bytes(flen, 'big'))\\n        # the first byte is just \\\\x04\\n        del xy[:1]\\n        # now xy needs to be separated into x, y\\n        xylen = len(xy)\\n        x, y = xy[:xylen // 2], xy[xylen // 2:]\\n        self.x = MPI(self.bytes_to_int(x))\\n        self.y = MPI(self.bytes_to_int(y))\"}, {\"identifier\":\"ECDSAPriv\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class ECDSAPriv(PrivKey, ECDSAPub):\\n    __privfields__ = ('s', )\\n\\n    def __privkey__(self):\\n        ecp = ec.EllipticCurvePublicNumbers(self.x, self.y, self.oid.curve())\\n        return ec.EllipticCurvePrivateNumbers(self.s, ecp).private_key(default_backend())\\n\\n    def _generate(self, oid):\\n        if any(c != 0 for c in self):  # pragma: no cover\\n            raise PGPError(\\\"Key is already populated!\\\")\\n\\n        self.oid = EllipticCurveOID(oid)\\n\\n        if not self.oid.can_gen:\\n            raise ValueError(\\\"Curve not currently supported: {}\\\".format(oid.name))\\n\\n        pk = ec.generate_private_key(self.oid.curve(), default_backend())\\n        pubn = pk.public_key().public_numbers()\\n        self.x = MPI(pubn.x)\\n        self.y = MPI(pubn.y)\\n        self.s = MPI(pk.private_numbers().private_value)\\n\\n    def parse(self, packet):\\n        super(ECDSAPriv, self).parse(packet)\\n        self.s2k.parse(packet)\\n\\n        if not self.s2k:\\n            self.s = MPI(packet)\\n\\n            if self.s2k.usage == 0:\\n                self.chksum = packet[:2]\\n                del packet[:2]\\n\\n        else:\\n            ##TODO: this needs to be bounded to the length of the encrypted key material\\n            self.encbytes = packet\\n\\n    def decrypt_keyblob(self, passphrase):\\n        kb = super(ECDSAPriv, self).decrypt_keyblob(passphrase)\\n        del passphrase\\n\\n        self.s = MPI(kb)\\n\\n    def sign(self, sigdata, hash_alg):\\n        signer = self.__privkey__().signer(ec.ECDSA(hash_alg))\\n        signer.update(sigdata)\\n        return signer.finalize()\"}, {\"identifier\":\"ECDSASignature\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class ECDSASignature(DSASignature):\\n    def from_signer(self, sig):\\n        seq, _ = decoder.decode(sig)\\n        self.r = MPI(seq[0])\\n        self.s = MPI(seq[1])\"}, {\"identifier\":\"ECDHPub\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class ECDHPub(PubKey):\\n    __pubfields__ = ('x', 'y')\\n\\n    def __init__(self):\\n        super(ECDHPub, self).__init__()\\n        self.oid = None\\n        self.kdf = ECKDF()\\n\\n    def __len__(self):\\n        return sum([len(getattr(self, i)) - 2 for i in self.__pubfields__] +\\n                   [3,\\n                    len(self.kdf),\\n                    len(encoder.encode(self.oid.value)) - 1])\\n\\n    def __pubkey__(self):\\n        return ec.EllipticCurvePublicNumbers(self.x, self.y, self.oid.curve()).public_key(default_backend())\\n\\n    def __bytearray__(self):\\n        _b = bytearray()\\n        _b += encoder.encode(self.oid.value)[1:]\\n        # 0x04 || x || y\\n        # where x and y are the same length\\n        _xy = b'\\\\x04' + self.x.to_mpibytes()[2:] + self.y.to_mpibytes()[2:]\\n        _b += MPI(self.bytes_to_int(_xy, 'big')).to_mpibytes()\\n        _b += self.kdf.__bytearray__()\\n\\n        return _b\\n\\n    def __copy__(self):\\n        pkt = super(ECDHPub, self).__copy__()\\n        pkt.oid = self.oid\\n        pkt.kdf = copy.copy(self.kdf)\\n        return pkt\\n\\n    def parse(self, packet):\\n        \\\"\\\"\\\"\\n        Algorithm-Specific Fields for ECDH keys:\\n\\n          o  a variable-length field containing a curve OID, formatted\\n             as follows:\\n\\n             -  a one-octet size of the following field; values 0 and\\n                0xFF are reserved for future extensions\\n\\n             -  the octets representing a curve OID, defined in\\n                Section 11\\n\\n             -  MPI of an EC point representing a public key\\n\\n          o  a variable-length field containing KDF parameters,\\n             formatted as follows:\\n\\n             -  a one-octet size of the following fields; values 0 and\\n                0xff are reserved for future extensions\\n\\n             -  a one-octet value 01, reserved for future extensions\\n\\n             -  a one-octet hash function ID used with a KDF\\n\\n             -  a one-octet algorithm ID for the symmetric algorithm\\n                used to wrap the symmetric key used for the message\\n                encryption; see Section 8 for details\\n        \\\"\\\"\\\"\\n        oidlen = packet[0]\\n        del packet[0]\\n        _oid = bytearray(b'\\\\x06')\\n        _oid.append(oidlen)\\n        _oid += bytearray(packet[:oidlen])\\n        # try:\\n        oid, _  = decoder.decode(bytes(_oid))\\n\\n        # except:\\n        #     raise PGPError(\\\"Bad OID octet stream: b'{:s}'\\\".format(''.join(['\\\\\\\\x{:02X}'.format(c) for c in _oid])))\\n        self.oid = EllipticCurveOID(oid)\\n        del packet[:oidlen]\\n\\n        # flen = (self.oid.bit_length // 8)\\n        xy = bytearray(MPI(packet).to_mpibytes()[2:])\\n        # xy = bytearray(MPI(packet).to_bytes(flen, 'big'))\\n        # the first byte is just \\\\x04\\n        del xy[:1]\\n        # now xy needs to be separated into x, y\\n        xylen = len(xy)\\n        x, y = xy[:xylen // 2], xy[xylen // 2:]\\n        self.x = MPI(self.bytes_to_int(x))\\n        self.y = MPI(self.bytes_to_int(y))\\n\\n        self.kdf.parse(packet)\"}, {\"identifier\":\"ECDHPriv\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class ECDHPriv(ECDSAPriv, ECDHPub):\\n    def __bytearray__(self):\\n        _b = ECDHPub.__bytearray__(self)\\n        _b += self.s2k.__bytearray__()\\n        if not self.s2k:\\n            _b += self.s.to_mpibytes()\\n\\n            if self.s2k.usage == 0:\\n                _b += self.chksum\\n\\n        else:\\n            _b += self.encbytes\\n\\n        return _b\\n\\n    def __len__(self):\\n        # because of the inheritance used for this, ECDSAPub.__len__ is called instead of ECDHPub.__len__\\n        # the only real difference is self.kdf, so we can just add that\\n        return super(ECDHPriv, self).__len__() + len(self.kdf)\\n\\n    def _generate(self, oid):\\n        ECDSAPriv._generate(self, oid)\\n        self.kdf.halg = self.oid.kdf_halg\\n        self.kdf.encalg = self.oid.kek_alg\\n\\n    def publen(self):\\n        return ECDHPub.__len__(self)\\n\\n    def parse(self, packet):\\n        ECDHPub.parse(self, packet)\\n        self.s2k.parse(packet)\\n\\n        if not self.s2k:\\n            self.s = MPI(packet)\\n\\n            if self.s2k.usage == 0:\\n                self.chksum = packet[:2]\\n                del packet[:2]\\n\\n        else:\\n            ##TODO: this needs to be bounded to the length of the encrypted key material\\n            self.encbytes = packet\"}, {\"identifier\":\"ECDHCipherText\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class ECDHCipherText(CipherText):\\n    __mpis__ = ('vX', 'vY')\\n\\n    @classmethod\\n    def encrypt(cls, pk, *args):\\n        \\\"\\\"\\\"\\n        For convenience, the synopsis of the encoding method is given below;\\n        however, this section, [NIST-SP800-56A], and [RFC3394] are the\\n        normative sources of the definition.\\n\\n            Obtain the authenticated recipient public key R\\n            Generate an ephemeral key pair {v, V=vG}\\n            Compute the shared point S = vR;\\n            m = symm_alg_ID || session key || checksum || pkcs5_padding;\\n            curve_OID_len = (byte)len(curve_OID);\\n            Param = curve_OID_len || curve_OID || public_key_alg_ID || 03\\n            || 01 || KDF_hash_ID || KEK_alg_ID for AESKeyWrap || \\\"Anonymous\\n            Sender    \\\" || recipient_fingerprint;\\n            Z_len = the key size for the KEK_alg_ID used with AESKeyWrap\\n            Compute Z = KDF( S, Z_len, Param );\\n            Compute C = AESKeyWrap( Z, m ) as per [RFC3394]\\n            VB = convert point V to the octet string\\n            Output (MPI(VB) || len(C) || C).\\n\\n        The decryption is the inverse of the method given.  Note that the\\n        recipient obtains the shared secret by calculating\\n        \\\"\\\"\\\"\\n        # *args should be:\\n        # - m\\n        #\\n        _m, = args\\n\\n        # m may need to be PKCS5-padded\\n        padder = PKCS7(64).padder()\\n        m = padder.update(_m) + padder.finalize()\\n\\n        km = pk.keymaterial\\n\\n        ct = cls()\\n\\n        # generate ephemeral key pair, then store it in ct\\n        v = ec.generate_private_key(km.oid.curve(), default_backend())\\n        ct.vX = MPI(v.public_key().public_numbers().x)\\n        ct.vY = MPI(v.public_key().public_numbers().y)\\n\\n        # compute the shared point S\\n        s = v.exchange(ec.ECDH(), km.__pubkey__())\\n\\n        # derive the wrapping key\\n        z = km.kdf.derive_key(s, km.oid, PubKeyAlgorithm.ECDH, pk.fingerprint)\\n\\n        # compute C\\n        ct.c = aes_key_wrap(z, m, default_backend())\\n\\n        return ct\\n\\n    def decrypt(self, pk, *args):\\n        km = pk.keymaterial\\n        # assemble the public component of ephemeral key v\\n        v = ec.EllipticCurvePublicNumbers(self.vX, self.vY, km.oid.curve()).public_key(default_backend())\\n\\n        # compute s using the inverse of how it was derived during encryption\\n        s = km.__privkey__().exchange(ec.ECDH(), v)\\n\\n        # derive the wrapping key\\n        z = km.kdf.derive_key(s, km.oid, PubKeyAlgorithm.ECDH, pk.fingerprint)\\n\\n        # unwrap and unpad m\\n        _m = aes_key_unwrap(z, self.c, default_backend())\\n\\n        padder = PKCS7(64).unpadder()\\n        return padder.update(_m) + padder.finalize()\\n\\n    def __init__(self):\\n        super(ECDHCipherText, self).__init__()\\n        self.c = bytearray(0)\\n\\n    def __bytearray__(self):\\n        _bytes = bytearray()\\n        _xy = b'\\\\x04' + self.vX.to_mpibytes()[2:] + self.vY.to_mpibytes()[2:]\\n        _bytes += MPI(self.bytes_to_int(_xy, 'big')).to_mpibytes()\\n        _bytes.append(len(self.c))\\n        _bytes += self.c\\n\\n        return _bytes\\n\\n    def parse(self, packet):\\n        # self.v = MPI(packet)\\n        xy = bytearray(MPI(packet).to_mpibytes()[2:])\\n        del xy[:1]\\n        xylen = len(xy)\\n        x, y = xy[:xylen // 2], xy[xylen // 2:]\\n        self.vX = MPI(self.bytes_to_int(x))\\n        self.vY = MPI(self.bytes_to_int(y))\\n\\n        clen = packet[0]\\n        del packet[0]\\n\\n        self.c += packet[:clen]\\n        del packet[:clen]\"}, {\"identifier\":\"ElGCipherText\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class ElGCipherText(CipherText):\\n    __mpis__ = ('gk_mod_p', 'myk_mod_p')\\n\\n    @classmethod\\n    def encrypt(cls, encfn, *args):\\n        raise NotImplementedError()\\n\\n    def decrypt(self, decfn, *args):\\n        raise NotImplementedError()\\n\\n    def parse(self, packet):\\n        self.gk_mod_p = MPI(packet)\\n        self.myk_mod_p = MPI(packet)\"}, {\"identifier\":\"ElGPriv\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class ElGPriv(PrivKey, ElGPub):\\n    __privfields__ = ('x', )\\n\\n    def __privkey__(self):\\n        raise NotImplementedError()\\n\\n    def _generate(self, key_size):\\n        raise NotImplementedError(PubKeyAlgorithm.ElGamal)\\n\\n    def parse(self, packet):\\n        super(ElGPriv, self).parse(packet)\\n        self.s2k.parse(packet)\\n\\n        if not self.s2k:\\n            self.x = MPI(packet)\\n\\n        else:\\n            self.encbytes = packet\\n\\n        if self.s2k.usage in [0, 255]:\\n            self.chksum = packet[:2]\\n            del packet[:2]\\n\\n    def decrypt_keyblob(self, passphrase):\\n        kb = super(ElGPriv, self).decrypt_keyblob(passphrase)\\n        del passphrase\\n\\n        self.x = MPI(kb)\\n\\n        if self.s2k.usage in [254, 255]:\\n            self.chksum = kb\\n            del kb\"}, {\"identifier\":\"ElGPub\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class ElGPub(PubKey):\\n    __pubfields__ = ('p', 'g', 'y')\\n\\n    def __pubkey__(self):\\n        raise NotImplementedError()\\n\\n    def parse(self, packet):\\n        self.p = MPI(packet)\\n        self.g = MPI(packet)\\n        self.y = MPI(packet)\"}, {\"identifier\":\"OpaquePubKey\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class OpaquePubKey(PubKey):  # pragma: no cover\\n    def __init__(self):\\n        super(OpaquePubKey, self).__init__()\\n        self.data = bytearray()\\n\\n    def __iter__(self):\\n        yield self.data\\n\\n    def __pubkey__(self):\\n        return NotImplemented\\n\\n    def __bytearray__(self):\\n        return self.data\\n\\n    def parse(self, packet):\\n        ##TODO: this needs to be length-bounded to the end of the packet\\n        self.data = packet\"}, {\"identifier\":\"OpaquePrivKey\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class OpaquePrivKey(PrivKey, OpaquePubKey):  # pragma: no cover\\n    def __privkey__(self):\\n        return NotImplemented\\n\\n    def _generate(self, key_size):\\n        # return NotImplemented\\n        raise NotImplementedError()\\n\\n    def decrypt_keyblob(self, passphrase):\\n        return NotImplemented\"}, {\"identifier\":\"RSACipherText\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class RSACipherText(CipherText):\\n    __mpis__ = ('me_mod_n', )\\n\\n    @classmethod\\n    def encrypt(cls, encfn, *args):\\n        ct = cls()\\n        ct.me_mod_n = MPI(cls.bytes_to_int(encfn(*args)))\\n        return ct\\n\\n    def decrypt(self, decfn, *args):\\n        return decfn(*args)\\n\\n    def parse(self, packet):\\n        self.me_mod_n = MPI(packet)\"}, {\"identifier\":\"RSAPriv\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class RSAPriv(PrivKey, RSAPub):\\n    __privfields__ = ('d', 'p', 'q', 'u')\\n\\n    def __privkey__(self):\\n        return rsa.RSAPrivateNumbers(self.p, self.q, self.d,\\n                                     rsa.rsa_crt_dmp1(self.d, self.p),\\n                                     rsa.rsa_crt_dmq1(self.d, self.q),\\n                                     rsa.rsa_crt_iqmp(self.p, self.q),\\n                                     rsa.RSAPublicNumbers(self.e, self.n)).private_key(default_backend())\\n\\n    def _generate(self, key_size):\\n        if any(c != 0 for c in self):  # pragma: no cover\\n            raise PGPError(\\\"key is already populated\\\")\\n\\n        # generate some big numbers!\\n        pk = rsa.generate_private_key(65537, key_size, default_backend())\\n        pkn = pk.private_numbers()\\n\\n        self.n = MPI(pkn.public_numbers.n)\\n        self.e = MPI(pkn.public_numbers.e)\\n        self.d = MPI(pkn.d)\\n        self.p = MPI(pkn.p)\\n        self.q = MPI(pkn.q)\\n        # from the RFC:\\n        # \\\"- MPI of u, the multiplicative inverse of p, mod q.\\\"\\n        # or, simply, p^-1 mod p\\n        # rsa.rsa_crt_iqmp(p, q) normally computes q^-1 mod p,\\n        # so if we swap the values around we get the answer we want\\n        self.u = MPI(rsa.rsa_crt_iqmp(pkn.q, pkn.p))\\n\\n        del pkn\\n        del pk\\n\\n        self._compute_chksum()\\n\\n    def parse(self, packet):\\n        super(RSAPriv, self).parse(packet)\\n        self.s2k.parse(packet)\\n\\n        if not self.s2k:\\n            self.d = MPI(packet)\\n            self.p = MPI(packet)\\n            self.q = MPI(packet)\\n            self.u = MPI(packet)\\n\\n            if self.s2k.usage == 0:\\n                self.chksum = packet[:2]\\n                del packet[:2]\\n\\n        else:\\n            ##TODO: this needs to be bounded to the length of the encrypted key material\\n            self.encbytes = packet\\n\\n    def decrypt_keyblob(self, passphrase):\\n        kb = super(RSAPriv, self).decrypt_keyblob(passphrase)\\n        del passphrase\\n\\n        self.d = MPI(kb)\\n        self.p = MPI(kb)\\n        self.q = MPI(kb)\\n        self.u = MPI(kb)\\n\\n        if self.s2k.usage in [254, 255]:\\n            self.chksum = kb\\n            del kb\\n\\n    def sign(self, sigdata, hash_alg):\\n        signer = self.__privkey__().signer(padding.PKCS1v15(), hash_alg)\\n        signer.update(sigdata)\\n        return signer.finalize()\"}, {\"identifier\":\"RSAPub\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class RSAPub(PubKey):\\n    __pubfields__ = ('n', 'e')\\n\\n    def __pubkey__(self):\\n        return rsa.RSAPublicNumbers(self.e, self.n).public_key(default_backend())\\n\\n    def verify(self, subj, sigbytes, hash_alg):\\n        # zero-pad sigbytes if necessary\\n        sigbytes = (b'\\\\x00' * (self.n.byte_length() - len(sigbytes))) + sigbytes\\n        verifier = self.__pubkey__().verifier(sigbytes, padding.PKCS1v15(), hash_alg)\\n        verifier.update(subj)\\n\\n        try:\\n            verifier.verify()\\n\\n        except InvalidSignature:\\n            return False\\n\\n        return True\\n\\n    def parse(self, packet):\\n        self.n = MPI(packet)\\n        self.e = MPI(packet)\"}, {\"identifier\":\"RSASignature\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class RSASignature(Signature):\\n    __mpis__ = ('md_mod_n', )\\n\\n    def __sig__(self):\\n        return self.md_mod_n.to_mpibytes()[2:]\\n\\n    def parse(self, packet):\\n        self.md_mod_n = MPI(packet)\\n\\n    def from_signer(self, sig):\\n        self.md_mod_n = MPI(self.bytes_to_int(sig))\"}, {\"identifier\":\"String2Key\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class String2Key(Field):\\n    \\\"\\\"\\\"\\n    3.7.  String-to-Key (S2K) Specifiers\\n\\n    String-to-key (S2K) specifiers are used to convert passphrase strings\\n    into symmetric-key encryption/decryption keys.  They are used in two\\n    places, currently: to encrypt the secret part of private keys in the\\n    private keyring, and to convert passphrases to encryption keys for\\n    symmetrically encrypted messages.\\n\\n    3.7.1.  String-to-Key (S2K) Specifier Types\\n\\n    There are three types of S2K specifiers currently supported, and\\n    some reserved values:\\n\\n       ID          S2K Type\\n       --          --------\\n       0           Simple S2K\\n       1           Salted S2K\\n       2           Reserved value\\n       3           Iterated and Salted S2K\\n       100 to 110  Private/Experimental S2K\\n\\n    These are described in Sections 3.7.1.1 - 3.7.1.3.\\n\\n    3.7.1.1.  Simple S2K\\n\\n    This directly hashes the string to produce the key data.  See below\\n    for how this hashing is done.\\n\\n       Octet 0:        0x00\\n       Octet 1:        hash algorithm\\n\\n    Simple S2K hashes the passphrase to produce the session key.  The\\n    manner in which this is done depends on the size of the session key\\n    (which will depend on the cipher used) and the size of the hash\\n    algorithm's output.  If the hash size is greater than the session key\\n    size, the high-order (leftmost) octets of the hash are used as the\\n    key.\\n\\n    If the hash size is less than the key size, multiple instances of the\\n    hash context are created -- enough to produce the required key data.\\n    These instances are preloaded with 0, 1, 2, ... octets of zeros (that\\n    is to say, the first instance has no preloading, the second gets\\n    preloaded with 1 octet of zero, the third is preloaded with two\\n    octets of zeros, and so forth).\\n\\n    As the data is hashed, it is given independently to each hash\\n    context.  Since the contexts have been initialized differently, they\\n    will each produce different hash output.  Once the passphrase is\\n    hashed, the output data from the multiple hashes is concatenated,\\n    first hash leftmost, to produce the key data, with any excess octets\\n    on the right discarded.\\n\\n    3.7.1.2.  Salted S2K\\n\\n    This includes a \\\"salt\\\" value in the S2K specifier -- some arbitrary\\n    data -- that gets hashed along with the passphrase string, to help\\n    prevent dictionary attacks.\\n\\n       Octet 0:        0x01\\n       Octet 1:        hash algorithm\\n       Octets 2-9:     8-octet salt value\\n\\n    Salted S2K is exactly like Simple S2K, except that the input to the\\n    hash function(s) consists of the 8 octets of salt from the S2K\\n    specifier, followed by the passphrase.\\n\\n    3.7.1.3.  Iterated and Salted S2K\\n\\n    This includes both a salt and an octet count.  The salt is combined\\n    with the passphrase and the resulting value is hashed repeatedly.\\n    This further increases the amount of work an attacker must do to try\\n    dictionary attacks.\\n\\n       Octet  0:        0x03\\n       Octet  1:        hash algorithm\\n       Octets 2-9:      8-octet salt value\\n       Octet  10:       count, a one-octet, coded value\\n\\n    The count is coded into a one-octet number using the following\\n    formula:\\n\\n       #define EXPBIAS 6\\n           count = ((Int32)16 + (c & 15)) << ((c >> 4) + EXPBIAS);\\n\\n    The above formula is in C, where \\\"Int32\\\" is a type for a 32-bit\\n    integer, and the variable \\\"c\\\" is the coded count, Octet 10.\\n\\n    Iterated-Salted S2K hashes the passphrase and salt data multiple\\n    times.  The total number of octets to be hashed is specified in the\\n    encoded count in the S2K specifier.  Note that the resulting count\\n    value is an octet count of how many octets will be hashed, not an\\n    iteration count.\\n\\n    Initially, one or more hash contexts are set up as with the other S2K\\n    algorithms, depending on how many octets of key data are needed.\\n    Then the salt, followed by the passphrase data, is repeatedly hashed\\n    until the number of octets specified by the octet count has been\\n    hashed.  The one exception is that if the octet count is less than\\n    the size of the salt plus passphrase, the full salt plus passphrase\\n    will be hashed even though that is greater than the octet count.\\n    After the hashing is done, the data is unloaded from the hash\\n    context(s) as with the other S2K algorithms.\\n    \\\"\\\"\\\"\\n    @sdproperty\\n    def encalg(self):\\n        return self._encalg\\n\\n    @encalg.register(int)\\n    @encalg.register(SymmetricKeyAlgorithm)\\n    def encalg_int(self, val):\\n        self._encalg = SymmetricKeyAlgorithm(val)\\n\\n    @sdproperty\\n    def specifier(self):\\n        return self._specifier\\n\\n    @specifier.register(int)\\n    @specifier.register(String2KeyType)\\n    def specifier_int(self, val):\\n        self._specifier = String2KeyType(val)\\n\\n    @sdproperty\\n    def halg(self):\\n        return self._halg\\n\\n    @halg.register(int)\\n    @halg.register(HashAlgorithm)\\n    def halg_int(self, val):\\n        self._halg = HashAlgorithm(val)\\n\\n    @sdproperty\\n    def count(self):\\n        return (16 + (self._count & 15)) << ((self._count >> 4) + 6)\\n\\n    @count.register(int)\\n    def count_int(self, val):\\n        if val < 0 or val > 255:  # pragma: no cover\\n            raise ValueError(\\\"count must be between 0 and 256\\\")\\n        self._count = val\\n\\n    def __init__(self):\\n        super(String2Key, self).__init__()\\n        self.usage = 0\\n        self.encalg = 0\\n        self.specifier = 0\\n        self.iv = None\\n\\n        # specifier-specific fields\\n        # simple, salted, iterated\\n        self.halg = 0\\n\\n        # salted, iterated\\n        self.salt = bytearray()\\n\\n        # iterated\\n        self.count = 0\\n\\n    def __bytearray__(self):\\n        _bytes = bytearray()\\n        _bytes.append(self.usage)\\n        if bool(self):\\n            _bytes.append(self.encalg)\\n            _bytes.append(self.specifier)\\n            if self.specifier >= String2KeyType.Simple:\\n                _bytes.append(self.halg)\\n            if self.specifier >= String2KeyType.Salted:\\n                _bytes += self.salt\\n            if self.specifier == String2KeyType.Iterated:\\n                _bytes.append(self._count)\\n            if self.iv is not None:\\n                _bytes += self.iv\\n        return _bytes\\n\\n    def __len__(self):\\n        return len(self.__bytearray__())\\n\\n    def __bool__(self):\\n        return self.usage in [254, 255]\\n\\n    def __nonzero__(self):\\n        return self.__bool__()\\n\\n    def __copy__(self):\\n        s2k = String2Key()\\n        s2k.usage = self.usage\\n        s2k.encalg = self.encalg\\n        s2k.specifier = self.specifier\\n        s2k.iv = self.iv\\n        s2k.halg = self.halg\\n        s2k.salt = copy.copy(self.salt)\\n        s2k.count = self._count\\n        return s2k\\n\\n    def parse(self, packet, iv=True):\\n        self.usage = packet[0]\\n        del packet[0]\\n\\n        if bool(self):\\n            self.encalg = packet[0]\\n            del packet[0]\\n\\n            self.specifier = packet[0]\\n            del packet[0]\\n\\n            if self.specifier >= String2KeyType.Simple:\\n                # this will always be true\\n                self.halg = packet[0]\\n                del packet[0]\\n\\n            if self.specifier >= String2KeyType.Salted:\\n                self.salt = packet[:8]\\n                del packet[:8]\\n\\n            if self.specifier == String2KeyType.Iterated:\\n                self.count = packet[0]\\n                del packet[0]\\n\\n            if iv:\\n                self.iv = packet[:(self.encalg.block_size // 8)]\\n                del packet[:(self.encalg.block_size // 8)]\\n\\n    def derive_key(self, passphrase):\\n        ##TODO: raise an exception if self.usage is not 254 or 255\\n        keylen = self.encalg.key_size\\n        hashlen = self.halg.digest_size * 8\\n\\n        ctx = int(math.ceil((keylen / hashlen)))\\n\\n        # Simple S2K - always done\\n        hsalt = b''\\n        hpass = passphrase.encode('latin-1')\\n\\n        # salted, iterated S2K\\n        if self.specifier >= String2KeyType.Salted:\\n            hsalt = bytes(self.salt)\\n\\n        count = len(hsalt + hpass)\\n        if self.specifier == String2KeyType.Iterated and self.count > len(hsalt + hpass):\\n            count = self.count\\n\\n        hcount = (count // len(hsalt + hpass))\\n        hleft = count - (hcount * len(hsalt + hpass))\\n\\n        hashdata = ((hsalt + hpass) * hcount) + (hsalt + hpass)[:hleft]\\n\\n        h = []\\n        for i in range(0, ctx):\\n            _h = self.halg.hasher\\n            _h.update(b'\\\\x00' * i)\\n            _h.update(hashdata)\\n            h.append(_h)\\n\\n        # GC some stuff\\n        del hsalt\\n        del hpass\\n        del hashdata\\n\\n        # and return the key!\\n        return b''.join(hc.digest() for hc in h)[:(keylen // 8)]\"}, {\"identifier\":\"SubPackets\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class SubPackets(collections.MutableMapping, Field):\\n    _spmodule = signature\\n\\n    def __init__(self):\\n        super(SubPackets, self).__init__()\\n        self._hashed_sp = collections.OrderedDict()\\n        self._unhashed_sp = collections.OrderedDict()\\n\\n    def __bytearray__(self):\\n        _bytes = bytearray()\\n        _bytes += self.__hashbytearray__()\\n        _bytes += self.__unhashbytearray__()\\n        return _bytes\\n\\n    def __hashbytearray__(self):\\n        _bytes = bytearray()\\n        _bytes += self.int_to_bytes(sum(len(sp) for sp in self._hashed_sp.values()), 2)\\n        for hsp in self._hashed_sp.values():\\n            _bytes += hsp.__bytearray__()\\n        return _bytes\\n\\n    def __unhashbytearray__(self):\\n        _bytes = bytearray()\\n        _bytes += self.int_to_bytes(sum(len(sp) for sp in self._unhashed_sp.values()), 2)\\n        for uhsp in self._unhashed_sp.values():\\n            _bytes += uhsp.__bytearray__()\\n        return _bytes\\n\\n    def __len__(self):  # pragma: no cover\\n        return sum(sp.header.length for sp in itertools.chain(self._hashed_sp.values(), self._unhashed_sp.values())) + 4\\n\\n    def __iter__(self):\\n        for sp in itertools.chain(self._hashed_sp.values(), self._unhashed_sp.values()):\\n            yield sp\\n\\n    def __setitem__(self, key, val):\\n        # the key provided should always be the classname for the subpacket\\n        # but, there can be multiple subpackets of the same type\\n        # so, it should be stored in the format: [h_]<key>_<seqid>\\n        # where:\\n        #  - <key> is the classname of val\\n        #  - <seqid> is a sequence id, starting at 0, for a given classname\\n\\n        i = 0\\n        if isinstance(key, tuple):  # pragma: no cover\\n            key, i = key\\n\\n        d = self._unhashed_sp\\n        if key.startswith('h_'):\\n            d, key = self._hashed_sp, key[2:]\\n\\n        while (key, i) in d:\\n            i += 1\\n\\n        d[(key, i)] = val\\n\\n    def __getitem__(self, key):\\n        if isinstance(key, tuple):  # pragma: no cover\\n            return self._hashed_sp.get(key, self._unhashed_sp.get(key))\\n\\n        if key.startswith('h_'):\\n            return [v for k, v in self._hashed_sp.items() if key[2:] == k[0]]\\n\\n        else:\\n            return [v for k, v in itertools.chain(self._hashed_sp.items(), self._unhashed_sp.items()) if key == k[0]]\\n\\n    def __delitem__(self, key):\\n        ##TODO: this\\n        raise NotImplementedError\\n\\n    def __contains__(self, key):\\n        return key in set(k for k, _ in itertools.chain(self._hashed_sp, self._unhashed_sp))\\n\\n    def __copy__(self):\\n        sp = SubPackets()\\n        sp._hashed_sp = self._hashed_sp.copy()\\n        sp._unhashed_sp = self._unhashed_sp.copy()\\n\\n        return sp\\n\\n    def addnew(self, spname, hashed=False, **kwargs):\\n        nsp = getattr(self._spmodule, spname)()\\n        for p, v in kwargs.items():\\n            if hasattr(nsp, p):\\n                setattr(nsp, p, v)\\n        nsp.update_hlen()\\n        if hashed:\\n            self['h_' + spname] = nsp\\n\\n        else:\\n            self[spname] = nsp\\n\\n    def update_hlen(self):\\n        for sp in self:\\n            sp.update_hlen()\\n\\n    def parse(self, packet):\\n        hl = self.bytes_to_int(packet[:2])\\n        del packet[:2]\\n\\n        # we do it this way because we can't ensure that subpacket headers are sized appropriately\\n        # for their contents, but we can at least output that correctly\\n        # so instead of tracking how many bytes we can now output, we track how many bytes have we parsed so far\\n        plen = len(packet)\\n        while plen - len(packet) < hl:\\n            sp = SignatureSP(packet)\\n            self['h_' + sp.__class__.__name__] = sp\\n\\n        uhl = self.bytes_to_int(packet[:2])\\n        del packet[:2]\\n\\n        plen = len(packet)\\n        while plen - len(packet) < uhl:\\n            sp = SignatureSP(packet)\\n            self[sp.__class__.__name__] = sp\"}, {\"identifier\":\"UserAttributeSubPackets\", \"path\":\"src/leap/mx/vendor/pgpy/packet/fields.py\", \"snippet\":\"class UserAttributeSubPackets(SubPackets):\\n    \\\"\\\"\\\"\\n    This is nearly the same as just the unhashed subpackets from above,\\n    except that there isn't a length specifier. So, parse will only parse one packet,\\n    appending that one packet to self.__unhashed_sp.\\n    \\\"\\\"\\\"\\n    _spmodule = userattribute\\n\\n    def __bytearray__(self):\\n        _bytes = bytearray()\\n        for uhsp in self._unhashed_sp.values():\\n            _bytes += uhsp.__bytearray__()\\n        return _bytes\\n\\n    def __len__(self):  # pragma: no cover\\n        return sum(len(sp) for sp in self._unhashed_sp.values())\\n\\n    def parse(self, packet):\\n        # parse just one packet and add it to the unhashed subpacket ordereddict\\n        # I actually have yet to come across a User Attribute packet with more than one subpacket\\n        # which makes sense, given that there is only one defined subpacket\\n        sp = UserAttribute(packet)\\n        self[sp.__class__.__name__] = sp\"}, {\"identifier\":\"Packet\", \"path\":\"src/leap/mx/vendor/pgpy/packet/types.py\", \"snippet\":\"class Packet(Dispatchable):\\n    __typeid__ = -1\\n    __headercls__ = Header\\n\\n    def __init__(self):\\n        super(Packet, self).__init__()\\n        self.header = self.__headercls__()\\n        if isinstance(self.__typeid__, six.integer_types):\\n            self.header.tag = self.__typeid__\\n\\n    @abc.abstractmethod\\n    def __bytearray__(self):\\n        return self.header.__bytearray__()\\n\\n    def __len__(self):\\n        return len(self.header) + self.header.length\\n\\n    def __repr__(self):\\n        return \\\"<{cls:s} [tag 0x{tag:02d}] at 0x{id:x}>\\\".format(cls=self.__class__.__name__, tag=self.header.tag, id=id(self))\\n\\n    def update_hlen(self):\\n        self.header.length = len(self.__bytearray__()) - len(self.header)\\n\\n    @abc.abstractmethod\\n    def parse(self, packet):\\n        if self.header.tag == 0:\\n            self.header.parse(packet)\"}, {\"identifier\":\"Primary\", \"path\":\"src/leap/mx/vendor/pgpy/packet/types.py\", \"snippet\":\"class Primary(Key):\\n    pass\"}, {\"identifier\":\"Private\", \"path\":\"src/leap/mx/vendor/pgpy/packet/types.py\", \"snippet\":\"class Private(Key):\\n    pass\"}, {\"identifier\":\"Public\", \"path\":\"src/leap/mx/vendor/pgpy/packet/types.py\", \"snippet\":\"class Public(Key):\\n    pass\"}, {\"identifier\":\"Sub\", \"path\":\"src/leap/mx/vendor/pgpy/packet/types.py\", \"snippet\":\"class Sub(Key):\\n    pass\"}, {\"identifier\":\"VersionedPacket\", \"path\":\"src/leap/mx/vendor/pgpy/packet/types.py\", \"snippet\":\"class VersionedPacket(Packet):\\n    __headercls__ = VersionedHeader\\n\\n    def __init__(self):\\n        super(VersionedPacket, self).__init__()\\n        if isinstance(self.__ver__, six.integer_types):\\n            self.header.version = self.__ver__\\n\\n    def __repr__(self):\\n        return \\\"<{cls:s} [tag 0x{tag:02d}][v{ver:d}] at 0x{id:x}>\\\".format(cls=self.__class__.__name__, tag=self.header.tag,\\n                                                                          ver=self.header.version, id=id(self))\"}, {\"identifier\":\"CompressionAlgorithm\", \"path\":\"src/leap/mx/vendor/pgpy/constants.py\", \"snippet\":\"class CompressionAlgorithm(IntEnum):\\n    #: No compression\\n    Uncompressed = 0x00\\n    #: ZIP DEFLATE\\n    ZIP = 0x01\\n    #: ZIP DEFLATE with zlib headers\\n    ZLIB = 0x02\\n    #: Bzip2\\n    BZ2 = 0x03\\n\\n    def compress(self, data):\\n        if self is CompressionAlgorithm.Uncompressed:\\n            return data\\n\\n        if self is CompressionAlgorithm.ZIP:\\n            return zlib.compress(data)[2:-4]\\n\\n        if self is CompressionAlgorithm.ZLIB:\\n            return zlib.compress(data)\\n\\n        if self is CompressionAlgorithm.BZ2:\\n            return bz2.compress(data)\\n\\n        raise NotImplementedError(self)\\n\\n    def decompress(self, data):\\n        if six.PY2:\\n            data = bytes(data)\\n\\n        if self is CompressionAlgorithm.Uncompressed:\\n            return data\\n\\n        if self is CompressionAlgorithm.ZIP:\\n            return zlib.decompress(data, -15)\\n\\n        if self is CompressionAlgorithm.ZLIB:\\n            return zlib.decompress(data)\\n\\n        if self is CompressionAlgorithm.BZ2:\\n            return bz2.decompress(data)\\n\\n        raise NotImplementedError(self)\"}, {\"identifier\":\"HashAlgorithm\", \"path\":\"src/leap/mx/vendor/pgpy/constants.py\", \"snippet\":\"class HashAlgorithm(IntEnum):\\n    Invalid = 0x00\\n    MD5 = 0x01\\n    SHA1 = 0x02\\n    RIPEMD160 = 0x03\\n    _reserved_1 = 0x04\\n    _reserved_2 = 0x05\\n    _reserved_3 = 0x06\\n    _reserved_4 = 0x07\\n    SHA256 = 0x08\\n    SHA384 = 0x09\\n    SHA512 = 0x0A\\n    SHA224 = 0x0B\\n\\n    def __init__(self, *args):\\n        super(self.__class__, self).__init__()\\n        self._tuned_count = 0\\n\\n    @property\\n    def hasher(self):\\n        return hashlib.new(self.name)\\n\\n    @property\\n    def digest_size(self):\\n        return self.hasher.digest_size\\n\\n    @property\\n    def tuned_count(self):\\n        if self._tuned_count == 0:\\n            self.tune_count()\\n\\n        return self._tuned_count\\n\\n    def tune_count(self):\\n        start = end = 0\\n        htd = _hashtunedata[:]\\n\\n        while start == end:\\n            # potentially do this multiple times in case the resolution of time.time is low enough that\\n            # hashing 100 KiB isn't enough time to produce a measurable difference\\n            # (e.g. if the timer for time.time doesn't have enough precision)\\n            htd = htd + htd\\n            h = self.hasher\\n\\n            start = time.time()\\n            h.update(htd)\\n            end = time.time()\\n\\n        # now calculate how many bytes need to be hashed to reach our expected time period\\n        # GnuPG tunes for about 100ms, so we'll do that as well\\n        _TIME = 0.100\\n        ct = int(len(htd) * (_TIME / (end - start)))\\n        c1 = ((ct >> (ct.bit_length() - 5)) - 16)\\n        c2 = (ct.bit_length() - 11)\\n        c = ((c2 << 4) + c1)\\n\\n        # constrain self._tuned_count to be between 0 and 255\\n        self._tuned_count = max(min(c, 255), 0)\"}, {\"identifier\":\"PubKeyAlgorithm\", \"path\":\"src/leap/mx/vendor/pgpy/constants.py\", \"snippet\":\"class PubKeyAlgorithm(IntEnum):\\n    Invalid = 0x00\\n    #: Signifies that a key is an RSA key.\\n    RSAEncryptOrSign = 0x01\\n    RSAEncrypt = 0x02  # deprecated\\n    RSASign = 0x03     # deprecated\\n    #: Signifies that a key is an ElGamal key.\\n    ElGamal = 0x10\\n    #: Signifies that a key is a DSA key.\\n    DSA = 0x11\\n    #: Signifies that a key is an ECDH key.\\n    ECDH = 0x12\\n    #: Signifies that a key is an ECDSA key.\\n    ECDSA = 0x13\\n    FormerlyElGamalEncryptOrSign = 0x14  # deprecated - do not generate\\n    # DiffieHellman = 0x15  # X9.42\\n\\n    @property\\n    def can_gen(self):\\n        return self in {PubKeyAlgorithm.RSAEncryptOrSign,\\n                        PubKeyAlgorithm.DSA,\\n                        PubKeyAlgorithm.ECDSA,\\n                        PubKeyAlgorithm.ECDH}\\n\\n    @property\\n    def can_encrypt(self):  # pragma: no cover\\n        return self in {PubKeyAlgorithm.RSAEncryptOrSign, PubKeyAlgorithm.ElGamal, PubKeyAlgorithm.ECDH}\\n\\n    @property\\n    def can_sign(self):\\n        return self in {PubKeyAlgorithm.RSAEncryptOrSign, PubKeyAlgorithm.DSA, PubKeyAlgorithm.ECDSA}\\n\\n    @property\\n    def deprecated(self):\\n        return self in {PubKeyAlgorithm.RSAEncrypt,\\n                        PubKeyAlgorithm.RSASign,\\n                        PubKeyAlgorithm.FormerlyElGamalEncryptOrSign}\"}, {\"identifier\":\"SignatureType\", \"path\":\"src/leap/mx/vendor/pgpy/constants.py\", \"snippet\":\"class SignatureType(IntEnum):\\n    BinaryDocument = 0x00\\n    CanonicalDocument = 0x01\\n    Standalone = 0x02\\n    Generic_Cert = 0x10\\n    Persona_Cert = 0x11\\n    Casual_Cert = 0x12\\n    Positive_Cert = 0x13\\n    Subkey_Binding = 0x18\\n    PrimaryKey_Binding = 0x19\\n    DirectlyOnKey = 0x1F\\n    KeyRevocation = 0x20\\n    SubkeyRevocation = 0x28\\n    CertRevocation = 0x30\\n    Timestamp = 0x40\\n    ThirdParty_Confirmation = 0x50\"}, {\"identifier\":\"SymmetricKeyAlgorithm\", \"path\":\"src/leap/mx/vendor/pgpy/constants.py\", \"snippet\":\"class SymmetricKeyAlgorithm(IntEnum):\\n    \\\"\\\"\\\"Supported symmetric key algorithms.\\\"\\\"\\\"\\n    Plaintext = 0x00\\n    #: .. warning::\\n    #:     IDEA is insecure. PGPy only allows it to be used for decryption, not encryption!\\n    IDEA = 0x01\\n    #: Triple-DES with 168-bit key derived from 192\\n    TripleDES = 0x02\\n    #: CAST5 (or CAST-128) with 128-bit key\\n    CAST5 = 0x03\\n    #: Blowfish with 128-bit key and 16 rounds\\n    Blowfish = 0x04\\n    #: AES with 128-bit key\\n    AES128 = 0x07\\n    #: AES with 192-bit key\\n    AES192 = 0x08\\n    #: AES with 256-bit key\\n    AES256 = 0x09\\n    # Twofish with 256-bit key - not currently supported\\n    Twofish256 = 0x0A\\n    #: Camellia with 128-bit key\\n    Camellia128 = 0x0B\\n    #: Camellia with 192-bit key\\n    Camellia192 = 0x0C\\n    #: Camellia with 256-bit key\\n    Camellia256 = 0x0D\\n\\n    @property\\n    def cipher(self):\\n        bs = {SymmetricKeyAlgorithm.IDEA: algorithms.IDEA,\\n              SymmetricKeyAlgorithm.TripleDES: algorithms.TripleDES,\\n              SymmetricKeyAlgorithm.CAST5: algorithms.CAST5,\\n              SymmetricKeyAlgorithm.Blowfish: algorithms.Blowfish,\\n              SymmetricKeyAlgorithm.AES128: algorithms.AES,\\n              SymmetricKeyAlgorithm.AES192: algorithms.AES,\\n              SymmetricKeyAlgorithm.AES256: algorithms.AES,\\n              SymmetricKeyAlgorithm.Twofish256: namedtuple('Twofish256', ['block_size'])(block_size=128),\\n              SymmetricKeyAlgorithm.Camellia128: algorithms.Camellia,\\n              SymmetricKeyAlgorithm.Camellia192: algorithms.Camellia,\\n              SymmetricKeyAlgorithm.Camellia256: algorithms.Camellia}\\n\\n        if self in bs:\\n            return bs[self]\\n\\n        raise NotImplementedError(repr(self))\\n\\n    @property\\n    def is_insecure(self):\\n        insecure_ciphers = {SymmetricKeyAlgorithm.IDEA}\\n        return self in insecure_ciphers\\n\\n    @property\\n    def block_size(self):\\n        return self.cipher.block_size\\n\\n    @property\\n    def key_size(self):\\n        ks = {SymmetricKeyAlgorithm.IDEA: 128,\\n              SymmetricKeyAlgorithm.TripleDES: 192,\\n              SymmetricKeyAlgorithm.CAST5: 128,\\n              SymmetricKeyAlgorithm.Blowfish: 128,\\n              SymmetricKeyAlgorithm.AES128: 128,\\n              SymmetricKeyAlgorithm.AES192: 192,\\n              SymmetricKeyAlgorithm.AES256: 256,\\n              SymmetricKeyAlgorithm.Twofish256: 256,\\n              SymmetricKeyAlgorithm.Camellia128: 128,\\n              SymmetricKeyAlgorithm.Camellia192: 192,\\n              SymmetricKeyAlgorithm.Camellia256: 256}\\n\\n        if self in ks:\\n            return ks[self]\\n\\n        raise NotImplementedError(repr(self))\\n\\n    def gen_iv(self):\\n        return os.urandom(self.block_size // 8)\\n\\n    def gen_key(self):\\n        return os.urandom(self.key_size // 8)\"}, {\"identifier\":\"TrustFlags\", \"path\":\"src/leap/mx/vendor/pgpy/constants.py\", \"snippet\":\"class TrustFlags(FlagEnum):\\n    Revoked = 0x20\\n    SubRevoked = 0x40\\n    Disabled = 0x80\\n    PendingCheck = 0x100\"}, {\"identifier\":\"TrustLevel\", \"path\":\"src/leap/mx/vendor/pgpy/constants.py\", \"snippet\":\"class TrustLevel(IntEnum):\\n    Unknown = 0\\n    Expired = 1\\n    Undefined = 2\\n    Never = 3\\n    Marginal = 4\\n    Fully = 5\\n    Ultimate = 6\"}, {\"identifier\":\"sdproperty\", \"path\":\"src/leap/mx/vendor/pgpy/decorators.py\", \"snippet\":\"def sdproperty(fget):\\n    def defset(obj, val):  # pragma: no cover\\n        raise TypeError(str(val.__class__))\\n\\n    class SDProperty(property):\\n        def register(self, cls=None, fset=None):\\n            return self.fset.register(cls, fset)\\n\\n        def setter(self, fset):\\n            self.register(object, fset)\\n            return type(self)(self.fget, self.fset, self.fdel, self.__doc__)\\n\\n    return SDProperty(fget, sdmethod(defset))\"}, {\"identifier\":\"PGPDecryptionError\", \"path\":\"src/leap/mx/vendor/pgpy/errors.py\", \"snippet\":\"class PGPDecryptionError(Exception):\\n    \\\"\\\"\\\"Raised when decryption fails\\\"\\\"\\\"\\n    pass\"}, {\"identifier\":\"_decrypt\", \"path\":\"src/leap/mx/vendor/pgpy/symenc.py\", \"snippet\":\"def _decrypt(ct, key, alg, iv=None):\\n    if iv is None:\\n        \\\"\\\"\\\"\\n        Instead of using an IV, OpenPGP prefixes a string of length\\n        equal to the block size of the cipher plus two to the data before it\\n        is encrypted. The first block-size octets (for example, 8 octets for\\n        a 64-bit block length) are random, and the following two octets are\\n        copies of the last two octets of the IV.\\n        \\\"\\\"\\\"\\n        iv = b'\\\\x00' * (alg.block_size // 8)\\n\\n    try:\\n        decryptor = Cipher(alg.cipher(key), modes.CFB(iv), default_backend()).decryptor()\\n\\n    except UnsupportedAlgorithm as ex:  # pragma: no cover\\n        six.raise_from(PGPDecryptionError, ex)\\n\\n    else:\\n        return bytearray(decryptor.update(ct) + decryptor.finalize())\"}, {\"identifier\":\"_encrypt\", \"path\":\"src/leap/mx/vendor/pgpy/symenc.py\", \"snippet\":\"def _encrypt(pt, key, alg, iv=None):\\n    if iv is None:\\n        iv = b'\\\\x00' * (alg.block_size // 8)\\n\\n    if alg.is_insecure:\\n        raise PGPInsecureCipher(\\\"{:s} is not secure. Do not use it for encryption!\\\".format(alg.name))\\n\\n    if not callable(alg.cipher):\\n        raise PGPEncryptionError(\\\"Cipher {:s} not supported\\\".format(alg.name))\\n\\n    try:\\n        encryptor = Cipher(alg.cipher(key), modes.CFB(iv), default_backend()).encryptor()\\n\\n    except UnsupportedAlgorithm as ex:  # pragma: no cover\\n        six.raise_from(PGPEncryptionError, ex)\\n\\n    else:\\n        return bytearray(encryptor.update(pt) + encryptor.finalize())\"}, {\"identifier\":\"Fingerprint\", \"path\":\"src/leap/mx/vendor/pgpy/types.py\", \"snippet\":\"class Fingerprint(str):\\n    \\\"\\\"\\\"\\n    A subclass of ``str``. Can be compared using == and != to ``str``, ``unicode``, and other :py:obj:`Fingerprint` instances.\\n\\n    Primarily used as a key for internal dictionaries, so it ignores spaces when comparing and hashing\\n    \\\"\\\"\\\"\\n    @property\\n    def keyid(self):\\n        return str(self).replace(' ', '')[-16:]\\n\\n    @property\\n    def shortid(self):\\n        return str(self).replace(' ', '')[-8:]\\n\\n    def __new__(cls, content):\\n        if isinstance(content, Fingerprint):\\n            return content\\n\\n        # validate input before continuing: this should be a string of 40 hex digits\\n        content = content.upper().replace(' ', '')\\n        if not bool(re.match(r'^[A-F0-9]{40}$', content)):\\n            raise ValueError(\\\"Expected: String of 40 hex digits\\\")\\n\\n        # store in the format: \\\"AAAA BBBB CCCC DDDD EEEE  FFFF 0000 1111 2222 3333\\\"\\n        #                                               ^^ note 2 spaces here\\n        spaces = [ ' ' if i != 4 else '  ' for i in range(10) ]\\n        chunks = [ ''.join(g) for g in six.moves.zip_longest(*[iter(content)] * 4) ]\\n        content = ''.join(j for i in six.moves.zip_longest(chunks, spaces, fillvalue='') for j in i).strip()\\n\\n        return str.__new__(cls, content)\\n\\n    def __eq__(self, other):\\n        if isinstance(other, Fingerprint):\\n            return str(self) == str(other)\\n\\n        if isinstance(other, (six.text_type, bytes, bytearray)):\\n            if isinstance(other, (bytes, bytearray)):  # pragma: no cover\\n                other = other.decode('latin-1')\\n\\n            other = str(other).replace(' ', '')\\n            return any([self.replace(' ', '') == other,\\n                        self.keyid == other,\\n                        self.shortid == other])\\n\\n        return False  # pragma: no cover\\n\\n    def __ne__(self, other):\\n        return not (self == other)\\n\\n    def __hash__(self):\\n        return hash(str(self.replace(' ', '')))\\n\\n    def __bytes__(self):\\n        return binascii.unhexlify(six.b(self.replace(' ', '')))\"}]", "import_statement": "import abc\nimport binascii\nimport calendar\nimport copy\nimport hashlib\nimport os\nimport re\nimport six\nfrom datetime import datetime\nfrom cryptography.hazmat.primitives import constant_time\nfrom cryptography.hazmat.primitives.asymmetric import padding\nfrom .fields import DSAPriv, DSAPub, DSASignature\nfrom .fields import ECDSAPub, ECDSAPriv, ECDSASignature\nfrom .fields import ECDHPub, ECDHPriv, ECDHCipherText\nfrom .fields import ElGCipherText, ElGPriv, ElGPub\nfrom .fields import OpaquePubKey\nfrom .fields import OpaquePrivKey\nfrom .fields import RSACipherText, RSAPriv, RSAPub, RSASignature\nfrom .fields import String2Key\nfrom .fields import SubPackets\nfrom .fields import UserAttributeSubPackets\nfrom .types import Packet\nfrom .types import Primary\nfrom .types import Private\nfrom .types import Public\nfrom .types import Sub\nfrom .types import VersionedPacket\nfrom ..constants import CompressionAlgorithm\nfrom ..constants import HashAlgorithm\nfrom ..constants import PubKeyAlgorithm\nfrom ..constants import SignatureType\nfrom ..constants import SymmetricKeyAlgorithm\nfrom ..constants import TrustFlags\nfrom ..constants import TrustLevel\nfrom ..decorators import sdproperty\nfrom ..errors import PGPDecryptionError\nfrom ..symenc import _decrypt\nfrom ..symenc import _encrypt\nfrom ..types import Fingerprint", "code": "    def pubalg(self):\n        return self._pubalg\n\n    @pubalg.register(int)\n    @pubalg.register(PubKeyAlgorithm)\n    def pubalg_int(self, val):\n        self._pubalg = PubKeyAlgorithm(val)\n        if self._pubalg in [PubKeyAlgorithm.RSAEncryptOrSign, PubKeyAlgorithm.RSAEncrypt, PubKeyAlgorithm.RSASign]:\n            self.signature = RSASignature()\n\n        elif self._pubalg == PubKeyAlgorithm.DSA:\n            self.signature = DSASignature()\n\n    @sdproperty\n    def halg(self):\n        return self._halg\n\n    @halg.register(int)\n    @halg.register(HashAlgorithm)\n    def halg_int(self, val):\n        try:\n            self._halg = HashAlgorithm(val)\n\n        except ValueError:  # pragma: no cover\n            self._halg = val\n\n    @sdproperty\n    def signer(self):\n        return self._signer\n\n    @signer.register(str)\n    @signer.register(six.text_type)\n    def signer_str(self, val):\n        self._signer = val\n\n    @signer.register(bytearray)\n    def signer_bin(self, val):\n        self._signer = binascii.hexlify(val).upper().decode('latin-1')\n\n    def __init__(self):\n        super(OnePassSignatureV3, self).__init__()\n        self._sigtype = None\n        self._halg = None\n        self._pubalg = None\n        self._signer = b'\\x00' * 8\n        self.nested = False\n\n    def __bytearray__(self):\n        _bytes = bytearray()\n        _bytes += super(OnePassSignatureV3, self).__bytearray__()\n        _bytes += bytearray([self.sigtype])\n        _bytes += bytearray([self.halg])\n        _bytes += bytearray([self.pubalg])\n        _bytes += binascii.unhexlify(six.b(self.signer))\n        _bytes += bytearray([int(self.nested)])\n        return _bytes\n\n    def parse(self, packet):\n        super(OnePassSignatureV3, self).parse(packet)\n        self.sigtype = packet[0]\n        del packet[0]\n\n        self.halg = packet[0]\n        del packet[0]\n\n        self.pubalg = packet[0]\n        del packet[0]\n\n        self.signer = packet[:8]\n        del packet[:8]\n\n        self.nested = (packet[0] == 1)\n        del packet[0]\n\n\nclass PrivKey(VersionedPacket, Primary, Private):\n    __typeid__ = 0x05\n    __ver__ = 0\n\n\nclass PubKey(VersionedPacket, Primary, Public):\n    __typeid__ = 0x06\n    __ver__ = 0\n\n    @abc.abstractproperty\n    def fingerprint(self):\n        \"\"\"compute and return the fingerprint of the key\"\"\"\n\n\nclass PubKeyV4(PubKey):\n    __ver__ = 4\n\n    @sdproperty\n    def created(self):\n        return self._created\n\n    @created.register(datetime)\n    def created_datetime(self, val):\n        self._created = val\n\n    @created.register(int)\n    def created_int(self, val):\n        self.created = datetime.utcfromtimestamp(val)\n\n    @created.register(bytes)\n    @created.register(bytearray)\n    def created_bin(self, val):\n        self.created = self.bytes_to_int(val)\n\n    @sdproperty\n    def pkalg(self):\n        return self._pkalg\n\n    @pkalg.register(int)\n    @pkalg.register(PubKeyAlgorithm)\n    def pkalg_int(self, val):\n        self._pkalg = PubKeyAlgorithm(val)\n\n        _c = {\n            # True means public\n", "next_line": "            (True, PubKeyAlgorithm.RSAEncryptOrSign): RSAPub,", "gold_snippet_index": 16, "id": 18, "__internal_uuid__": "e1dd4744-0289-4092-a188-dcca70cf4739"}
{"repo_name": "mshunshin/SegNetCMR", "file_path": "pydicom/filereader.py", "context": "[{\"identifier\":\"TupleTag\", \"path\":\"pydicom/tag.py\", \"snippet\":\"def TupleTag(group_elem):\\n    \\\"\\\"\\\"Fast factory for BaseTag object with known safe (group, element) tuple\\\"\\\"\\\"\\n    long_value = group_elem[0] << 16 | group_elem[1]\\n    return BaseTag(long_value)\"}, {\"identifier\":\"RawDataElement\", \"path\":\"pydicom/dataelem.py\", \"snippet\":\"def isMultiValue(value):\\ndef isString(val):\\ndef isStringOrStringList(val):\\n    def __init__(self, tag, VR, value, file_value_tell=None,\\n                 is_undefined_length=False, already_converted=False):\\n    def value(self):\\n    def value(self, val):\\n    def VM(self):\\n    def _convert_value(self, val):\\n    def _convert(self, val):\\n    def __eq__(self, other):\\n    def __ne__(self, other):\\n    def __str__(self):\\n    def repval(self):\\n    def __unicode__(self):\\n    def __getitem__(self, key):\\n    def name(self):\\n    def description(self):\\n    def is_retired(self):\\n    def keyword(self):\\n    def __repr__(self):\\n    def __init__(self, tag, VR, fp, file_mtime, data_element_tell, length):\\n    def repval(self):\\n    def value(self):\\n    def value(self, val):\\ndef DataElement_from_raw(raw_data_element, encoding=None):\\nclass DataElement(object):\\nclass DeferredDataElement(DataElement):\\n    VR = raw.VR\\n            VR = dictionaryVR(raw.tag)\\n                VR = 'OB'  # just read the bytes, no way to know what they mean\\n                VR = 'UL'\"}, {\"identifier\":\"bytes2hex\", \"path\":\"pydicom/util/hexutil.py\", \"snippet\":\"def bytes2hex(byte_string):\\n    s = b2a_hex(byte_string)\\n    if not in_py2:\\n        s = s.decode()\\n    return \\\" \\\".join(s[i:i + 2] for i in range(0, len(s), 2))\"}, {\"identifier\":\"extra_length_VRs\", \"path\":\"pydicom/valuerep.py\", \"snippet\":\"class DA(date):\\nclass DT(datetime):\\nclass TM(time):\\nclass DSfloat(float):\\nclass DSdecimal(Decimal):\\nclass IS(int):\\nclass PersonName3(object):\\nclass PersonNameBase(object):\\nclass PersonName(PersonNameBase, bytes):\\nclass PersonNameUnicode(PersonNameBase, compat.text_type):\\n    def __getstate__(self):\\n    def __setstate__(self, state):\\n    def __reduce__(self):\\n    def __new__(cls, val):\\n    def __init__(self, val):\\n    def __str__(self):\\n    def __getstate__(self):\\n    def __setstate__(self, state):\\n    def __reduce__(self):\\n    def __new__(cls, val):\\n    def __init__(self, val):\\n    def __str__(self):\\n    def __getstate__(self):\\n    def __setstate__(self, state):\\n    def __reduce__(self):\\n    def __new__(cls, val):\\n    def __init__(self, val):\\n    def __str__(self):\\n    def __getstate__(self):\\n    def __setstate__(self, state):\\n    def __init__(self, val):\\n    def __str__(self):\\n    def __repr__(self):\\n    def __getstate__(self):\\n    def __setstate__(self, state):\\n    def __new__(cls, val):\\n    def __init__(self, val):\\n    def __str__(self):\\n    def __repr__(self):\\ndef DS(val):\\n        def __getstate__(self):\\n        def __setstate__(self, state):\\n    def __new__(cls, val):\\n    def __init__(self, val):\\n    def __repr__(self):\\ndef MultiString(val, valtype=str):\\n    def __init__(self, val, encodings=default_encoding):\\n    def parse(self, val):\\n    def __eq__(self, other):\\n    def __ne__(self, other):\\n    def __str__(self):\\n    def __repr__(self):\\n    def decode(self, encodings=None):\\n    def encode(self, encodings=None):\\n    def family_comma_given(self):\\n    def formatted(self, format_str):\\n    def _verify_encodings(self, encodings):\\n    def __init__(self, val):\\n    def formatted(self, format_str):\\n    def parse(self):\\n    def __new__(cls, val):\\n    def encode(self, *args):\\n    def family_comma_given(self):\\n    def __new__(cls, val, encodings):\\n    def __init__(self, val, encodings):\\n    def _verify_encodings(self, encodings):\\n    def encode(self, encodings):\\n    def family_comma_given(self):\"}, {\"identifier\":\"default_encoding\", \"path\":\"pydicom/charset.py\", \"snippet\":\"def clean_escseq(element, encodings):\\ndef convert_encodings(encodings):\\ndef decode(data_element, dicom_character_set):\"}, {\"identifier\":\"in_py2\", \"path\":\"pydicom/compat.py\", \"snippet\":\"def reraise(tp, value, tb):\"}, {\"identifier\":\"compat\", \"path\":\"pydicom/compat.py\", \"snippet\":\"def reraise(tp, value, tb):\"}, {\"identifier\":\"config\", \"path\":\"pydicom/config.py\", \"snippet\":\"def reset_data_element_callback():\\ndef DS_decimal(use_Decimal_boolean=True):\\ndef debug(debug_on=True):\"}, {\"identifier\":\"logger\", \"path\":\"pydicom/config.py\", \"snippet\":\"def reset_data_element_callback():\\ndef DS_decimal(use_Decimal_boolean=True):\\ndef debug(debug_on=True):\"}, {\"identifier\":\"InvalidDicomError\", \"path\":\"pydicom/errors.py\", \"snippet\":\"class InvalidDicomError(Exception):\\n    \\\"\\\"\\\"Exception that is raised when the the file does not seem\\n    to be a valid dicom file, usually when the four characters\\n    \\\"DICM\\\" are not present at position 128 in the file.\\n    (According to the dicom specification, each dicom file should\\n    have this.)\\n\\n    To force reading the file (because maybe it is a dicom file without\\n    a header), use read_file(..., force=True).\\n    \\\"\\\"\\\"\\n    def __init__(self, *args):\\n        if not args:\\n            args = ('The specified file is not a valid DICOM file.',)\\n        Exception.__init__(self, *args)\"}, {\"identifier\":\"DicomFile\", \"path\":\"pydicom/filebase.py\", \"snippet\":\"def DicomFile(*args, **kwargs):\\n    return DicomFileLike(open(*args, **kwargs))\"}, {\"identifier\":\"Dataset\", \"path\":\"pydicom/dataset.py\", \"snippet\":\"class Dataset(dict):\\n    \\\"\\\"\\\"A collection (dictionary) of Dicom `DataElement` instances.\\n\\n    Example of two ways to retrieve or set values:\\n\\n    1. dataset[0x10, 0x10].value --> patient's name\\n    2. dataset.PatientName --> patient's name\\n\\n    Example (2) uses DICOM \\\"keywords\\\", defined starting in 2011 standard.\\n    PatientName is not actually a member of the object, but unknown member\\n    requests are checked against the DICOM dictionary. If the name matches a\\n    DicomDictionary descriptive string, the corresponding tag is used\\n    to look up or set the `DataElement` instance's value.\\n\\n    :attribute indent_chars: for string display, the characters used to indent\\n       nested Data Elements (e.g. sequence items). Default is three spaces.\\n\\n    \\\"\\\"\\\"\\n    indent_chars = \\\"   \\\"\\n\\n    # Python 2: Classes which define __eq__ should flag themselves as unhashable\\n    __hash__ = None\\n\\n    def __init__(self, *args, **kwargs):\\n        self._parent_encoding = kwargs.get('parent_encoding', default_encoding)\\n        dict.__init__(self, *args)\\n\\n    def __enter__(self):\\n        return self\\n\\n    def __exit__(self, exc_type, exc_val, exc_tb):\\n        return False\\n\\n    def add(self, data_element):\\n        \\\"\\\"\\\"Equivalent to dataset[data_element.tag] = data_element.\\\"\\\"\\\"\\n        self[data_element.tag] = data_element\\n\\n    def add_new(self, tag, VR, value):\\n        \\\"\\\"\\\"Create a new DataElement instance and add it to this Dataset.\\\"\\\"\\\"\\n        data_element = DataElement(tag, VR, value)\\n        # use data_element.tag since DataElement verified it\\n        self[data_element.tag] = data_element\\n\\n    def data_element(self, name):\\n        \\\"\\\"\\\"Return the full data_element instance for the given descriptive name.\\n\\n        Parameters\\n        ----------\\n        name: str\\n            A DICOM keyword\\n\\n        Returns\\n        -------\\n        DataElement instance or None\\n            Returns a DataElement instance in this dataset with the given name.\\n            If the tag for that name is not found, returns None.\\n        \\\"\\\"\\\"\\n        tag = tag_for_name(name)\\n        if tag:\\n            return self[tag]\\n        return None\\n\\n    def __contains__(self, name):\\n        \\\"\\\"\\\"Extend dict.__contains__() to handle DICOM keywords.\\n\\n        This is called for code like: ``if 'SliceLocation' in dataset``.\\n\\n        \\\"\\\"\\\"\\n        if isinstance(name, (str, compat.text_type)):\\n            tag = tag_for_name(name)\\n        else:\\n            try:\\n                tag = Tag(name)\\n            except:\\n                return False\\n        if tag:\\n            return dict.__contains__(self, tag)\\n        else:\\n            return dict.__contains__(self, name)  # will no doubt raise an exception\\n\\n    def decode(self):\\n        \\\"\\\"\\\"Apply character set decoding to all data elements.\\n\\n        See DICOM PS3.5-2008 6.1.1.\\n        \\\"\\\"\\\"\\n        # Find specific character set. 'ISO_IR 6' is default\\n        # May be multi-valued, but let pydicom.charset handle all logic on that\\n        dicom_character_set = self._character_set\\n\\n        # Shortcut to the decode function in pydicom.charset\\n        decode_data_element = pydicom.charset.decode\\n\\n        # Callback for walk(), to decode the chr strings if necessary\\n        # This simply calls the pydicom.charset.decode function\\n        def decode_callback(ds, data_element):\\n            if data_element.VR == 'SQ':\\n                [dset.decode() for dset in data_element.value]\\n            else:\\n                decode_data_element(data_element, dicom_character_set)\\n\\n        self.walk(decode_callback, recursive=False)\\n\\n    def __delattr__(self, name):\\n        \\\"\\\"\\\"Intercept requests to delete an attribute by name, e.g. del ds.name\\n\\n        If name is a DICOM keyword, then delete the corresponding tag\\n           and data_element. Else, delete an instance (python) attribute\\n           as any other class would do\\n\\n        \\\"\\\"\\\"\\n        # First check if a valid DICOM keyword and if we have that data element\\n        tag = tag_for_name(name)\\n        if tag is not None and tag in self:\\n            dict.__delitem__(self, tag)  # direct to dict as we know we have key\\n        # If not a DICOM name in this dataset, check for regular instance name\\n        #   can't do delete directly, that will call __delattr__ again\\n        elif name in self.__dict__:\\n            del self.__dict__[name]\\n        # Not found, raise an error in same style as python does\\n        else:\\n            raise AttributeError(name)\\n\\n    def __delitem__(self, key):\\n        \\\"\\\"\\\"Intercept requests to delete an attribute by key, e.g. del ds[tag]\\\"\\\"\\\"\\n        # Assume is a standard tag (for speed in common case)\\n        try:\\n            dict.__delitem__(self, key)\\n        # If not a standard tag, than convert to Tag and try again\\n        except KeyError:\\n            tag = Tag(key)\\n            dict.__delitem__(self, tag)\\n\\n    def __dir__(self):\\n        \\\"\\\"\\\"Give a list of attributes available in the dataset\\n\\n        List of attributes is used, for example, in auto-completion in editors\\n           or command-line environments.\\n        \\\"\\\"\\\"\\n        # Force zip object into a list in case of python3. Also backwards\\n        # compatible\\n        meths = set(list(zip(\\n                    *inspect.getmembers(Dataset, inspect.isroutine)))[0])\\n        props = set(list(zip(\\n                    *inspect.getmembers(Dataset, inspect.isdatadescriptor)))[0])\\n        dicom_names = set(self.dir())\\n        alldir = sorted(props | meths | dicom_names)\\n        return alldir\\n\\n    def dir(self, *filters):\\n        \\\"\\\"\\\"Return an alphabetical list of data_element keywords in the dataset.\\n\\n        Intended mainly for use in interactive Python sessions.\\n\\n        Parameters\\n        ----------\\n        filters : str\\n            Zero or more string arguments to the function. Used for\\n            case-insensitive match to any part of the DICOM name.\\n\\n        Returns\\n        -------\\n        All data_element names in this dataset matching the filters.\\n        If no filters, return all DICOM keywords in the dataset.\\n        \\\"\\\"\\\"\\n        allnames = []\\n        for tag, data_element in self.items():\\n            allnames.extend(all_names_for_tag(tag))\\n        # remove blanks - tags without valid names (e.g. private tags)\\n        allnames = [x for x in allnames if x]\\n        # Store found names in a dict, so duplicate names appear only once\\n        matches = {}\\n        for filter_ in filters:\\n            filter_ = filter_.lower()\\n            match = [x for x in allnames if x.lower().find(filter_) != -1]\\n            matches.update(dict([(x, 1) for x in match]))\\n        if filters:\\n            names = sorted(matches.keys())\\n            return names\\n        else:\\n            return sorted(allnames)\\n    \\n    def __eq__(self, other):\\n        \\\"\\\"\\\" \\n        Compare `self` and `other` for equality\\n\\n        Returns\\n        -------\\n        bool\\n            The result if `self` and `other` are the same class\\n        NotImplemented\\n            If `other` is not the same class as `self` then returning\\n            NotImplemented delegates the result to superclass.__eq__(subclass)\\n        \\\"\\\"\\\"\\n        # When comparing against self this will be faster\\n        if other is self:\\n            return True\\n\\n        if isinstance(other, self.__class__):\\n            # Compare Elements using values() and class variables using __dict__\\n            # Convert values() to a list for compatibility between python 2 and 3\\n            return (list(self.values()) == list(other.values())) and (self.__dict__ == other.__dict__)\\n\\n        return NotImplemented\\n\\n    def get(self, key, default=None):\\n        \\\"\\\"\\\"Extend dict.get() to handle DICOM keywords\\\"\\\"\\\"\\n        if isinstance(key, (str, compat.text_type)):\\n            try:\\n                return getattr(self, key)\\n            except AttributeError:\\n                return default\\n        else:\\n            # is not a string, try to make it into a tag and then hand it\\n            # off to the underlying dict\\n            if not isinstance(key, BaseTag):\\n                try:\\n                    key = Tag(key)\\n                except:\\n                    raise TypeError(\\\"Dataset.get key must be a string or tag\\\")\\n        try:\\n            return_val = self.__getitem__(key)\\n        except KeyError:\\n            return_val = default\\n        return return_val\\n\\n    def __getattr__(self, name):\\n        \\\"\\\"\\\"Intercept requests for unknown Dataset python-attribute names.\\n\\n        If the name matches a Dicom keyword,\\n        return the value for the data_element with the corresponding tag.\\n\\n        \\\"\\\"\\\"\\n        # __getattr__ only called if instance cannot find name in self.__dict__\\n        # So, if name is not a dicom string, then is an error\\n        tag = tag_for_name(name)\\n        if tag is None:\\n            raise AttributeError(\\\"Dataset does not have attribute \\\"\\n                                 \\\"'{0:s}'.\\\".format(name))\\n        tag = Tag(tag)\\n        if tag not in self:\\n            raise AttributeError(\\\"Dataset does not have attribute \\\"\\n                                 \\\"'{0:s}'.\\\".format(name))\\n        else:  # do have that dicom data_element\\n            return self[tag].value\\n\\n    @property\\n    def _character_set(self):\\n        char_set = self.get('SpecificCharacterSet', None)\\n\\n        if not char_set:\\n            char_set = self._parent_encoding\\n        else:\\n            char_set = convert_encodings(char_set)\\n\\n        return char_set\\n\\n    def __getitem__(self, key):\\n        \\\"\\\"\\\"Operator for dataset[key] request.\\\"\\\"\\\"\\n        tag = Tag(key)\\n        data_elem = dict.__getitem__(self, tag)\\n\\n        if isinstance(data_elem, DataElement):\\n            return data_elem\\n        elif isinstance(data_elem, tuple):\\n            # If a deferred read, then go get the value now\\n            if data_elem.value is None:\\n                from pydicom.filereader import read_deferred_data_element\\n                data_elem = read_deferred_data_element(self.fileobj_type,\\n                                                       self.filename, self.timestamp, data_elem)\\n\\n            if tag != (0x08, 0x05):\\n                character_set = self._character_set\\n            else:\\n                character_set = default_encoding\\n            # Not converted from raw form read from file yet; do so now\\n            self[tag] = DataElement_from_raw(data_elem, character_set)\\n        return dict.__getitem__(self, tag)\\n\\n    def get_item(self, key):\\n        \\\"\\\"\\\"Return the raw data element if possible.\\n        It will be raw if the user has never accessed the value,\\n        or set their own value.\\n        Note if the data element is a deferred-read element,\\n        then it is read and converted before being returned\\n        \\\"\\\"\\\"\\n        tag = Tag(key)\\n        data_elem = dict.__getitem__(self, tag)\\n        # If a deferred read, return using __getitem__ to read and convert it\\n        if isinstance(data_elem, tuple) and data_elem.value is None:\\n            return self[key]\\n        return data_elem\\n\\n    def group_dataset(self, group):\\n        \\\"\\\"\\\"Return a Dataset containing only data_elements of a certain group.\\n\\n        Parameters\\n        ----------\\n        group : the group part of a dicom (group, element) tag.\\n\\n        Returns\\n        -------\\n        A dataset instance containing data elements of the group specified.\\n        \\\"\\\"\\\"\\n        ds = Dataset()\\n        ds.update(dict([(tag, data_element) for tag, data_element in self.items()\\n                        if tag.group == group]))\\n        return ds\\n\\n    def __iter__(self):\\n        \\\"\\\"\\\"Method to iterate through the dataset, returning data_elements.\\n        e.g.:\\n        for data_element in dataset:\\n            do_something...\\n        The data_elements are returned in DICOM order,\\n        i.e. in increasing order by tag value.\\n        Sequence items are returned as a single data_element; it is up to the\\n           calling code to recurse into the Sequence items if desired\\n        \\\"\\\"\\\"\\n        # Note this is different than the underlying dict class,\\n        #        which returns the key of the key:value mapping.\\n        #   Here the value is returned (but data_element.tag has the key)\\n        taglist = sorted(self.keys())\\n        for tag in taglist:\\n            yield self[tag]\\n\\n    def _is_uncompressed_transfer_syntax(self):\\n        # FIXME uses file_meta here, should really only be thus for FileDataset\\n        return self.file_meta.TransferSyntaxUID in NotCompressedPixelTransferSyntaxes\\n\\n    def __ne__(self, other):\\n        \\\"\\\"\\\" Compare `self` and `other` for inequality \\\"\\\"\\\"\\n        return not (self == other)\\n\\n    def _pixel_data_numpy(self):\\n        \\\"\\\"\\\"Return a NumPy array of the pixel data if NumPy is available.\\n        Falls back to GDCM in case of unsupported transfer syntaxes.\\n\\n        Raises\\n        ------\\n        TypeError\\n            If there is no pixel data or not a supported data type\\n        ImportError\\n            If NumPy isn't found, or in the case of fallback, if GDCM isn't found.\\n\\n        Returns\\n        -------\\n        NumPy array\\n        \\\"\\\"\\\"\\n        if not self._is_uncompressed_transfer_syntax():\\n            if not have_gdcm:\\n                raise NotImplementedError(\\\"Pixel Data is compressed in a format pydicom does not yet handle. Cannot return array. Pydicom might be able to convert the pixel data using GDCM if it is installed.\\\")\\n            elif not self.filename:\\n                raise NotImplementedError(\\\"GDCM is only supported when the dataset has been created with a filename.\\\")\\n        if not have_numpy:\\n            msg = \\\"The Numpy package is required to use pixel_array, and numpy could not be imported.\\\"\\n            raise ImportError(msg)\\n        if 'PixelData' not in self:\\n            raise TypeError(\\\"No pixel data found in this dataset.\\\")\\n\\n        # There are two cases:\\n        # 1) uncompressed PixelData -> use numpy\\n        # 2) compressed PixelData, filename is available and GDCM is available -> use GDCM\\n        if self._is_uncompressed_transfer_syntax():\\n            # Make NumPy format code, e.g. \\\"uint16\\\", \\\"int32\\\" etc\\n            # from two pieces of info:\\n            #    self.PixelRepresentation -- 0 for unsigned, 1 for signed;\\n            #    self.BitsAllocated -- 8, 16, or 32\\n            format_str = '%sint%d' % (('u', '')[self.PixelRepresentation],\\n                                      self.BitsAllocated)\\n            try:\\n                numpy_dtype = numpy.dtype(format_str)\\n            except TypeError:\\n                msg = (\\\"Data type not understood by NumPy: \\\"\\n                       \\\"format='%s', PixelRepresentation=%d, BitsAllocated=%d\\\")\\n                raise TypeError(msg % (format_str, self.PixelRepresentation,\\n                                self.BitsAllocated))\\n\\n            if self.is_little_endian != sys_is_little_endian:\\n                numpy_dtype = numpy_dtype.newbyteorder('S')\\n\\n            pixel_bytearray = self.PixelData\\n        elif have_gdcm and self.filename:\\n            # read the file using GDCM\\n            # FIXME this should just use self.PixelData instead of self.filename\\n            #       but it is unclear how this should be achieved using GDCM\\n            gdcm_image_reader = gdcm.ImageReader()\\n            gdcm_image_reader.SetFileName(self.filename)\\n            if not gdcm_image_reader.Read():\\n                raise TypeError(\\\"GDCM could not read DICOM image\\\")\\n            gdcm_image = gdcm_image_reader.GetImage()\\n\\n            # determine the correct numpy datatype\\n            gdcm_numpy_typemap = {\\n                gdcm.PixelFormat.INT8:     numpy.int8,\\n                gdcm.PixelFormat.UINT8:    numpy.uint8,\\n                gdcm.PixelFormat.UINT16:   numpy.uint16,\\n                gdcm.PixelFormat.INT16:    numpy.int16,\\n                gdcm.PixelFormat.UINT32:   numpy.uint32,\\n                gdcm.PixelFormat.INT32:    numpy.int32,\\n                gdcm.PixelFormat.FLOAT32:  numpy.float32,\\n                gdcm.PixelFormat.FLOAT64:  numpy.float64\\n            }\\n            gdcm_pixel_format = gdcm_image.GetPixelFormat().GetScalarType()\\n            if gdcm_pixel_format in gdcm_numpy_typemap:\\n                numpy_dtype = gdcm_numpy_typemap[gdcm_pixel_format]\\n            else:\\n                raise TypeError('{0} is not a GDCM supported pixel format'.format(gdcm_pixel_format))\\n\\n            # GDCM returns char* as type str. Under Python 2 `str` are\\n            # byte arrays by default. Python 3 decodes this to\\n            # unicode strings by default.\\n            # The SWIG docs mention that they always decode byte streams\\n            # as utf-8 strings for Python 3, with the `surrogateescape`\\n            # error handler configured.\\n            # Therefore, we can encode them back to their original bytearray\\n            # representation on Python 3 by using the same parameters.\\n            pixel_bytearray = gdcm_image.GetBuffer()\\n            if sys.version_info >= (3, 0):\\n                pixel_bytearray = pixel_bytearray.encode(\\\"utf-8\\\", \\\"surrogateescape\\\")\\n\\n            # if GDCM indicates that a byte swap is in order, make sure to inform numpy as well\\n            if gdcm_image.GetNeedByteSwap():\\n                numpy_dtype = numpy_dtype.newbyteorder('S')\\n\\n            # Here we need to be careful because in some cases, GDCM reads a\\n            # buffer that is too large, so we need to make sure we only include\\n            # the first n_rows * n_columns * dtype_size bytes.\\n\\n            n_bytes = self.Rows * self.Columns * numpy.dtype(numpy_dtype).itemsize\\n\\n            if len(pixel_bytearray) > n_bytes:\\n\\n                # We make sure that all the bytes after are in fact zeros\\n                padding = pixel_bytearray[n_bytes:]\\n                if numpy.any(numpy.fromstring(padding, numpy.byte)):\\n                    pixel_bytearray = pixel_bytearray[:n_bytes]\\n                else:\\n                    # We revert to the old behavior which should then result in a\\n                    # Numpy error later on.\\n                    pass\\n\\n        pixel_array = numpy.fromstring(pixel_bytearray, dtype=numpy_dtype)\\n\\n        # Note the following reshape operations return a new *view* onto pixel_array, but don't copy the data\\n        if 'NumberOfFrames' in self and self.NumberOfFrames > 1:\\n            if self.SamplesPerPixel > 1:\\n                # TODO: Handle Planar Configuration attribute\\n                assert self.PlanarConfiguration == 0\\n                pixel_array = pixel_array.reshape(self.NumberOfFrames, self.Rows, self.Columns, self.SamplesPerPixel)\\n            else:\\n                pixel_array = pixel_array.reshape(self.NumberOfFrames, self.Rows, self.Columns)\\n        else:\\n            if self.SamplesPerPixel > 1:\\n                if self.BitsAllocated == 8:\\n                    if self.PlanarConfiguration == 0:\\n                        pixel_array = pixel_array.reshape(self.Rows, self.Columns, self.SamplesPerPixel)\\n                    else:\\n                        pixel_array = pixel_array.reshape(self.SamplesPerPixel, self.Rows, self.Columns)\\n                        pixel_array = pixel_array.transpose(1, 2, 0)\\n                else:\\n                    raise NotImplementedError(\\\"This code only handles SamplesPerPixel > 1 if Bits Allocated = 8\\\")\\n            else:\\n                pixel_array = pixel_array.reshape(self.Rows, self.Columns)\\n        return pixel_array\\n\\n    def _compressed_pixel_data_numpy(self):\\n        \\\"\\\"\\\"Return a NumPy array of the pixel data.\\n\\n        NumPy is a numerical package for python. It is used if available.\\n\\n        :raises TypeError: if no pixel data in this dataset.\\n        :raises ImportError: if cannot import numpy.\\n\\n        \\\"\\\"\\\"\\n        if 'PixelData' not in self:\\n            raise TypeError(\\\"No pixel data found in this dataset.\\\")\\n\\n        if not have_numpy:\\n            msg = \\\"The Numpy package is required to use pixel_array, and numpy could not be imported.\\\"\\n            raise ImportError(msg)\\n\\n        # determine the type used for the array\\n        need_byteswap = (self.is_little_endian != sys_is_little_endian)\\n\\n        # Make NumPy format code, e.g. \\\"uint16\\\", \\\"int32\\\" etc\\n        # from two pieces of info:\\n        #    self.PixelRepresentation -- 0 for unsigned, 1 for signed;\\n        #    self.BitsAllocated -- 8, 16, or 32\\n        format_str = '%sint%d' % (('u', '')[self.PixelRepresentation],\\n                                  self.BitsAllocated)\\n        try:\\n            numpy_format = numpy.dtype(format_str)\\n        except TypeError:\\n            msg = (\\\"Data type not understood by NumPy: \\\"\\n                   \\\"format='%s', PixelRepresentation=%d, BitsAllocated=%d\\\")\\n            raise TypeError(msg % (numpy_format, self.PixelRepresentation,\\n                            self.BitsAllocated))\\n        if self.file_meta.TransferSyntaxUID in pydicom.uid.PILSupportedCompressedPixelTransferSyntaxes:\\n            UncompressedPixelData = self._get_PIL_supported_compressed_pixeldata()\\n        elif self.file_meta.TransferSyntaxUID in pydicom.uid.JPEGLSSupportedCompressedPixelTransferSyntaxes:\\n            UncompressedPixelData = self._get_jpeg_ls_supported_compressed_pixeldata()\\n        else:\\n            msg = \\\"The transfer syntax {0} is not currently supported.\\\".format(self.file_meta.TransferSyntaxUID)\\n            raise NotImplementedError(msg)\\n\\n        # Have correct Numpy format, so create the NumPy array\\n        arr = numpy.fromstring(UncompressedPixelData, numpy_format)\\n\\n        # XXX byte swap - may later handle this in read_file!!?\\n        if need_byteswap:\\n            arr.byteswap(True)  # True means swap in-place, don't make a new copy\\n        # Note the following reshape operations return a new *view* onto arr, but don't copy the data\\n        if 'NumberOfFrames' in self and self.NumberOfFrames > 1:\\n            if self.SamplesPerPixel > 1:\\n                arr = arr.reshape(self.NumberOfFrames, self.Rows, self.Columns,  self.SamplesPerPixel)\\n            else:\\n                arr = arr.reshape(self.NumberOfFrames, self.Rows, self.Columns)\\n        else:\\n            if self.SamplesPerPixel > 1:\\n                if self.BitsAllocated == 8:\\n                    if self.PlanarConfiguration == 0:\\n                        arr = arr.reshape(self.Rows, self.Columns, self.SamplesPerPixel)\\n                    else:\\n                        arr = arr.reshape(self.SamplesPerPixel, self.Rows, self.Columns)\\n                        arr = arr.transpose(1, 2, 0)\\n                else:\\n                    raise NotImplementedError(\\\"This code only handles SamplesPerPixel > 1 if Bits Allocated = 8\\\")\\n            else:\\n                arr = arr.reshape(self.Rows, self.Columns)\\n        if (self.file_meta.TransferSyntaxUID in pydicom.uid.JPEG2000CompressedPixelTransferSyntaxes and self.BitsStored == 16):\\n            # WHY IS THIS EVEN NECESSARY??\\n            arr &= 0x7FFF\\n        return arr\\n\\n    def _get_PIL_supported_compressed_pixeldata(self):\\n        if not have_pillow:\\n            msg = \\\"The pillow package is required to use pixel_array for this transfer syntax {0}, and pillow could not be imported.\\\".format(self.file_meta.TransferSyntaxUID)\\n            raise ImportError(msg)\\n        # decompress here\\n        if self.file_meta.TransferSyntaxUID in pydicom.uid.JPEGLossyCompressedPixelTransferSyntaxes:\\n            if self.BitsAllocated > 8:\\n                raise NotImplementedError(\\\"JPEG Lossy only supported if Bits Allocated = 8\\\")\\n            generic_jpeg_file_header = b'\\\\xff\\\\xd8\\\\xff\\\\xe0\\\\x00\\\\x10JFIF\\\\x00\\\\x01\\\\x01\\\\x01\\\\x00\\\\x01\\\\x00\\\\x01\\\\x00\\\\x00'\\n            frame_start_from = 2\\n        elif self.file_meta.TransferSyntaxUID in pydicom.uid.JPEG2000CompressedPixelTransferSyntaxes:\\n            generic_jpeg_file_header = b''\\n            # generic_jpeg_file_header = b'\\\\x00\\\\x00\\\\x00\\\\x0C\\\\x6A\\\\x50\\\\x20\\\\x20\\\\x0D\\\\x0A\\\\x87\\\\x0A'\\n            frame_start_from = 0\\n        else:\\n            generic_jpeg_file_header = b''\\n            frame_start_from = 0\\n        try:\\n            UncompressedPixelData = ''\\n            if 'NumberOfFrames' in self and self.NumberOfFrames > 1:\\n                # multiple compressed frames\\n                CompressedPixelDataSeq = pydicom.encaps.decode_data_sequence(self.PixelData)\\n                for frame in CompressedPixelDataSeq:\\n                    data = generic_jpeg_file_header + frame[frame_start_from:]\\n                    fio = io.BytesIO(data)\\n                    try:\\n                        decompressed_image = PILImg.open(fio)\\n                    except IOError as e:\\n                        raise NotImplementedError(e.message)\\n                    UncompressedPixelData += decompressed_image.tobytes()\\n            else:\\n                # single compressed frame\\n                UncompressedPixelData = pydicom.encaps.defragment_data(self.PixelData)\\n                UncompressedPixelData = generic_jpeg_file_header + UncompressedPixelData[frame_start_from:]\\n                try:\\n                    fio = io.BytesIO(UncompressedPixelData)\\n                    decompressed_image = PILImg.open(fio)\\n                except IOError as e:\\n                    raise NotImplementedError(e.message)\\n                UncompressedPixelData = decompressed_image.tobytes()\\n        except:\\n            raise\\n        return UncompressedPixelData\\n\\n    def _get_jpeg_ls_supported_compressed_pixeldata(self):\\n        if not have_jpeg_ls:\\n            msg = \\\"The jpeg_ls package is required to use pixel_array for this transfer syntax {0}, and jpeg_ls could not be imported.\\\".format(self.file_meta.TransferSyntaxUID)\\n            raise ImportError(msg)\\n        # decompress here\\n        UncompressedPixelData = ''\\n        if 'NumberOfFrames' in self and self.NumberOfFrames > 1:\\n            # multiple compressed frames\\n            CompressedPixelDataSeq = pydicom.encaps.decode_data_sequence(self.PixelData)\\n            # print len(CompressedPixelDataSeq)\\n            for frame in CompressedPixelDataSeq:\\n                decompressed_image = jpeg_ls.decode(numpy.fromstring(frame, dtype=numpy.uint8))\\n                UncompressedPixelData += decompressed_image.tobytes()\\n        else:\\n            # single compressed frame\\n            CompressedPixelData = pydicom.encaps.defragment_data(self.PixelData)\\n            decompressed_image = jpeg_ls.decode(numpy.fromstring(CompressedPixelData, dtype=numpy.uint8))\\n            UncompressedPixelData = decompressed_image.tobytes()\\n        return UncompressedPixelData\\n\\n    # Use by pixel_array property\\n    def _get_pixel_array(self):\\n        # Check if already have converted to a NumPy array\\n        # Also check if self.PixelData has changed. If so, get new NumPy array\\n        already_have = True\\n        if not hasattr(self, \\\"_pixel_array\\\"):\\n            already_have = False\\n        elif self._pixel_id != id(self.PixelData):\\n            already_have = False\\n        if not already_have and not self._is_uncompressed_transfer_syntax():\\n            try:\\n                # print(\\\"Pixel Data is compressed\\\")\\n                self._pixel_array = self._compressed_pixel_data_numpy()\\n                self._pixel_id = id(self.PixelData)  # is this guaranteed to work if memory is re-used??\\n                return self._pixel_array\\n            except IOError as I:\\n                logger.info(\\\"Pillow or JPLS did not support this transfer syntax\\\")\\n        if not already_have:\\n            self._pixel_array = self._pixel_data_numpy()\\n            self._pixel_id = id(self.PixelData)  # is this guaranteed to work if memory is re-used??\\n        return self._pixel_array\\n\\n    @property\\n    def pixel_array(self):\\n        \\\"\\\"\\\"Return the pixel data as a NumPy array\\\"\\\"\\\"\\n        try:\\n            return self._get_pixel_array()\\n        except AttributeError:\\n            t, e, tb = sys.exc_info()\\n            val = PropertyError(\\\"AttributeError in pixel_array property: \\\" +\\n                                e.args[0])\\n            compat.reraise(PropertyError, val, tb)\\n\\n    # Format strings spec'd according to python string formatting options\\n    #    See http://docs.python.org/library/stdtypes.html#string-formatting-operations\\n    default_element_format = \\\"%(tag)s %(name)-35.35s %(VR)s: %(repval)s\\\"\\n    default_sequence_element_format = \\\"%(tag)s %(name)-35.35s %(VR)s: %(repval)s\\\"\\n\\n    def formatted_lines(self, element_format=default_element_format,\\n                        sequence_element_format=default_sequence_element_format,\\n                        indent_format=None):\\n        \\\"\\\"\\\"A generator to give back a formatted string representing each line\\n        one at a time. Example:\\n            for line in dataset.formatted_lines(\\\"%(name)s=%(repval)s\\\", \\\"SQ:%(name)s=%(repval)s\\\"):\\n                print(line)\\n        See the source code for default values which illustrate some of the names that can be used in the\\n        format strings\\n        indent_format -- not used in current version. Placeholder for future functionality.\\n        \\\"\\\"\\\"\\n        for data_element in self.iterall():\\n            # Get all the attributes possible for this data element (e.g.\\n            #   gets descriptive text name too)\\n            # This is the dictionary of names that can be used in the format string\\n            elem_dict = dict([(x, getattr(data_element, x)()\\n                               if callable(getattr(data_element, x))\\n                               else getattr(data_element, x))\\n                              for x in dir(data_element) if not x.startswith(\\\"_\\\")])\\n            if data_element.VR == \\\"SQ\\\":\\n                yield sequence_element_format % elem_dict\\n            else:\\n                yield element_format % elem_dict\\n\\n    def _pretty_str(self, indent=0, top_level_only=False):\\n        \\\"\\\"\\\"Return a string of the data_elements in this dataset, with indented levels.\\n\\n        This private method is called by the __str__() method\\n        for handling print statements or str(dataset), and the __repr__() method.\\n        It is also used by top(), which is the reason for the top_level_only flag.\\n        This function recurses, with increasing indentation levels.\\n\\n        \\\"\\\"\\\"\\n        strings = []\\n        indent_str = self.indent_chars * indent\\n        nextindent_str = self.indent_chars * (indent + 1)\\n        for data_element in self:\\n            with tag_in_exception(data_element.tag):\\n                if data_element.VR == \\\"SQ\\\":   # a sequence\\n                    strings.append(indent_str + str(data_element.tag) + \\\"  %s   %i item(s) ---- \\\" % (data_element.description(), len(data_element.value)))\\n                    if not top_level_only:\\n                        for dataset in data_element.value:\\n                            strings.append(dataset._pretty_str(indent + 1))\\n                            strings.append(nextindent_str + \\\"---------\\\")\\n                else:\\n                    strings.append(indent_str + repr(data_element))\\n        return \\\"\\\\n\\\".join(strings)\\n\\n    def remove_private_tags(self):\\n        \\\"\\\"\\\"Remove all Dicom private tags in this dataset and those contained within.\\\"\\\"\\\"\\n        def RemoveCallback(dataset, data_element):\\n            \\\"\\\"\\\"Internal method to use as callback to walk() method.\\\"\\\"\\\"\\n            if data_element.tag.is_private:\\n                # can't del self[tag] - won't be right dataset on recursion\\n                del dataset[data_element.tag]\\n        self.walk(RemoveCallback)\\n\\n    def save_as(self, filename, write_like_original=True):\\n        \\\"\\\"\\\"Write the dataset to a file.\\n\\n        Parameters\\n        ----------\\n        filename : str\\n            Name of file to save new DICOM file to.\\n        write_like_original : boolean\\n            If True (default), preserves the following information from\\n            the dataset:\\n            -preamble -- if no preamble in read file, than not used here\\n            -hasFileMeta -- if writer did not do file meta information,\\n                then don't write here either\\n            -seq.is_undefined_length -- if original had delimiters, write them now too,\\n                instead of the more sensible length characters\\n            - is_undefined_length_sequence_item -- for datasets that belong to a\\n                sequence, write the undefined length delimiters if that is\\n                what the original had.\\n            If False, produces a \\\"nicer\\\" DICOM file for other readers,\\n                where all lengths are explicit.\\n\\n        See Also\\n        --------\\n        pydicom.filewriter.write_file\\n            Write a DICOM file from a FileDataset instance.\\n\\n        Notes\\n        -----\\n        Set dataset.preamble if you want something other than 128 0-bytes.\\n        If the dataset was read from an existing dicom file, then its preamble\\n        was stored at read time. It is up to the user to ensure the preamble is still\\n        correct for its purposes.\\n\\n        If there is no Transfer Syntax tag in the dataset, then set\\n        dataset.is_implicit_VR and dataset.is_little_endian\\n        to determine the transfer syntax used to write the file.\\n        \\\"\\\"\\\"\\n        pydicom.write_file(filename, self, write_like_original)\\n\\n    def __setattr__(self, name, value):\\n        \\\"\\\"\\\"Intercept any attempts to set a value for an instance attribute.\\n\\n        If name is a dicom descriptive string (cleaned with CleanName),\\n        then set the corresponding tag and data_element.\\n        Else, set an instance (python) attribute as any other class would do.\\n\\n        \\\"\\\"\\\"\\n        tag = tag_for_name(name)\\n        if tag is not None:  # successfully mapped name to a tag\\n            if tag not in self:  # don't have this tag yet->create the data_element instance\\n                VR = dictionaryVR(tag)\\n                data_element = DataElement(tag, VR, value)\\n            else:  # already have this data_element, just changing its value\\n                data_element = self[tag]\\n                data_element.value = value\\n            # Now have data_element - store it in this dict\\n            self[tag] = data_element\\n        else:  # name not in dicom dictionary - setting a non-dicom instance attribute\\n            # XXX note if user mis-spells a dicom data_element - no error!!!\\n            self.__dict__[name] = value\\n\\n    def __setitem__(self, key, value):\\n        \\\"\\\"\\\"Operator for dataset[key]=value. Check consistency, and deal with private tags\\\"\\\"\\\"\\n        if not isinstance(value, (DataElement, RawDataElement)):  # ok if is subclass, e.g. DeferredDataElement\\n            raise TypeError(\\\"Dataset contents must be DataElement instances.\\\\n\\\"\\n                            \\\"To set a data_element value use data_element.value=val\\\")\\n        tag = Tag(value.tag)\\n        if key != tag:\\n            raise ValueError(\\\"data_element.tag must match the dictionary key\\\")\\n\\n        data_element = value\\n        if tag.is_private:\\n            # See PS 3.5-2008 section 7.8.1 (p. 44) for how blocks are reserved\\n            logger.debug(\\\"Setting private tag %r\\\" % tag)\\n            private_block = tag.elem >> 8\\n            private_creator_tag = Tag(tag.group, private_block)\\n            if private_creator_tag in self and tag != private_creator_tag:\\n                if isinstance(data_element, RawDataElement):\\n                    data_element = DataElement_from_raw(data_element, self._character_set)\\n                data_element.private_creator = self[private_creator_tag].value\\n        dict.__setitem__(self, tag, data_element)\\n\\n    def __str__(self):\\n        \\\"\\\"\\\"Handle str(dataset).\\\"\\\"\\\"\\n        return self._pretty_str()\\n\\n    def top(self):\\n        \\\"\\\"\\\"Show the DICOM tags, but only the top level; do not recurse into Sequences\\\"\\\"\\\"\\n        return self._pretty_str(top_level_only=True)\\n\\n    def trait_names(self):\\n        \\\"\\\"\\\"Return a list of valid names for auto-completion code\\n        Used in IPython, so that data element names can be found\\n        and offered for autocompletion on the IPython command line\\n        \\\"\\\"\\\"\\n        return dir(self)  # only valid python >=2.6, else use self.__dir__()\\n\\n    def update(self, dictionary):\\n        \\\"\\\"\\\"Extend dict.update() to handle DICOM keywords.\\\"\\\"\\\"\\n        for key, value in list(dictionary.items()):\\n            if isinstance(key, (str, compat.text_type)):\\n                setattr(self, key, value)\\n            else:\\n                self[Tag(key)] = value\\n\\n    def iterall(self):\\n        \\\"\\\"\\\"Iterate through the dataset, yielding all data elements.\\n\\n        Unlike Dataset.__iter__, this *does* recurse into sequences,\\n        and so returns all data elements as if the file were \\\"flattened\\\".\\n        \\\"\\\"\\\"\\n        for data_element in self:\\n            yield data_element\\n            if data_element.VR == \\\"SQ\\\":\\n                sequence = data_element.value\\n                for dataset in sequence:\\n                    for elem in dataset.iterall():\\n                        yield elem\\n\\n    def walk(self, callback, recursive=True):\\n        \\\"\\\"\\\"Walk over given function for all dataset data_elements.\\n\\n        Visit all data_elements, possibly recursing into sequences and their datasets,\\n        The callback function is called for each data_element\\n        (including SQ element).\\n        Can be used to perform an operation on certain types of data_elements.\\n        E.g., `remove_private_tags`() finds all private tags and deletes them.\\n        `DataElement`s will come back in DICOM order (by increasing tag number\\n        within their dataset)\\n\\n        Parameters\\n        ----------\\n        callback: a callable that takes two arguments: a dataset, and\\n                  a data_element belonging to that dataset.\\n        recursive : boolean\\n            Flag to indicate whether to recurse into Sequences\\n        \\\"\\\"\\\"\\n        taglist = sorted(self.keys())\\n        for tag in taglist:\\n\\n            with tag_in_exception(tag):\\n                data_element = self[tag]\\n                callback(self, data_element)  # self = this Dataset\\n                # 'tag in self' below needed in case callback deleted data_element\\n                if recursive and tag in self and data_element.VR == \\\"SQ\\\":\\n                    sequence = data_element.value\\n                    for dataset in sequence:\\n                        dataset.walk(callback)\\n\\n    __repr__ = __str__\"}, {\"identifier\":\"FileDataset\", \"path\":\"pydicom/dataset.py\", \"snippet\":\"class FileDataset(Dataset):\\n    def __init__(self, filename_or_obj, dataset, preamble=None, file_meta=None,\\n                 is_implicit_VR=True, is_little_endian=True):\\n        \\\"\\\"\\\"Initialize a dataset read from a DICOM file.\\n\\n        Parameters\\n        ----------\\n        filename_or_obj : str, None\\n            Full path and filename to the file. Use None if is a BytesIO.\\n        dataset : Dataset, dict\\n            Some form of dictionary, usually a Dataset from read_dataset()\\n        preamble : None, optional\\n            The 128-byte DICOM preamble\\n        file_meta : None, optional\\n            The file meta info dataset, as returned by _read_file_meta,\\n            or an empty dataset if no file meta information is in the file.\\n        is_implicit_VR : boolean, optional\\n            True (default) if implicit VR transfer syntax used; False if explicit VR.\\n        is_little_endian : boolean\\n            True (default) if little-endian transfer syntax used; False if big-endian.\\n        \\\"\\\"\\\"\\n        Dataset.__init__(self, dataset)\\n        self.preamble = preamble\\n        self.file_meta = file_meta\\n        self.is_implicit_VR = is_implicit_VR\\n        self.is_little_endian = is_little_endian\\n        if isinstance(filename_or_obj, compat.string_types):\\n            self.filename = filename_or_obj\\n            self.fileobj_type = open\\n        elif isinstance(filename_or_obj, io.BufferedReader):\\n            self.filename = filename_or_obj.name\\n            # This is the appropriate constructor for io.BufferedReader\\n            self.fileobj_type = open\\n        else:\\n            self.fileobj_type = filename_or_obj.__class__  # use __class__ python <2.7?; http://docs.python.org/reference/datamodel.html\\n            if getattr(filename_or_obj, \\\"name\\\", False):\\n                self.filename = filename_or_obj.name\\n            elif getattr(filename_or_obj, \\\"filename\\\", False):  # gzip python <2.7?\\n                self.filename = filename_or_obj.filename\\n            else:\\n                self.filename = None  # e.g. came from BytesIO or something file-like\\n        self.timestamp = None\\n        if stat_available and self.filename and os.path.exists(self.filename):\\n            statinfo = os.stat(self.filename)\\n            self.timestamp = statinfo.st_mtime\"}, {\"identifier\":\"DicomDir\", \"path\":\"pydicom/dicomdir.py\", \"snippet\":\"class DicomDir(FileDataset):\\n    \\\"\\\"\\\"Hold a DICOMDIR dataset read from file.\\n\\n    Derived from FileDataset, but additional methods are available,\\n    specific to the Directory structure\\n    \\\"\\\"\\\"\\n    def __init__(self, filename_or_obj, dataset, preamble=None, file_meta=None,\\n                 is_implicit_VR=True, is_little_endian=True):\\n        \\\"\\\"\\\"Initialize a DICOMDIR dataset read from a DICOM file\\n        Carries forward all the initialization from FileDataset class\\n\\n        :param filename: full path and filename to the file. Use None if is a BytesIO.\\n        :param dataset: some form of dictionary, usually a Dataset from read_dataset()\\n        :param preamble: the 128-byte DICOM preamble\\n        :param file_meta: the file meta info dataset, as returned by _read_file_meta,\\n                or an empty dataset if no file meta information is in the file\\n        :param is_implicit_VR: True if implicit VR transfer syntax used; False if explicit VR. Default is True.\\n        :param is_little_endian: True if little-endian transfer syntax used; False if big-endian. Default is True.\\n        \\\"\\\"\\\"\\n        # Usually this class is created through filereader.read_partial,\\n        # and it checks class SOP, but in case of direct creation,\\n        # check here also\\n        if file_meta:\\n            class_uid = file_meta.MediaStorageSOPClassUID\\n            if not class_uid == \\\"Media Storage Directory Storage\\\":\\n                msg = \\\"SOP Class is not Media Storage Directory (DICOMDIR)\\\"\\n                raise InvalidDicomError(msg)\\n        FileDataset.__init__(self, filename_or_obj, dataset,\\n                             preamble, file_meta,\\n                             is_implicit_VR=True, is_little_endian=True)\\n        self.parse_records()\\n\\n    def parse_records(self):\\n        \\\"\\\"\\\"Build the hierarchy of given directory records, and structure\\n        into Patient, Studies, Series, Images hierarchy.\\n\\n        This is intended for initial read of file only,\\n        it will not reorganize correctly if records are changed.\\n\\n        :return: None\\n        \\\"\\\"\\\"\\n        # Define a helper function for organizing the records\\n        def get_siblings(record, map_offset_to_record):\\n            \\\"\\\"\\\"Return a list of all siblings of the given directory record,\\n            including itself.\\n            \\\"\\\"\\\"\\n            sibling_list = [record]\\n            current_record = record\\n            while current_record.OffsetOfTheNextDirectoryRecord:\\n                offset_of_next = current_record.OffsetOfTheNextDirectoryRecord\\n                sibling = map_offset_to_record[offset_of_next]\\n                sibling_list.append(sibling)\\n                current_record = sibling\\n            return sibling_list\\n\\n        # Build the mapping from file offsets to records\\n        records = self.DirectoryRecordSequence\\n        map_offset_to_record = {}\\n        for record in records:\\n            offset = record.seq_item_tell\\n            map_offset_to_record[offset] = record\\n        # logging.debug(\\\"Record offsets: \\\" + map_offset_to_record.keys())\\n\\n        # Find the children of each record\\n        for record in records:\\n            child_offset = record.OffsetOfReferencedLowerLevelDirectoryEntity\\n            if child_offset:\\n                child = map_offset_to_record[child_offset]\\n                record.children = get_siblings(child, map_offset_to_record)\\n            else:\\n                record.children = []\\n\\n        # Find the top-level records : siblings of the first record\\n        self.patient_records = get_siblings(records[0], map_offset_to_record)\"}, {\"identifier\":\"dictionaryVR\", \"path\":\"pydicom/datadict.py\", \"snippet\":\"def dictionaryVR(tag):\\n    \\\"\\\"\\\"Return the dicom value representation for the given dicom tag.\\\"\\\"\\\"\\n    return get_entry(tag)[0]\"}, {\"identifier\":\"DataElement\", \"path\":\"pydicom/dataelem.py\", \"snippet\":\"class DataElement(object):\\n    \\\"\\\"\\\"Contain and manipulate a Dicom data element, having a tag, VR, VM and value.\\n\\n    Most user code will not create data elements using this class directly,\\n    but rather through DICOM keywords in Dataset objects.\\n    See the Dataset class for a description of how Datasets, Sequences,\\n    and DataElements work.\\n\\n    Class Data\\n    ----------\\n    For string display (via __str__), the following are used:\\n\\n    descripWidth -- maximum width of description field (default 35).\\n    maxBytesToDisplay -- longer data will display \\\"array of # bytes\\\" (default 16).\\n    showVR -- True (default) to include the dicom VR just before the value.\\n\\n    Attributes\\n    ----------\\n    is_retired : bool\\n        For officially registered DICOM Data Elements this will be True if the\\n        retired status as given in PS3.6 Table 6-1 is 'RET'. For private or\\n        unknown Elements this will always be False\\n    keyword : str\\n        For officially registered DICOM Data Elements this will be the Keyword\\n        as given in PS3.6 Table 6-1. For private or unknown Elements this will\\n        return an empty string.\\n    name : str\\n        For officially registered DICOM Data Elements this will be the Name\\n        as given in PS3.6 Table 6-1. For private Elements known to pydicom this\\n        will be the Name in the format '[name]'. For unknown private Elements\\n        this will be 'Private Creator'. For unknown Elements this will return\\n        an empty string.\\n    tag : pydicom.tag.Tag\\n        The DICOM Tag for the Data Element\\n    value\\n        The Data Element's stored value(s)\\n    VM : int\\n        The Value Multiplicity of the Data Element's stored value(s)\\n    VR : str\\n        The Data Element's Value Representation value\\n    \\\"\\\"\\\"\\n    descripWidth = 35\\n    maxBytesToDisplay = 16\\n    showVR = 1\\n\\n    # Python 2: Classes which define __eq__ should flag themselves as unhashable\\n    __hash__ = None\\n\\n    def __init__(self, tag, VR, value, file_value_tell=None,\\n                 is_undefined_length=False, already_converted=False):\\n        \\\"\\\"\\\"Create a data element instance.\\n\\n        Most user code should instead use DICOM keywords\\n        to create data_elements, for which only the value is supplied,\\n        and the VR and tag are determined from the dicom dictionary.\\n\\n        tag -- dicom (group, element) tag in any form accepted by Tag().\\n        VR -- dicom value representation (see DICOM standard part 6)\\n        value -- the value of the data element. One of the following:\\n            - a single string value\\n            - a number\\n            - a list or tuple with all strings or all numbers\\n            - a multi-value string with backslash separator\\n        file_value_tell -- used internally by Dataset, to store the write\\n            position for ReplaceDataElementValue method\\n        is_undefined_length -- used internally to store whether the length\\n            field in this data element was 0xFFFFFFFFL, i.e. \\\"undefined length\\\"\\n\\n        \\\"\\\"\\\"\\n        self.tag = Tag(tag)\\n        self.VR = VR  # Note!: you must set VR before setting value\\n        if already_converted:\\n            self._value = value\\n        else:\\n            self.value = value  # calls property setter which will convert\\n        self.file_tell = file_value_tell\\n        self.is_undefined_length = is_undefined_length\\n\\n    @property\\n    def value(self):\\n        \\\"\\\"\\\"The value (possibly multiple values) of this data_element\\\"\\\"\\\"\\n        return self._value\\n\\n    @value.setter\\n    def value(self, val):\\n        \\\"\\\"\\\"Set method for 'value' property\\\"\\\"\\\"\\n        # Check if is a string with multiple values separated by '\\\\'\\n        # If so, turn them into a list of separate strings\\n        if isString(val) and self.VR not in \\\\\\n                ['UT', 'ST', 'LT', 'FL', 'FD', 'AT', 'OB', 'OW', 'OF', 'SL', 'SQ', 'SS',\\n                 'UL', 'OB/OW', 'OW/OB', 'OB or OW', 'OW or OB', 'UN'] and 'US' not in self.VR:  # latter covers 'US or SS' etc\\n            if _backslash in val:\\n                val = val.split(_backslash)\\n        self._value = self._convert_value(val)\\n\\n    @property\\n    def VM(self):\\n        \\\"\\\"\\\"The number of values in the data_element's 'value'\\\"\\\"\\\"\\n        if isMultiValue(self.value):\\n            return len(self.value)\\n        else:\\n            return 1\\n\\n    def _convert_value(self, val):\\n        \\\"\\\"\\\"Convert Dicom string values if possible to e.g. numbers. Handle the case\\n        of multiple value data_elements\\\"\\\"\\\"\\n        if self.VR == 'SQ':  # a sequence - leave it alone\\n            from pydicom.sequence import Sequence\\n            if isinstance(val, Sequence):\\n                return val\\n            else:\\n                return Sequence(val)\\n\\n        # if the value is a list, convert each element\\n        try:\\n            val.append\\n        except AttributeError:  # not a list\\n            return self._convert(val)\\n        else:\\n            returnvalue = []\\n            for subval in val:\\n                returnvalue.append(self._convert(subval))\\n            return returnvalue\\n\\n    def _convert(self, val):\\n        \\\"\\\"\\\"Take the value and convert to number, etc if possible\\\"\\\"\\\"\\n        if self.VR == 'IS':\\n            return pydicom.valuerep.IS(val)\\n        elif self.VR == 'DA' and config.datetime_conversion:\\n            return pydicom.valuerep.DA(val)\\n        elif self.VR == 'DS':\\n            return pydicom.valuerep.DS(val)\\n        elif self.VR == 'DT' and config.datetime_conversion:\\n            return pydicom.valuerep.DT(val)\\n        elif self.VR == 'TM' and config.datetime_conversion:\\n            return pydicom.valuerep.TM(val)\\n        elif self.VR == \\\"UI\\\":\\n            return UID(val)\\n        elif not in_py2 and self.VR == \\\"PN\\\":\\n            return PersonName(val)\\n        # Later may need this for PersonName as for UI,\\n        #    but needs more thought\\n        # elif self.VR == \\\"PN\\\":\\n        #    return PersonName(val)\\n        else:  # is either a string or a type 2 optionally blank string\\n            return val  # this means a \\\"numeric\\\" value could be empty string \\\"\\\"\\n        # except TypeError:\\n            # print \\\"Could not convert value '%s' to VR '%s' in tag %s\\\" \\\\\\n            # % (repr(val), self.VR, self.tag)\\n        # except ValueError:\\n            # print \\\"Could not convert value '%s' to VR '%s' in tag %s\\\" \\\\\\n            # % (repr(val), self.VR, self.tag)\\n\\n    def __eq__(self, other):\\n        \\\"\\\"\\\"\\n        Compare `self` and `other` for equality\\n\\n        Returns\\n        -------\\n        bool\\n            The result if `self` and `other` are the same class\\n        NotImplemented\\n            If `other` is not the same class as `self` then returning\\n            NotImplemented delegates the result to superclass.__eq__(subclass)\\n        \\\"\\\"\\\"\\n        # Faster result if same object\\n        if other is self:\\n            return True\\n\\n        if isinstance(other, self.__class__):\\n            return self.__dict__ == other.__dict__\\n\\n        return NotImplemented\\n\\n    def __ne__(self, other):\\n        \\\"\\\"\\\" Compare `self` and `other` for inequality \\\"\\\"\\\"\\n        return not (self == other)\\n\\n    def __str__(self):\\n        \\\"\\\"\\\"Return str representation of this data_element\\\"\\\"\\\"\\n        repVal = self.repval\\n        if self.showVR:\\n            s = \\\"%s %-*s %s: %s\\\" % (str(self.tag), self.descripWidth,\\n                                    self.description()[:self.descripWidth], self.VR, repVal)\\n        else:\\n            s = \\\"%s %-*s %s\\\" % (str(self.tag), self.descripWidth,\\n                                self.description()[:self.descripWidth], repVal)\\n        return s\\n\\n    @property\\n    def repval(self):\\n        \\\"\\\"\\\"Return a str representation of the current value for use in __str__\\\"\\\"\\\"\\n        byte_VRs = ['OB', 'OW', 'OW/OB', 'OW or OB', 'OB or OW', 'US or SS or OW', 'US or SS']\\n        if (self.VR in byte_VRs and len(self.value) > self.maxBytesToDisplay):\\n            repVal = \\\"Array of %d bytes\\\" % len(self.value)\\n        elif hasattr(self, 'original_string'):  # for VR of IS or DS\\n            repVal = repr(self.original_string)\\n        elif isinstance(self.value, UID):\\n            repVal = self.value.name\\n        else:\\n            repVal = repr(self.value)  # will tolerate unicode too\\n        return repVal\\n\\n    def __unicode__(self):\\n        \\\"\\\"\\\"Return unicode representation of this data_element\\\"\\\"\\\"\\n        if isinstance(self.value, compat.text_type):\\n            # start with the string rep then replace the value part with the unicode\\n            strVal = str(self)\\n            uniVal = compat.text_type(strVal.replace(self.repval, \\\"\\\")) + self.value\\n            return uniVal\\n        else:\\n            return compat.text_type(str(self))\\n\\n    def __getitem__(self, key):\\n        \\\"\\\"\\\"Returns the item from my value's Sequence, if it is one.\\\"\\\"\\\"\\n        try:\\n            return self.value[key]\\n        except TypeError:\\n            raise TypeError(\\\"DataElement value is unscriptable (not a Sequence)\\\")\\n\\n    @property\\n    def name(self):\\n        return self.description()\\n\\n    def description(self):\\n        \\\"\\\"\\\"Return the DICOM dictionary description for this dicom tag.\\\"\\\"\\\"\\n        if dictionary_has_tag(self.tag):\\n            name = dictionary_description(self.tag)\\n        elif self.tag.is_private:\\n            name = \\\"Private tag data\\\"  # default\\n            if hasattr(self, 'private_creator'):\\n                try:\\n                    # If have name from private dictionary, use it, but\\n                    #   but put in square brackets so is differentiated,\\n                    #   and clear that cannot access it by name\\n                    name = \\\"[\\\" + private_dictionary_description(self.tag, self.private_creator) + \\\"]\\\"\\n                except KeyError:\\n                    pass\\n            elif self.tag.elem >> 8 == 0:\\n                name = \\\"Private Creator\\\"\\n        elif self.tag.element == 0:  # implied Group Length dicom versions < 3\\n            name = \\\"Group Length\\\"\\n        else:\\n            name = \\\"\\\"\\n        return name\\n\\n    @property\\n    def is_retired(self):\\n        \\\"\\\"\\\"The data_element's retired status\\\"\\\"\\\"\\n        if dictionary_has_tag(self.tag):\\n            return dictionary_is_retired(self.tag)\\n        else:\\n            return False\\n\\n    @property\\n    def keyword(self):\\n        \\\"\\\"\\\"The data_element's keyword (if known)\\\"\\\"\\\"\\n        if dictionary_has_tag(self.tag):\\n            return dictionary_keyword(self.tag)\\n        else:\\n            return ''\\n\\n    def __repr__(self):\\n        \\\"\\\"\\\"Handle repr(data_element)\\\"\\\"\\\"\\n        if self.VR == \\\"SQ\\\":\\n            return repr(self.value)\\n        else:\\n            return str(self)\"}, {\"identifier\":\"ItemTag\", \"path\":\"pydicom/tag.py\", \"snippet\":\"def Tag(arg, arg2=None):\\n    def __lt__(self, other):\\n    def __eq__(self, other):\\n    def __ne__(self, other):\\n    def __str__(self):\\n    def group(self):\\n    def element(self):\\n    def is_private(self):\\ndef TupleTag(group_elem):\\nclass BaseTag(BaseTag_base_class):\"}, {\"identifier\":\"Sequence\", \"path\":\"pydicom/sequence.py\", \"snippet\":\"class Sequence(MultiValue):\\n    \\\"\\\"\\\"Class to hold multiple Datasets in a list\\n\\n    This class is derived from MultiValue and as such enforces that all items\\n    added to the list are Dataset instances. In order to due this, a validator\\n    is substituted for type_constructor when constructing the MultiValue super\\n    class\\n    \\\"\\\"\\\"\\n    def __init__(self, iterable=None):\\n        \\\"\\\"\\\"Initialize a list of Datasets\\n\\n        :param iterable: an iterable (e.g. list, tuple) of Datasets. If no\\n                        value is provided, an empty Sequence is generated\\n        \\\"\\\"\\\"\\n        # We add this extra check to throw a relevant error. Without it, the\\n        # error will be simply that a Sequence must contain Datasets (since a\\n        # Dataset IS iterable). This error, however, doesn't inform the user\\n        # that the actual issue is that their Dataset needs to be INSIDE an\\n        # iterable object\\n        if isinstance(iterable, Dataset):\\n            raise TypeError('The Sequence constructor requires an iterable')\\n\\n        # If no inputs are provided, we create an empty Sequence\\n        if not iterable:\\n            iterable = list()\\n\\n        # validate_dataset is used as a pseudo type_constructor\\n        super(Sequence, self).__init__(validate_dataset, iterable)\\n\\n    def __str__(self):\\n        lines = [str(x) for x in self]\\n        return \\\"[\\\" + \\\"\\\".join(lines) + \\\"]\\\"\\n\\n    def __repr__(self):\\n        \\\"\\\"\\\"Sequence-specific string representation\\\"\\\"\\\"\\n        formatstr = \\\"<%(classname)s, length %(count)d, at %(id)X>\\\"\\n        return formatstr % {'classname': self.__class__.__name__,\\n                            'id': id(self), 'count': len(self)}\"}, {\"identifier\":\"read_undefined_length_value\", \"path\":\"pydicom/fileutil.py\", \"snippet\":\"def read_undefined_length_value(fp, is_little_endian, delimiter_tag, defer_size=None,\\n                                read_size=128):\\n    \\\"\\\"\\\"Read until the delimiter tag found and return the value; ignore the delimiter.\\n\\n    On completion, the file will be set to the first byte after the delimiter and its\\n    following four zero bytes.\\n\\n    Parameters\\n    ----------\\n    fp : a file-like object\\n    is_little_endian : boolean\\n        True if file transfer syntax is little endian, else False.\\n    read_size : int\\n        Number of bytes to read at one time.\\n\\n    Returns\\n    -------\\n    delimiter : str, None\\n        The file delimiter\\n\\n    Raises\\n    ------\\n    EOFError\\n        If EOF is reached before delimiter found.\\n    \\\"\\\"\\\"\\n    data_start = fp.tell()\\n    search_rewind = 3\\n\\n    if is_little_endian:\\n        bytes_format = b\\\"<HH\\\"\\n    else:\\n        bytes_format = b\\\">HH\\\"\\n    bytes_to_find = pack(bytes_format, delimiter_tag.group, delimiter_tag.elem)\\n\\n    found = False\\n    EOF = False\\n    value_chunks = []\\n    byte_count = 0  # for defer_size checks\\n    while not found:\\n        chunk_start = fp.tell()\\n        bytes_read = fp.read(read_size)\\n        if len(bytes_read) < read_size:\\n            # try again - if still don't get required amount, this is last block\\n            new_bytes = fp.read(read_size - len(bytes_read))\\n            bytes_read += new_bytes\\n            if len(bytes_read) < read_size:\\n                EOF = True  # but will still check whatever we did get\\n        index = bytes_read.find(bytes_to_find)\\n        if index != -1:\\n            found = True\\n            new_bytes = bytes_read[:index]\\n            byte_count += len(new_bytes)\\n            if defer_size is None or byte_count < defer_size:\\n                value_chunks.append(bytes_read[:index])\\n            fp.seek(chunk_start + index + 4)  # rewind to end of delimiter\\n            length = fp.read(4)\\n            if length != b\\\"\\\\0\\\\0\\\\0\\\\0\\\":\\n                msg = \\\"Expected 4 zero bytes after undefined length delimiter at pos {0:04x}\\\"\\n                logger.error(msg.format(fp.tell() - 4))\\n        elif EOF:\\n            fp.seek(data_start)\\n            raise EOFError(\\\"End of file reached before delimiter {0!r} found\\\".format(delimiter_tag))\\n        else:\\n            fp.seek(fp.tell() - search_rewind)  # rewind a bit in case delimiter crossed read_size boundary\\n            # accumulate the bytes read (not including the rewind)\\n            new_bytes = bytes_read[:-search_rewind]\\n            byte_count += len(new_bytes)\\n            if defer_size is None or byte_count < defer_size:\\n                value_chunks.append(new_bytes)\\n    # if get here then have found the byte string\\n    if defer_size is not None and defer_size >= defer_size:\\n        return None\\n    else:\\n        return b\\\"\\\".join(value_chunks)\"}]", "import_statement": "import os.path\nimport warnings\nimport zlib\nimport pydicom.uid  # for Implicit/Explicit/Little/Big Endian transfer syntax UIDs\nfrom io import BytesIO\nfrom pydicom.tag import TupleTag\nfrom pydicom.dataelem import RawDataElement\nfrom pydicom.util.hexutil import bytes2hex\nfrom pydicom.valuerep import extra_length_VRs\nfrom pydicom.charset import default_encoding, convert_encodings\nfrom pydicom.compat import in_py2\nfrom pydicom import compat\nfrom pydicom import config  # don't import datetime_conversion directly\nfrom pydicom.config import logger\n    from os import stat\nfrom pydicom.errors import InvalidDicomError\nfrom pydicom.filebase import DicomFile\nfrom pydicom.dataset import Dataset, FileDataset\nfrom pydicom.dicomdir import DicomDir\nfrom pydicom.datadict import dictionaryVR\nfrom pydicom.dataelem import DataElement\nfrom pydicom.tag import ItemTag, SequenceDelimiterTag\nfrom pydicom.sequence import Sequence\nfrom pydicom.fileutil import read_undefined_length_value\nfrom struct import Struct, unpack\nfrom sys import byteorder\n                from pydicom.values import convert_string\n                    from pydicom.values import convert_string\n        from pydicom.values import converters", "code": "    #       The 4-byte length can be FFFFFFFF (undefined length)*\n    # If Explicit VR:\n    #    if OB, OW, OF, SQ, UN, or UT:\n    #       tag, VR, 2-bytes reserved (both zero), 4-byte length, value\n    #           For all but UT, the length can be FFFFFFFF (undefined length)*\n    #   else: (any other VR)\n    #       tag, VR, (2 byte length), value\n    # * for undefined length, a Sequence Delimitation Item marks the end\n    #        of the Value Field.\n    # Note, except for the special_VRs, both impl and expl VR use 8 bytes;\n    #    the special VRs follow the 8 bytes with a 4-byte length\n\n    # With a generator, state is stored, so we can break down\n    #    into the individual cases, and not have to check them again for each\n    #    data element\n\n    if is_little_endian:\n        endian_chr = \"<\"\n    else:\n        endian_chr = \">\"\n    if is_implicit_VR:\n        element_struct = Struct(endian_chr + \"HHL\")\n    else:  # Explicit VR\n        # tag, VR, 2-byte length (or 0 if special VRs)\n        element_struct = Struct(endian_chr + \"HH2sH\")\n        extra_length_struct = Struct(endian_chr + \"L\")  # for special VRs\n        extra_length_unpack = extra_length_struct.unpack  # for lookup speed\n\n    # Make local variables so have faster lookup\n    fp_read = fp.read\n    fp_tell = fp.tell\n    logger_debug = logger.debug\n    debugging = config.debugging\n    element_struct_unpack = element_struct.unpack\n\n    while True:\n        # Read tag, VR, length, get ready to read value\n        bytes_read = fp_read(8)\n        if len(bytes_read) < 8:\n            return  # at end of file\n        if debugging:\n            debug_msg = \"{0:08x}: {1}\".format(fp.tell() - 8,\n                                              bytes2hex(bytes_read))\n\n        if is_implicit_VR:\n            # must reset VR each time; could have set last iteration (e.g. SQ)\n            VR = None\n            group, elem, length = element_struct_unpack(bytes_read)\n        else:  # explicit VR\n            group, elem, VR, length = element_struct_unpack(bytes_read)\n            if not in_py2:\n                VR = VR.decode(default_encoding)\n            if VR in extra_length_VRs:\n                bytes_read = fp_read(4)\n                length = extra_length_unpack(bytes_read)[0]\n                if debugging:\n                    debug_msg += \" \" + bytes2hex(bytes_read)\n        if debugging:\n            debug_msg = \"%-47s  (%04x, %04x)\" % (debug_msg, group, elem)\n            if not is_implicit_VR:\n                debug_msg += \" %s \" % VR\n            if length != 0xFFFFFFFF:\n                debug_msg += \"Length: %d\" % length\n            else:\n                debug_msg += \"Length: Undefined length (FFFFFFFF)\"\n            logger_debug(debug_msg)\n\n        # Positioned to read the value, but may not want to -- check stop_when\n        value_tell = fp_tell()\n        tag = TupleTag((group, elem))\n        if stop_when is not None:\n            # XXX VR may be None here!! Should stop_when just take tag?\n            if stop_when(tag, VR, length):\n                if debugging:\n                    logger_debug(\"Reading ended by stop_when callback. \"\n                                 \"Rewinding to start of data element.\")\n                rewind_length = 8\n                if not is_implicit_VR and VR in extra_length_VRs:\n                    rewind_length += 4\n                fp.seek(value_tell - rewind_length)\n                return\n\n        # Reading the value\n        # First case (most common): reading a value with a defined length\n        if length != 0xFFFFFFFF:\n            if defer_size is not None and length > defer_size:\n                # Flag as deferred by setting value to None, and skip bytes\n                value = None\n                logger_debug(\"Defer size exceeded. \"\n                             \"Skipping forward to next data element.\")\n                fp.seek(fp_tell() + length)\n            else:\n                value = fp_read(length)\n                if debugging:\n                    dotdot = \"   \"\n                    if length > 12:\n                        dotdot = \"...\"\n                    logger_debug(\"%08x: %-34s %s %r %s\" % (value_tell,\n                                                           bytes2hex(value[:12]), dotdot, value[:12], dotdot))\n\n            # If the tag is (0008,0005) Specific Character Set, then store it\n            if tag == (0x08, 0x05):\n                encoding = convert_string(value, is_little_endian, encoding=default_encoding)\n                # Store the encoding value in the generator for use with future elements (SQs)\n                encoding = convert_encodings(encoding)\n\n            yield RawDataElement(tag, VR, length, value, value_tell,\n                                 is_implicit_VR, is_little_endian)\n\n        # Second case: undefined length - must seek to delimiter,\n        # unless is SQ type, in which case is easier to parse it, because\n        # undefined length SQs and items of undefined lengths can be nested\n        # and it would be error-prone to read to the correct outer delimiter\n        else:\n            # Try to look up type to see if is a SQ\n            # if private tag, won't be able to look it up in dictionary,\n            #   in which case just ignore it and read the bytes unless it is\n            #   identified as a Sequence\n            if VR is None:\n                try:\n", "next_line": "                    VR = dictionaryVR(tag)", "gold_snippet_index": 14, "id": 19, "__internal_uuid__": "e5317dec-58b1-4d8e-bc37-04772450625b"}