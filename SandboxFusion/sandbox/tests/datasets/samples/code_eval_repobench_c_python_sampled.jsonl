{"repo_name": "evenge/EVENGE", "file_path": "index.py", "context": "# Path: _oo/classes/evento.py\n# class Evento(ndb.Model):\n#     nombre = ndb.StringProperty()\n#     tipo = ndb.StringProperty()\n#     privado = ndb.BooleanProperty()\n#     idCreador = ndb.StringProperty()\n#     hora = ndb.TimeProperty()\n#     fecha = ndb.DateProperty()\n#     fechaInsercion = ndb.DateTimeProperty(auto_now_add=True)\n#     lugar = ndb.StringProperty()\n#     coordenadas = ndb.GeoPtProperty(indexed=False)\n#     descripcion = ndb.TextProperty(indexed=False)\n#     asistencia = ndb.BooleanProperty(indexed=False)\n#     asistentes = ndb.StructuredProperty(Asistente, repeated=True)\n#     ponentes = ndb.StringProperty(repeated=True)\n# \n#     def getKey(self):\n#         return str(self.key.id())\n# \n# Path: _oo/model/controladorEvento.py\n# def GetEventoById(idEvento):\n# def SetEvento(nombre, tipo, privado, idCreador, hora, fecha, lugar, lat, lon, descripcion, asistencia):\n# def updateEvento(idE, nombre, tipo, privado, hora, fecha, lugar, lat, lon, descripcion, asistencia):\n# def setAsistente(idEvento, nom, ape, ema, tel, twi, dn):\n# def getUltimosEventos(num):\n# def getAsistentesEvento(idEvento):\n# def deleteEvento(idEvento):\n# def setPonente(idP, idE):\n# def deleteAsistente(idE, dni):\n# def confirmarAsistente(idE, dni):\n# \n# Path: _oo/model/controladorUsuario.py\n# def getKey(usuario):\n# def nuevoRegistroUsuario(nombre, apellidos, email, telefono, twitter, web, password, ciudad):\n# def loginCorrecto(email,password):\n# def getUsuarioById(idUsuario):\n# def getUsuarioLogeado(handler):\n# def listarUsuarios(self):\n# def getEventosAsociados(idUsuario):\n# def getEventosAsociadosCount(idUsuario):\n# def setEventoId(idEvento, idU):\n# def setPonenteId(idU, idP):\n# def getPonentesAsociadosCount(idU):\n# def getPonentes(idU):\n# def setOrganizacion(idO, idU):\n# def getOrganizacion(idU):\n# def setImage(img, idU):\n# def modificarUsuario(nombre, apellidos, telefono, twitter, web, ciudad, idU):\n# def deletePonente(idP, idU):\n# \n# Path: _oo/model/controladorPonente.py\n# def getKey(ponente):\n# def setPonente(nombre, apellidos, email, telefono, twitter, web, descripcion):\n# def updatePonente(nombre, apellidos, email, telefono, twitter, web, descripcion, idP):\n# def getPonenteById(idPonente):\n# def listarPonentes(self):\n# \n# Path: _oo/model/controladorOrganizacion.py\n# def getKeyOrg(organizacion):\n# def setOrganizacion(nombre, email, telefono, twitter, web):\n# def setUsuarioOrganizacion(idO, idU):\n# def getOrganizacion(idO):\n# def setEventoId(idE, idO):\n# def createInvitacion(idO, ema):\n# def deleteInvitacion(idO, ema):\n# def setImage(img, idO):\n# def updateOrganizacion(nombre, telefono, twitter, web, idO):\n# \n# Path: _oo/model/moduloEmail.py\n# def makeCuerpo(contain,extra_css):\n# def enviarEmail(to,subject,contain,extra_css=\"\"):\n# def enviarConfirmacionLogin(usuario):\n\n", "import_statement": "import os\nimport urllib\nimport jinja2\nimport webapp2\nimport hashlib\nimport json\nimport logging\nfrom google.appengine.ext import ndb\nfrom _oo.classes.evento import Evento\nfrom _oo.model import controladorEvento\nfrom _oo.model import controladorUsuario\nfrom _oo.model import controladorPonente\nfrom _oo.model import controladorOrganizacion\nfrom _oo.model import moduloEmail\nfrom datetime import datetime", "code": "      'numeroPonetes': False\n    }\n    #Comprobamos si el usuario está logueado o es False\n    if user:\n        info['numeroEventos'] = controladorUsuario.getEventosAsociadosCount(controladorUsuario.getKey(user))\n        info['numeroPonentes'] = controladorUsuario.getPonentesAsociadosCount(controladorUsuario.getKey(user))\n        info['userLogin'] = True\n        info['gravatar'] = user.nombre[0:1] + user.apellidos[0:1]\n\n    return info\n\n\n\nclass Index(webapp2.RequestHandler):\n    \"\"\"Es llamada por /\"\"\"\n    def get(self):\n        \"\"\"\n        Devuelve el index en función del logueo del usuario.\n        Si está logueado devuelve la plantilla con todos sus eventos\n        Si no está logueado devuelve un landpage de presentación\n        \"\"\"\n        info = getInfo(self)\n        usuario = controladorUsuario.getUsuarioLogeado(self)\n        organizacion = False\n        organizacionEventos = []\n\n        if usuario:\n            eventos = []\n            evs = controladorUsuario.getEventosAsociados(usuario.key.id())\n            for ev in evs:\n", "prompt": "# Path: _oo/classes/evento.py\n# class Evento(ndb.Model):\n#     nombre = ndb.StringProperty()\n#     tipo = ndb.StringProperty()\n#     privado = ndb.BooleanProperty()\n#     idCreador = ndb.StringProperty()\n#     hora = ndb.TimeProperty()\n#     fecha = ndb.DateProperty()\n#     fechaInsercion = ndb.DateTimeProperty(auto_now_add=True)\n#     lugar = ndb.StringProperty()\n#     coordenadas = ndb.GeoPtProperty(indexed=False)\n#     descripcion = ndb.TextProperty(indexed=False)\n#     asistencia = ndb.BooleanProperty(indexed=False)\n#     asistentes = ndb.StructuredProperty(Asistente, repeated=True)\n#     ponentes = ndb.StringProperty(repeated=True)\n# \n#     def getKey(self):\n#         return str(self.key.id())\n# \n# Path: _oo/model/controladorEvento.py\n# def GetEventoById(idEvento):\n# def SetEvento(nombre, tipo, privado, idCreador, hora, fecha, lugar, lat, lon, descripcion, asistencia):\n# def updateEvento(idE, nombre, tipo, privado, hora, fecha, lugar, lat, lon, descripcion, asistencia):\n# def setAsistente(idEvento, nom, ape, ema, tel, twi, dn):\n# def getUltimosEventos(num):\n# def getAsistentesEvento(idEvento):\n# def deleteEvento(idEvento):\n# def setPonente(idP, idE):\n# def deleteAsistente(idE, dni):\n# def confirmarAsistente(idE, dni):\n# \n# Path: _oo/model/controladorUsuario.py\n# def getKey(usuario):\n# def nuevoRegistroUsuario(nombre, apellidos, email, telefono, twitter, web, password, ciudad):\n# def loginCorrecto(email,password):\n# def getUsuarioById(idUsuario):\n# def getUsuarioLogeado(handler):\n# def listarUsuarios(self):\n# def getEventosAsociados(idUsuario):\n# def getEventosAsociadosCount(idUsuario):\n# def setEventoId(idEvento, idU):\n# def setPonenteId(idU, idP):\n# def getPonentesAsociadosCount(idU):\n# def getPonentes(idU):\n# def setOrganizacion(idO, idU):\n# def getOrganizacion(idU):\n# def setImage(img, idU):\n# def modificarUsuario(nombre, apellidos, telefono, twitter, web, ciudad, idU):\n# def deletePonente(idP, idU):\n# \n# Path: _oo/model/controladorPonente.py\n# def getKey(ponente):\n# def setPonente(nombre, apellidos, email, telefono, twitter, web, descripcion):\n# def updatePonente(nombre, apellidos, email, telefono, twitter, web, descripcion, idP):\n# def getPonenteById(idPonente):\n# def listarPonentes(self):\n# \n# Path: _oo/model/controladorOrganizacion.py\n# def getKeyOrg(organizacion):\n# def setOrganizacion(nombre, email, telefono, twitter, web):\n# def setUsuarioOrganizacion(idO, idU):\n# def getOrganizacion(idO):\n# def setEventoId(idE, idO):\n# def createInvitacion(idO, ema):\n# def deleteInvitacion(idO, ema):\n# def setImage(img, idO):\n# def updateOrganizacion(nombre, telefono, twitter, web, idO):\n# \n# Path: _oo/model/moduloEmail.py\n# def makeCuerpo(contain,extra_css):\n# def enviarEmail(to,subject,contain,extra_css=\"\"):\n# def enviarConfirmacionLogin(usuario):\n\n\n# Path: index.py\nimport os\nimport urllib\nimport jinja2\nimport webapp2\nimport hashlib\nimport json\nimport logging\nfrom google.appengine.ext import ndb\nfrom _oo.classes.evento import Evento\nfrom _oo.model import controladorEvento\nfrom _oo.model import controladorUsuario\nfrom _oo.model import controladorPonente\nfrom _oo.model import controladorOrganizacion\nfrom _oo.model import moduloEmail\nfrom datetime import datetime\n\n      'numeroPonetes': False\n    }\n    #Comprobamos si el usuario está logueado o es False\n    if user:\n        info['numeroEventos'] = controladorUsuario.getEventosAsociadosCount(controladorUsuario.getKey(user))\n        info['numeroPonentes'] = controladorUsuario.getPonentesAsociadosCount(controladorUsuario.getKey(user))\n        info['userLogin'] = True\n        info['gravatar'] = user.nombre[0:1] + user.apellidos[0:1]\n\n    return info\n\n\n\nclass Index(webapp2.RequestHandler):\n    \"\"\"Es llamada por /\"\"\"\n    def get(self):\n        \"\"\"\n        Devuelve el index en función del logueo del usuario.\n        Si está logueado devuelve la plantilla con todos sus eventos\n        Si no está logueado devuelve un landpage de presentación\n        \"\"\"\n        info = getInfo(self)\n        usuario = controladorUsuario.getUsuarioLogeado(self)\n        organizacion = False\n        organizacionEventos = []\n\n        if usuario:\n            eventos = []\n            evs = controladorUsuario.getEventosAsociados(usuario.key.id())\n            for ev in evs:\n", "next_line": "                eventos.append(controladorEvento.GetEventoById(ev))", "id": 0, "__internal_uuid__": "221fb193-4d07-404e-83d9-54118397548e"}
{"repo_name": "laowantong/mocodo", "file_path": "mocodo/mcd_to_svg.py", "context": "# Path: mocodo/common.py\n# def safe_print_for_PHP(s):\n#     def __init__(self, params):\n#     def output_success_message(self, path):\n#     def timestamp(self):\n#     def load_input_file(self):\n#     def load_style(self):\n#         def load_by_name(name):\n#         def may_apply_scaling(shapes):\n#         def ensure_margin_sizes_are_integer(shapes):\n#     def dump_output_file(self, result):\n#     def dump_mld_files(self, relations):\n#     def process_geometry(self, mcd, style):\n#         def dump_geo_file(d):\n# class Common:\n# \n# Path: mocodo/file_helpers.py\n# def read_contents(filename, encoding=\"utf8\"):\n#     with codecs.open(filename, encoding=encoding) as f:\n#         return f.read()\n# \n# Path: mocodo/dynamic.py\n# class Dynamic(str):\n#     \"\"\"Wrapper for the strings that need to be dynamically interpreted by the generated Python files.\"\"\"\n#     pass\n\n", "import_statement": "from .common import version\nfrom .file_helpers import read_contents\nfrom .dynamic import Dynamic\n    from .argument_parser import parsed_arguments\n    from .mcd import Mcd\n    from .common import Common\nimport string\nimport random\nimport re\nimport os", "code": "#!/usr/bin/env python\n# encoding: utf-8\n\nfrom __future__ import division\n\n\ndef main(mcd, common):\n    params = common.params\n    style = common.load_style()\n    for (k, v) in style.items():\n        if k.endswith(\"_color\") and v is None:\n            style[k] = \"none\"\n    mcd.calculate_size(style)\n    result = []\n    result.append(\"#!/usr/bin/env python\")\n    result.append(\"# encoding: utf-8\")\n    result.append(\"# %s\\n\" % common.timestamp())\n    result.append(\"from __future__ import division\\nfrom math import hypot\\n\")\n    result.append(\"import time, codecs\\n\")\n    result.extend(common.process_geometry(mcd, style))\n    for name in [\"card_max_width\", \"card_max_height\", \"card_margin\", \"arrow_width\", \"arrow_half_height\", \"arrow_axis\", \"card_baseline\"]:\n        result.append(\"%s = %s\" % (name, style[name]))\n    result.append(read_contents(os.path.join(params[\"script_directory\"], \"drawing_helpers.py\")))\n    result.append(read_contents(os.path.join(params[\"script_directory\"], \"drawing_helpers_svg.py\")))\n    result.append(\"\"\"\\nlines = '<?xml version=\"1.0\" standalone=\"no\"?>\\\\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\\\\n\"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">'\"\"\")\n    result.append(\"\"\"lines += '\\\\n\\\\n<svg width=\"%s\" height=\"%s\" view_box=\"0 0 %s %s\"\\\\nxmlns=\"http://www.w3.org/2000/svg\"\\\\nxmlns:link=\"http://www.w3.org/1999/xlink\">' % (width,height,width,height)\"\"\")\n", "prompt": "# Path: mocodo/common.py\n# def safe_print_for_PHP(s):\n#     def __init__(self, params):\n#     def output_success_message(self, path):\n#     def timestamp(self):\n#     def load_input_file(self):\n#     def load_style(self):\n#         def load_by_name(name):\n#         def may_apply_scaling(shapes):\n#         def ensure_margin_sizes_are_integer(shapes):\n#     def dump_output_file(self, result):\n#     def dump_mld_files(self, relations):\n#     def process_geometry(self, mcd, style):\n#         def dump_geo_file(d):\n# class Common:\n# \n# Path: mocodo/file_helpers.py\n# def read_contents(filename, encoding=\"utf8\"):\n#     with codecs.open(filename, encoding=encoding) as f:\n#         return f.read()\n# \n# Path: mocodo/dynamic.py\n# class Dynamic(str):\n#     \"\"\"Wrapper for the strings that need to be dynamically interpreted by the generated Python files.\"\"\"\n#     pass\n\n\n# Path: mocodo/mcd_to_svg.py\nfrom .common import version\nfrom .file_helpers import read_contents\nfrom .dynamic import Dynamic\n    from .argument_parser import parsed_arguments\n    from .mcd import Mcd\n    from .common import Common\nimport string\nimport random\nimport re\nimport os\n\n#!/usr/bin/env python\n# encoding: utf-8\n\nfrom __future__ import division\n\n\ndef main(mcd, common):\n    params = common.params\n    style = common.load_style()\n    for (k, v) in style.items():\n        if k.endswith(\"_color\") and v is None:\n            style[k] = \"none\"\n    mcd.calculate_size(style)\n    result = []\n    result.append(\"#!/usr/bin/env python\")\n    result.append(\"# encoding: utf-8\")\n    result.append(\"# %s\\n\" % common.timestamp())\n    result.append(\"from __future__ import division\\nfrom math import hypot\\n\")\n    result.append(\"import time, codecs\\n\")\n    result.extend(common.process_geometry(mcd, style))\n    for name in [\"card_max_width\", \"card_max_height\", \"card_margin\", \"arrow_width\", \"arrow_half_height\", \"arrow_axis\", \"card_baseline\"]:\n        result.append(\"%s = %s\" % (name, style[name]))\n    result.append(read_contents(os.path.join(params[\"script_directory\"], \"drawing_helpers.py\")))\n    result.append(read_contents(os.path.join(params[\"script_directory\"], \"drawing_helpers_svg.py\")))\n    result.append(\"\"\"\\nlines = '<?xml version=\"1.0\" standalone=\"no\"?>\\\\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\\\\n\"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">'\"\"\")\n    result.append(\"\"\"lines += '\\\\n\\\\n<svg width=\"%s\" height=\"%s\" view_box=\"0 0 %s %s\"\\\\nxmlns=\"http://www.w3.org/2000/svg\"\\\\nxmlns:link=\"http://www.w3.org/1999/xlink\">' % (width,height,width,height)\"\"\")\n", "next_line": "    result.append(_(\"\"\"lines += u'\\\\n\\\\n<desc>Generated by Mocodo {version} on {date}</desc>'\"\"\").format(version=version, date=\"%s\") + \"\"\" % time.strftime(\"%a, %d %b %Y %H:%M:%S\", time.localtime())\"\"\")", "id": 1, "__internal_uuid__": "2d78d1a6-990b-4c1a-a6ee-8259dda4e69f"}
{"repo_name": "mathiasertl/django-xmpp-account", "file_path": "core/middleware.py", "context": "# Path: core/exceptions.py\n# class RateException(Exception):\n#     \"\"\"Raised when the user views/posts too often.\"\"\"\n#     pass\n# \n# Path: core/exceptions.py\n# class RegistrationRateException(RateException):\n#     \"\"\"Raised when the user exceeds rate for registrations.\"\"\"\n# \n# Path: core/exceptions.py\n# class SpamException(Exception):\n#     \"\"\"Raised whenever we detect behaviour that suggests a spam-bot.\"\"\"\n#     pass\n# \n# Path: core/exceptions.py\n# class TemporaryError(Exception):\n#     \"\"\"Raised when a temporary error occurs.\"\"\"\n#     pass\n\n", "import_statement": "import logging\nfrom django.conf import settings\nfrom django.core.cache import cache\nfrom django.shortcuts import render\nfrom django.utils import six\nfrom django.utils.six.moves.urllib.parse import urlsplit\nfrom core.exceptions import RateException\nfrom core.exceptions import RegistrationRateException\nfrom core.exceptions import SpamException\nfrom core.exceptions import TemporaryError", "code": "class AntiSpamMiddleware(object):\n    def get_context(self, request, message):\n        host = request.META['REMOTE_ADDR']\n\n        context = {\n            'EXCEPTION': message,\n            'HOST': host,\n            'REGISTER_URL': request.build_absolute_uri('/'),\n        }\n        if 'CANONICAL_HOST' in request.site:\n            context['REGISTER_URL'] = urlsplit(context['REGISTER_URL'])._replace(\n                netloc=request.site['CANONICAL_HOST']).geturl()\n\n        return context\n\n    def process_request(self, request):\n        message = cache.get('spamblock-%s' % request.META['REMOTE_ADDR'])  # Added by previous SpamException\n        if message:\n            context = self.get_context(request, message)\n            return render(request, 'core/spambot.html', context)\n\n    def process_exception(self, request, exception):\n        if six.PY3:\n            message = ' '.join(exception.args)\n        else:\n            #TODO: This still is sometimes True!\n            message = exception.message or 'UNKNOWN REASON'\n\n        context = self.get_context(request, message)\n\n", "prompt": "# Path: core/exceptions.py\n# class RateException(Exception):\n#     \"\"\"Raised when the user views/posts too often.\"\"\"\n#     pass\n# \n# Path: core/exceptions.py\n# class RegistrationRateException(RateException):\n#     \"\"\"Raised when the user exceeds rate for registrations.\"\"\"\n# \n# Path: core/exceptions.py\n# class SpamException(Exception):\n#     \"\"\"Raised whenever we detect behaviour that suggests a spam-bot.\"\"\"\n#     pass\n# \n# Path: core/exceptions.py\n# class TemporaryError(Exception):\n#     \"\"\"Raised when a temporary error occurs.\"\"\"\n#     pass\n\n\n# Path: core/middleware.py\nimport logging\nfrom django.conf import settings\nfrom django.core.cache import cache\nfrom django.shortcuts import render\nfrom django.utils import six\nfrom django.utils.six.moves.urllib.parse import urlsplit\nfrom core.exceptions import RateException\nfrom core.exceptions import RegistrationRateException\nfrom core.exceptions import SpamException\nfrom core.exceptions import TemporaryError\n\nclass AntiSpamMiddleware(object):\n    def get_context(self, request, message):\n        host = request.META['REMOTE_ADDR']\n\n        context = {\n            'EXCEPTION': message,\n            'HOST': host,\n            'REGISTER_URL': request.build_absolute_uri('/'),\n        }\n        if 'CANONICAL_HOST' in request.site:\n            context['REGISTER_URL'] = urlsplit(context['REGISTER_URL'])._replace(\n                netloc=request.site['CANONICAL_HOST']).geturl()\n\n        return context\n\n    def process_request(self, request):\n        message = cache.get('spamblock-%s' % request.META['REMOTE_ADDR'])  # Added by previous SpamException\n        if message:\n            context = self.get_context(request, message)\n            return render(request, 'core/spambot.html', context)\n\n    def process_exception(self, request, exception):\n        if six.PY3:\n            message = ' '.join(exception.args)\n        else:\n            #TODO: This still is sometimes True!\n            message = exception.message or 'UNKNOWN REASON'\n\n        context = self.get_context(request, message)\n\n", "next_line": "        if isinstance(exception, SpamException):", "id": 2, "__internal_uuid__": "3e4a3e36-6515-4a69-ac75-0f446f5ec257"}
{"repo_name": "stephane-caron/pymanoid", "file_path": "pymanoid/swing_foot.py", "context": "# Path: pymanoid/gui.py\n# def draw_trajectory(points, color='b', linewidth=3, pointsize=0.01):\n#     \"\"\"\n#     Draw a trajectory as a set of points connected by line segments.\n# \n#     Parameters\n#     ----------\n#     points = array, shape=(N, 3)\n#         List of points or 2D array.\n#     color : char or triplet, optional\n#         Color letter or RGB values, default is 'g' for green.\n#     linewidth : scalar\n#         Thickness of drawn line.\n#     pointsize : scalar\n#         Vertex size.\n# \n#     Returns\n#     -------\n#     handles : list of openravepy.GraphHandle\n#         OpenRAVE graphical handles. Must be stored in some variable, otherwise\n#         the drawn object will vanish instantly.\n#     \"\"\"\n#     handles = []\n#     prev = points[0]\n#     for (i, pt) in enumerate(points):\n#         if pointsize > 5e-4:\n#             handles.append(draw_point(pt, color=color, pointsize=pointsize))\n#         handles.append(draw_line(prev, pt, color=color, linewidth=linewidth))\n#         prev = pt\n#     return handles\n# \n# Path: pymanoid/interp.py\n# def interpolate_cubic_hermite(p0, v0, p1, v1):\n#     \"\"\"\n#     Compute the third-order polynomial of the Hermite curve connecting\n#     :math:`(p_0, v_0)` to :math:`(p_1, v_1)`.\n# \n#     Parameters\n#     ----------\n#     p0 : (3,) array\n#         Start point.\n#     v0 : (3,) array\n#         Start velocity.\n#     p1 : (3,) array\n#         End point.\n#     v1 : (3,) array\n#         End velocity.\n# \n#     Returns\n#     -------\n#     P : NDPolynomial\n#         Polynomial function of the Hermite curve.\n# \n#     \"\"\"\n#     c3 = 2 * p0 - 2 * p1 + v0 + v1\n#     c2 = -3 * p0 + 3 * p1 - 2 * v0 - v1\n#     c1 = v0\n#     c0 = p0\n#     return NDPolynomial([c0, c1, c2, c3])\n\n", "import_statement": "from numpy import array, dot, eye, hstack, linspace, zeros\nfrom openravepy import InterpolateQuatSlerp as quat_slerp\nfrom qpsolvers import solve_qp\nfrom .gui import draw_trajectory\nfrom .interp import interpolate_cubic_hermite", "code": "    def H_mu(s):\n        return s ** 2 * (s - 1) * n1\n\n    def H_cst(s):\n        return p0 + s ** 2 * (3 - 2 * s) * (p1 - p0)\n\n    return H_lambda, H_mu, H_cst\n\n\nclass SwingFoot(object):\n\n    \"\"\"\n    Polynomial swing foot interpolator.\n\n    Parameters\n    ----------\n    start_contact : pymanoid.Contact\n        Initial contact.\n    end_contact : pymanoid.Contact\n        Target contact.\n    duration : scalar\n        Swing duration in [s].\n    takeoff_clearance : scalar, optional\n        Takeoff clearance height at 1/4th of the interpolated trajectory.\n    landing_clearance : scalar, optional\n        Landing clearance height at 3/4th of the interpolated trajectory.\n    \"\"\"\n\n    def __init__(self, start_contact, end_contact, duration,\n                 takeoff_clearance=0.05, landing_clearance=0.05):\n", "prompt": "# Path: pymanoid/gui.py\n# def draw_trajectory(points, color='b', linewidth=3, pointsize=0.01):\n#     \"\"\"\n#     Draw a trajectory as a set of points connected by line segments.\n# \n#     Parameters\n#     ----------\n#     points = array, shape=(N, 3)\n#         List of points or 2D array.\n#     color : char or triplet, optional\n#         Color letter or RGB values, default is 'g' for green.\n#     linewidth : scalar\n#         Thickness of drawn line.\n#     pointsize : scalar\n#         Vertex size.\n# \n#     Returns\n#     -------\n#     handles : list of openravepy.GraphHandle\n#         OpenRAVE graphical handles. Must be stored in some variable, otherwise\n#         the drawn object will vanish instantly.\n#     \"\"\"\n#     handles = []\n#     prev = points[0]\n#     for (i, pt) in enumerate(points):\n#         if pointsize > 5e-4:\n#             handles.append(draw_point(pt, color=color, pointsize=pointsize))\n#         handles.append(draw_line(prev, pt, color=color, linewidth=linewidth))\n#         prev = pt\n#     return handles\n# \n# Path: pymanoid/interp.py\n# def interpolate_cubic_hermite(p0, v0, p1, v1):\n#     \"\"\"\n#     Compute the third-order polynomial of the Hermite curve connecting\n#     :math:`(p_0, v_0)` to :math:`(p_1, v_1)`.\n# \n#     Parameters\n#     ----------\n#     p0 : (3,) array\n#         Start point.\n#     v0 : (3,) array\n#         Start velocity.\n#     p1 : (3,) array\n#         End point.\n#     v1 : (3,) array\n#         End velocity.\n# \n#     Returns\n#     -------\n#     P : NDPolynomial\n#         Polynomial function of the Hermite curve.\n# \n#     \"\"\"\n#     c3 = 2 * p0 - 2 * p1 + v0 + v1\n#     c2 = -3 * p0 + 3 * p1 - 2 * v0 - v1\n#     c1 = v0\n#     c0 = p0\n#     return NDPolynomial([c0, c1, c2, c3])\n\n\n# Path: pymanoid/swing_foot.py\nfrom numpy import array, dot, eye, hstack, linspace, zeros\nfrom openravepy import InterpolateQuatSlerp as quat_slerp\nfrom qpsolvers import solve_qp\nfrom .gui import draw_trajectory\nfrom .interp import interpolate_cubic_hermite\n\n    def H_mu(s):\n        return s ** 2 * (s - 1) * n1\n\n    def H_cst(s):\n        return p0 + s ** 2 * (3 - 2 * s) * (p1 - p0)\n\n    return H_lambda, H_mu, H_cst\n\n\nclass SwingFoot(object):\n\n    \"\"\"\n    Polynomial swing foot interpolator.\n\n    Parameters\n    ----------\n    start_contact : pymanoid.Contact\n        Initial contact.\n    end_contact : pymanoid.Contact\n        Target contact.\n    duration : scalar\n        Swing duration in [s].\n    takeoff_clearance : scalar, optional\n        Takeoff clearance height at 1/4th of the interpolated trajectory.\n    landing_clearance : scalar, optional\n        Landing clearance height at 3/4th of the interpolated trajectory.\n    \"\"\"\n\n    def __init__(self, start_contact, end_contact, duration,\n                 takeoff_clearance=0.05, landing_clearance=0.05):\n", "next_line": "        self.draw_trajectory = False", "id": 3, "__internal_uuid__": "8e4da779-b4c4-4a9f-8db5-20adb29c3d9e"}
{"repo_name": "wintoncode/winton-kafka-streams", "file_path": "winton_kafka_streams/state/factory/base_storage_key_value_store_factory.py", "context": "# Path: winton_kafka_streams/processor/serialization/serde.py\n# class Serde(AsymmetricSerde[T, T]):\n#     \"\"\"\n#     Get Serializer\n# \n#     Returns:\n#     --------\n#     serializer : Serializer\n#     \"\"\"\n# \n#     @property\n#     @abc.abstractmethod\n#     def serializer(self) -> Serializer[T]:\n#         pass\n# \n#     \"\"\"\n#     Get Deserializer\n# \n#     Returns:\n#     --------\n#     deserializer : Deserializer\n#     \"\"\"\n# \n#     @property\n#     @abc.abstractmethod\n#     def deserializer(self) -> Deserializer[T]:\n#         pass\n# \n#     \"\"\"\n#     Configure this class, which will configure the underlying serializer and deserializer.\n# \n#     Parameters:\n#     -----------\n#     configs : dict\n#         configs in key/value pairs\n#     is_key : bool\n#         whether is for key or value\n#     \"\"\"\n# \n#     @abc.abstractmethod\n#     def configure(self, configs, is_key):\n#         pass\n# \n#     \"\"\"\n#     Close this serde class, which will close the underlying serializer and deserializer.\n#     This method has to be idempotent because it might be called multiple times.\n#     \"\"\"\n# \n#     @abc.abstractmethod\n#     def close(self):\n#         pass\n# \n# Path: winton_kafka_streams/state/state_store_supplier.py\n# class StateStoreSupplier(ABC, Generic[KT, VT]):\n#     \"\"\"\n#     StateStoreSuppliers are added to a topology and are accessible from each StreamThread\n# \n#     \"\"\"\n# \n#     def __init__(self, name: str, key_serde: Serde[KT], value_serde: Serde[VT], logging_enabled: bool) -> None:\n#         self.logging_enabled: bool = logging_enabled\n#         self._value_serde: Serde[VT] = value_serde\n#         self._key_serde: Serde[KT] = key_serde\n#         self._name: str = name\n# \n#     @property\n#     def name(self) -> str:\n#         return self._name\n# \n#     @abstractmethod\n#     def _build_state_store(self) -> StateStore[KT, VT]:\n#         pass\n# \n#     def get(self) -> StateStore[KT, VT]:\n#         \"\"\"Create a StateStore for each StreamTask. *These StateStores may exist in different threads.*\"\"\"\n#         inner = self._build_state_store()\n#         if self.logging_enabled:\n#             return ChangeLoggingStateStore(self.name, self._key_serde, self._value_serde, self.logging_enabled, inner)\n#         else:\n#             return inner\n\n", "import_statement": "from typing import Generic, TypeVar\nfrom winton_kafka_streams.processor.serialization import Serde\nfrom abc import ABC, abstractmethod\nfrom winton_kafka_streams.state.state_store_supplier import StateStoreSupplier", "code": "\n\n\nKT = TypeVar('KT')  # Key type.\nVT = TypeVar('VT')  # Value type.\n\n\nclass BaseStorageKeyValueStoreFactory(ABC, Generic[KT, VT]):\n", "prompt": "# Path: winton_kafka_streams/processor/serialization/serde.py\n# class Serde(AsymmetricSerde[T, T]):\n#     \"\"\"\n#     Get Serializer\n# \n#     Returns:\n#     --------\n#     serializer : Serializer\n#     \"\"\"\n# \n#     @property\n#     @abc.abstractmethod\n#     def serializer(self) -> Serializer[T]:\n#         pass\n# \n#     \"\"\"\n#     Get Deserializer\n# \n#     Returns:\n#     --------\n#     deserializer : Deserializer\n#     \"\"\"\n# \n#     @property\n#     @abc.abstractmethod\n#     def deserializer(self) -> Deserializer[T]:\n#         pass\n# \n#     \"\"\"\n#     Configure this class, which will configure the underlying serializer and deserializer.\n# \n#     Parameters:\n#     -----------\n#     configs : dict\n#         configs in key/value pairs\n#     is_key : bool\n#         whether is for key or value\n#     \"\"\"\n# \n#     @abc.abstractmethod\n#     def configure(self, configs, is_key):\n#         pass\n# \n#     \"\"\"\n#     Close this serde class, which will close the underlying serializer and deserializer.\n#     This method has to be idempotent because it might be called multiple times.\n#     \"\"\"\n# \n#     @abc.abstractmethod\n#     def close(self):\n#         pass\n# \n# Path: winton_kafka_streams/state/state_store_supplier.py\n# class StateStoreSupplier(ABC, Generic[KT, VT]):\n#     \"\"\"\n#     StateStoreSuppliers are added to a topology and are accessible from each StreamThread\n# \n#     \"\"\"\n# \n#     def __init__(self, name: str, key_serde: Serde[KT], value_serde: Serde[VT], logging_enabled: bool) -> None:\n#         self.logging_enabled: bool = logging_enabled\n#         self._value_serde: Serde[VT] = value_serde\n#         self._key_serde: Serde[KT] = key_serde\n#         self._name: str = name\n# \n#     @property\n#     def name(self) -> str:\n#         return self._name\n# \n#     @abstractmethod\n#     def _build_state_store(self) -> StateStore[KT, VT]:\n#         pass\n# \n#     def get(self) -> StateStore[KT, VT]:\n#         \"\"\"Create a StateStore for each StreamTask. *These StateStores may exist in different threads.*\"\"\"\n#         inner = self._build_state_store()\n#         if self.logging_enabled:\n#             return ChangeLoggingStateStore(self.name, self._key_serde, self._value_serde, self.logging_enabled, inner)\n#         else:\n#             return inner\n\n\n# Path: winton_kafka_streams/state/factory/base_storage_key_value_store_factory.py\nfrom typing import Generic, TypeVar\nfrom winton_kafka_streams.processor.serialization import Serde\nfrom abc import ABC, abstractmethod\nfrom winton_kafka_streams.state.state_store_supplier import StateStoreSupplier\n\n\n\n\nKT = TypeVar('KT')  # Key type.\nVT = TypeVar('VT')  # Value type.\n\n\nclass BaseStorageKeyValueStoreFactory(ABC, Generic[KT, VT]):\n", "next_line": "    def __init__(self, name: str, key_serde: Serde[KT], value_serde: Serde[VT]) -> None:", "id": 4, "__internal_uuid__": "d1d80dee-5d07-430c-a1a5-6aece1d82e69"}
{"repo_name": "wintoncode/winton-kafka-streams", "file_path": "winton_kafka_streams/processor/serialization/serdes/avro_serde.py", "context": "# Path: winton_kafka_streams/processor/serialization/_avro.py\n# class AvroSerializer(Serializer):\n#     def __init__(self):\n#         self._avro_helper = AvroHelper()\n# \n#     def serialize(self, topic, data):\n#         return self._avro_helper.serialize(topic, data)\n# \n#     def configure(self, configs, is_key):\n#         self._avro_helper.configure(configs, is_key)\n# \n#     def close(self):\n#         pass\n# \n# class AvroDeserializer(Deserializer):\n#     def __init__(self):\n#         self._avro_helper = AvroHelper()\n# \n#     def deserialize(self, topic, data):\n#         return self._avro_helper.deserialize(topic, data)\n# \n#     def configure(self, configs, is_key):\n#         self._avro_helper.configure(configs, is_key)\n# \n#     def close(self):\n#         pass\n# \n# Path: winton_kafka_streams/processor/serialization/serdes/wrapper_serde.py\n# class WrapperSerde(Serde[T]):\n#     def __init__(self, serializer: Serializer[T], deserializer: Deserializer[T]) -> None:\n#         self._serializer = serializer\n#         self._deserializer = deserializer\n# \n#     @property\n#     def serializer(self) -> Serializer[T]:\n#         return self._serializer\n# \n#     @property\n#     def deserializer(self) -> Deserializer[T]:\n#         return self._deserializer\n# \n#     def configure(self, configs, is_key) -> None:\n#         self.serializer.configure(configs, is_key)\n#         self.deserializer.configure(configs, is_key)\n# \n#     def close(self) -> None:\n#         self.serializer.close()\n#         self.deserializer.close()\n\n", "import_statement": "from .._avro import AvroSerializer, AvroDeserializer\nfrom .wrapper_serde import WrapperSerde", "code": "\"\"\"\nAvro Serde\n\n\"\"\"\n\n\nclass AvroSerde(WrapperSerde):\n    \"\"\"\n    Avro Serde that will use Avro and a schema registry\n    for serialization and deserialization\n    \"\"\"\n\n    def __init__(self):\n        serializer = AvroSerializer()\n", "prompt": "# Path: winton_kafka_streams/processor/serialization/_avro.py\n# class AvroSerializer(Serializer):\n#     def __init__(self):\n#         self._avro_helper = AvroHelper()\n# \n#     def serialize(self, topic, data):\n#         return self._avro_helper.serialize(topic, data)\n# \n#     def configure(self, configs, is_key):\n#         self._avro_helper.configure(configs, is_key)\n# \n#     def close(self):\n#         pass\n# \n# class AvroDeserializer(Deserializer):\n#     def __init__(self):\n#         self._avro_helper = AvroHelper()\n# \n#     def deserialize(self, topic, data):\n#         return self._avro_helper.deserialize(topic, data)\n# \n#     def configure(self, configs, is_key):\n#         self._avro_helper.configure(configs, is_key)\n# \n#     def close(self):\n#         pass\n# \n# Path: winton_kafka_streams/processor/serialization/serdes/wrapper_serde.py\n# class WrapperSerde(Serde[T]):\n#     def __init__(self, serializer: Serializer[T], deserializer: Deserializer[T]) -> None:\n#         self._serializer = serializer\n#         self._deserializer = deserializer\n# \n#     @property\n#     def serializer(self) -> Serializer[T]:\n#         return self._serializer\n# \n#     @property\n#     def deserializer(self) -> Deserializer[T]:\n#         return self._deserializer\n# \n#     def configure(self, configs, is_key) -> None:\n#         self.serializer.configure(configs, is_key)\n#         self.deserializer.configure(configs, is_key)\n# \n#     def close(self) -> None:\n#         self.serializer.close()\n#         self.deserializer.close()\n\n\n# Path: winton_kafka_streams/processor/serialization/serdes/avro_serde.py\nfrom .._avro import AvroSerializer, AvroDeserializer\nfrom .wrapper_serde import WrapperSerde\n\n\"\"\"\nAvro Serde\n\n\"\"\"\n\n\nclass AvroSerde(WrapperSerde):\n    \"\"\"\n    Avro Serde that will use Avro and a schema registry\n    for serialization and deserialization\n    \"\"\"\n\n    def __init__(self):\n        serializer = AvroSerializer()\n", "next_line": "        deserializer = AvroDeserializer()", "id": 5, "__internal_uuid__": "c28ec62d-77ee-4137-93fe-b5093d9eae24"}
{"repo_name": "photo/openphoto-python", "file_path": "trovebox/http.py", "context": "# Path: trovebox/objects/trovebox_object.py\n# class TroveboxObject(object):\n#     \"\"\" Base object supporting the storage of custom fields as attributes \"\"\"\n#     _type = \"None\"\n#     def __init__(self, client, json_dict):\n#         self.id = None\n#         self.name = None\n#         self._client = client\n#         self._json_dict = json_dict\n#         self._set_fields(json_dict)\n# \n#     def _set_fields(self, json_dict):\n#         \"\"\" Set this object's attributes specified in json_dict \"\"\"\n#         for key, value in json_dict.items():\n#             if not key.startswith(\"_\"):\n#                 setattr(self, key, value)\n# \n#     def _replace_fields(self, json_dict):\n#         \"\"\"\n#         Delete this object's attributes, and replace with\n#         those in json_dict.\n#         \"\"\"\n#         for key in self._json_dict.keys():\n#             if not key.startswith(\"_\"):\n#                 delattr(self, key)\n#         self._json_dict = json_dict\n#         self._set_fields(json_dict)\n# \n#     def _delete_fields(self):\n#         \"\"\"\n#         Delete this object's attributes, including name and id\n#         \"\"\"\n#         for key in self._json_dict.keys():\n#             if not key.startswith(\"_\"):\n#                 delattr(self, key)\n#         self._json_dict = {}\n#         self.id = None\n#         self.name = None\n# \n#     def __repr__(self):\n#         if self.name is not None:\n#             value = \"<%s name='%s'>\" % (self.__class__.__name__, self.name)\n#         elif self.id is not None:\n#             value = \"<%s id='%s'>\" % (self.__class__.__name__, self.id)\n#         else:\n#             value = \"<%s>\" % (self.__class__.__name__)\n# \n#         # Python2 requires a bytestring\n#         if sys.version < '3':\n#             return value.encode('utf-8')\n#         else: # pragma: no cover\n#             return value\n# \n#     def get_fields(self):\n#         \"\"\" Returns this object's attributes \"\"\"\n#         return self._json_dict\n# \n#     def get_type(self):\n#         \"\"\" Return this object's type (eg. \"photo\") \"\"\"\n#         return self._type\n# \n# Path: trovebox/errors.py\n# class TroveboxError(Exception):\n#     \"\"\" Indicates that a Trovebox operation failed \"\"\"\n#     pass\n# \n# class Trovebox404Error(Exception):\n#     \"\"\"\n#     Indicates that an Http 404 error code was received\n#     (resource not found)\n#     \"\"\"\n#     pass\n# \n# class TroveboxDuplicateError(TroveboxError):\n#     \"\"\" Indicates that an upload operation failed due to a duplicate photo \"\"\"\n#     pass\n# \n# Path: trovebox/auth.py\n# class Auth(object):\n#     \"\"\"OAuth secrets\"\"\"\n#     def __init__(self, config_file, host,\n#                  consumer_key, consumer_secret,\n#                  token, token_secret):\n#         if host is None:\n#             self.config_path = get_config_path(config_file)\n#             config = read_config(self.config_path)\n#             self.host = config['host']\n#             self.consumer_key = config['consumerKey']\n#             self.consumer_secret = config['consumerSecret']\n#             self.token = config['token']\n#             self.token_secret = config['tokenSecret']\n#         else:\n#             self.config_path = None\n#             self.host = host\n#             self.consumer_key = consumer_key\n#             self.consumer_secret = consumer_secret\n#             self.token = token\n#             self.token_secret = token_secret\n# \n#         if host is not None and config_file is not None:\n#             raise ValueError(\"Cannot specify both host and config_file\")\n\n", "import_statement": "import sys\nimport requests\nimport requests_oauthlib\nimport logging\n    from urllib.parse import urlparse, urlunparse # Python3\n    from urlparse import urlparse, urlunparse # Python2\nfrom trovebox.objects.trovebox_object import TroveboxObject\nfrom .errors import TroveboxError, Trovebox404Error, TroveboxDuplicateError\nfrom .auth import Auth", "code": "        # Unknown - just do our best\n        else:\n            return str(value).encode(\"utf-8\")\n\n    @staticmethod\n    def _process_response(response):\n        \"\"\"\n        Decodes the JSON response, returning a dict.\n        Raises an exception if an invalid response code is received.\n        \"\"\"\n        if response.status_code == 404:\n            raise Trovebox404Error(\"HTTP Error %d: %s\" %\n                                   (response.status_code, response.reason))\n        try:\n            json_response = response.json()\n            code = json_response[\"code\"]\n            message = json_response[\"message\"]\n        except (ValueError, KeyError):\n            # Response wasn't Trovebox JSON - check the HTTP status code\n            if 200 <= response.status_code < 300:\n                # Status code was valid, so just reraise the exception\n                raise\n            else:\n                raise TroveboxError(\"HTTP Error %d: %s\" %\n                                    (response.status_code, response.reason))\n\n        if 200 <= code < 300:\n            return json_response\n        elif (code == DUPLICATE_RESPONSE[\"code\"] and\n               DUPLICATE_RESPONSE[\"message\"] in message):\n", "prompt": "# Path: trovebox/objects/trovebox_object.py\n# class TroveboxObject(object):\n#     \"\"\" Base object supporting the storage of custom fields as attributes \"\"\"\n#     _type = \"None\"\n#     def __init__(self, client, json_dict):\n#         self.id = None\n#         self.name = None\n#         self._client = client\n#         self._json_dict = json_dict\n#         self._set_fields(json_dict)\n# \n#     def _set_fields(self, json_dict):\n#         \"\"\" Set this object's attributes specified in json_dict \"\"\"\n#         for key, value in json_dict.items():\n#             if not key.startswith(\"_\"):\n#                 setattr(self, key, value)\n# \n#     def _replace_fields(self, json_dict):\n#         \"\"\"\n#         Delete this object's attributes, and replace with\n#         those in json_dict.\n#         \"\"\"\n#         for key in self._json_dict.keys():\n#             if not key.startswith(\"_\"):\n#                 delattr(self, key)\n#         self._json_dict = json_dict\n#         self._set_fields(json_dict)\n# \n#     def _delete_fields(self):\n#         \"\"\"\n#         Delete this object's attributes, including name and id\n#         \"\"\"\n#         for key in self._json_dict.keys():\n#             if not key.startswith(\"_\"):\n#                 delattr(self, key)\n#         self._json_dict = {}\n#         self.id = None\n#         self.name = None\n# \n#     def __repr__(self):\n#         if self.name is not None:\n#             value = \"<%s name='%s'>\" % (self.__class__.__name__, self.name)\n#         elif self.id is not None:\n#             value = \"<%s id='%s'>\" % (self.__class__.__name__, self.id)\n#         else:\n#             value = \"<%s>\" % (self.__class__.__name__)\n# \n#         # Python2 requires a bytestring\n#         if sys.version < '3':\n#             return value.encode('utf-8')\n#         else: # pragma: no cover\n#             return value\n# \n#     def get_fields(self):\n#         \"\"\" Returns this object's attributes \"\"\"\n#         return self._json_dict\n# \n#     def get_type(self):\n#         \"\"\" Return this object's type (eg. \"photo\") \"\"\"\n#         return self._type\n# \n# Path: trovebox/errors.py\n# class TroveboxError(Exception):\n#     \"\"\" Indicates that a Trovebox operation failed \"\"\"\n#     pass\n# \n# class Trovebox404Error(Exception):\n#     \"\"\"\n#     Indicates that an Http 404 error code was received\n#     (resource not found)\n#     \"\"\"\n#     pass\n# \n# class TroveboxDuplicateError(TroveboxError):\n#     \"\"\" Indicates that an upload operation failed due to a duplicate photo \"\"\"\n#     pass\n# \n# Path: trovebox/auth.py\n# class Auth(object):\n#     \"\"\"OAuth secrets\"\"\"\n#     def __init__(self, config_file, host,\n#                  consumer_key, consumer_secret,\n#                  token, token_secret):\n#         if host is None:\n#             self.config_path = get_config_path(config_file)\n#             config = read_config(self.config_path)\n#             self.host = config['host']\n#             self.consumer_key = config['consumerKey']\n#             self.consumer_secret = config['consumerSecret']\n#             self.token = config['token']\n#             self.token_secret = config['tokenSecret']\n#         else:\n#             self.config_path = None\n#             self.host = host\n#             self.consumer_key = consumer_key\n#             self.consumer_secret = consumer_secret\n#             self.token = token\n#             self.token_secret = token_secret\n# \n#         if host is not None and config_file is not None:\n#             raise ValueError(\"Cannot specify both host and config_file\")\n\n\n# Path: trovebox/http.py\nimport sys\nimport requests\nimport requests_oauthlib\nimport logging\n    from urllib.parse import urlparse, urlunparse # Python3\n    from urlparse import urlparse, urlunparse # Python2\nfrom trovebox.objects.trovebox_object import TroveboxObject\nfrom .errors import TroveboxError, Trovebox404Error, TroveboxDuplicateError\nfrom .auth import Auth\n\n        # Unknown - just do our best\n        else:\n            return str(value).encode(\"utf-8\")\n\n    @staticmethod\n    def _process_response(response):\n        \"\"\"\n        Decodes the JSON response, returning a dict.\n        Raises an exception if an invalid response code is received.\n        \"\"\"\n        if response.status_code == 404:\n            raise Trovebox404Error(\"HTTP Error %d: %s\" %\n                                   (response.status_code, response.reason))\n        try:\n            json_response = response.json()\n            code = json_response[\"code\"]\n            message = json_response[\"message\"]\n        except (ValueError, KeyError):\n            # Response wasn't Trovebox JSON - check the HTTP status code\n            if 200 <= response.status_code < 300:\n                # Status code was valid, so just reraise the exception\n                raise\n            else:\n                raise TroveboxError(\"HTTP Error %d: %s\" %\n                                    (response.status_code, response.reason))\n\n        if 200 <= code < 300:\n            return json_response\n        elif (code == DUPLICATE_RESPONSE[\"code\"] and\n               DUPLICATE_RESPONSE[\"message\"] in message):\n", "next_line": "            raise TroveboxDuplicateError(\"Code %d: %s\" % (code, message))", "id": 6, "__internal_uuid__": "f352132a-7f3f-4613-a153-49f7b14070ab"}
{"repo_name": "GIR-at-AAMU/gir_app_labs_at_aamu", "file_path": "pages/authors/asessom.py", "context": "# Path: pages/author_list.py\n# class Page(base.Page):\n#     URL_PATH = '/a'\n#     TEMPLATE_FILE = 'author_list.html'\n#     def user_path(cls, user_name):\n#     def add_author(cls, user_name, handler):\n#     def add_author_page(cls, page):\n#     def template_values(self):\n\n", "import_statement": "import webapp2\nfrom pages import author_list", "code": "# Copyright 2017 The GiR @ AAMU Authors. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\n\nclass AuthorPage(webapp2.RequestHandler):\n    \"\"\"The asessom author page of the GiR App Labs at AAMU app.\"\"\"\n\n    def get(self):\n        \"\"\"HTTP GET handler for the tlarsen Users page.\"\"\"\n\n        self.response.headers['Content-Type'] = 'text/plain'\n        self.response.write(\"Hey, it's Mari!\")\n\n\n", "prompt": "# Path: pages/author_list.py\n# class Page(base.Page):\n#     URL_PATH = '/a'\n#     TEMPLATE_FILE = 'author_list.html'\n#     def user_path(cls, user_name):\n#     def add_author(cls, user_name, handler):\n#     def add_author_page(cls, page):\n#     def template_values(self):\n\n\n# Path: pages/authors/asessom.py\nimport webapp2\nfrom pages import author_list\n\n# Copyright 2017 The GiR @ AAMU Authors. All rights reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n\n\n\n\nclass AuthorPage(webapp2.RequestHandler):\n    \"\"\"The asessom author page of the GiR App Labs at AAMU app.\"\"\"\n\n    def get(self):\n        \"\"\"HTTP GET handler for the tlarsen Users page.\"\"\"\n\n        self.response.headers['Content-Type'] = 'text/plain'\n        self.response.write(\"Hey, it's Mari!\")\n\n\n", "next_line": "author_list.Page.add_author('asessom', AuthorPage)", "id": 7, "__internal_uuid__": "11368328-68db-4956-916c-8487d3024e20"}
{"repo_name": "yt-project/unyt", "file_path": "unyt/unit_registry.py", "context": "# Path: unyt/dimensions.py\n# def accepts(**arg_units):\n#     def check_accepts(f):\n#         def new_f(*args, **kwargs):\n# def returns(r_unit):\n#     def check_returns(f):\n#         def new_f(*args, **kwargs):\n# def _has_dimensions(quant, dim):\n# \n# Path: unyt/exceptions.py\n# class SymbolNotFoundError(Exception):\n#     \"\"\"Raised when a unit name is not available in a unit registry\n# \n#     Example\n#     -------\n# \n#     >>> from unyt.unit_registry import default_unit_registry\n#     >>> default_unit_registry['made_up_unit']\\\n#   # doctest: +IGNORE_EXCEPTION_DETAIL +NORMALIZE_WHITESPACE\n#     Traceback (most recent call last):\n#     ...\n#     unyt.exceptions.SymbolNotFoundError: The symbol 'made_up_unit'\n#     does not exist in this registry.\n#     \"\"\"\n# \n#     pass\n# \n# class UnitParseError(Exception):\n#     \"\"\"Raised when a string unit name is not parseable as a valid unit\n# \n#     Example\n#     -------\n# \n#     >>> from unyt import Unit\n#     >>> Unit('hello')\\\n#   # doctest: +IGNORE_EXCEPTION_DETAIL +NORMALIZE_WHITESPACE\n#     Traceback (most recent call last):\n#     ...\n#     unyt.exceptions.UnitParseError: Could not find unit symbol\n#     'hello' in the provided symbols.\n#     \"\"\"\n# \n#     pass\n# \n# Path: unyt/_unit_lookup_table.py\n# def generate_name_alternatives():\n#     def append_name(n, okey, key):\n# \n# Path: unyt/unit_systems.py\n# def add_symbols(namespace, registry):\n# def add_constants(namespace, registry):\n# def _split_prefix(symbol_str, unit_symbol_lut):\n# def _get_system_unit_string(dims, base_units):\n#     def __init__(\n#         self,\n#         name,\n#         length_unit,\n#         mass_unit,\n#         time_unit,\n#         temperature_unit=\"K\",\n#         angle_unit=\"rad\",\n#         current_mks_unit=\"A\",\n#         luminous_intensity_unit=\"cd\",\n#         logarithmic_unit=\"Np\",\n#         registry=None,\n#     ):\n#     def __getitem__(self, key):\n#     def __setitem__(self, key, value):\n#     def __str__(self):\n#     def __repr__(self):\n#     def has_current_mks(self):\n# class UnitSystem(object):\n\n", "import_statement": "import json\nfrom functools import lru_cache\nfrom unyt import dimensions as unyt_dims\nfrom unyt.exceptions import SymbolNotFoundError, UnitParseError\nfrom unyt._unit_lookup_table import default_unit_symbol_lut, unit_prefixes\nfrom unyt.unit_systems import mks_unit_system, _split_prefix, unit_system_registry\nfrom hashlib import md5\nfrom sympy import sympify\n        from unyt.unit_object import _validate_dimensions", "code": "\"\"\"\nA registry for units that can be added to and modified.\n\n\n\"\"\"\n\n# -----------------------------------------------------------------------------\n# Copyright (c) 2018, yt Development Team.\n#\n# Distributed under the terms of the Modified BSD License.\n#\n# The full license is in the LICENSE file, distributed with this software.\n# -----------------------------------------------------------------------------\n\n\n\n\n\ndef _sanitize_unit_system(unit_system, obj):\n\n    if unit_system is None:\n        try:\n            unit_system = obj.units.registry.unit_system\n        except AttributeError:\n", "prompt": "# Path: unyt/dimensions.py\n# def accepts(**arg_units):\n#     def check_accepts(f):\n#         def new_f(*args, **kwargs):\n# def returns(r_unit):\n#     def check_returns(f):\n#         def new_f(*args, **kwargs):\n# def _has_dimensions(quant, dim):\n# \n# Path: unyt/exceptions.py\n# class SymbolNotFoundError(Exception):\n#     \"\"\"Raised when a unit name is not available in a unit registry\n# \n#     Example\n#     -------\n# \n#     >>> from unyt.unit_registry import default_unit_registry\n#     >>> default_unit_registry['made_up_unit']\\\n#   # doctest: +IGNORE_EXCEPTION_DETAIL +NORMALIZE_WHITESPACE\n#     Traceback (most recent call last):\n#     ...\n#     unyt.exceptions.SymbolNotFoundError: The symbol 'made_up_unit'\n#     does not exist in this registry.\n#     \"\"\"\n# \n#     pass\n# \n# class UnitParseError(Exception):\n#     \"\"\"Raised when a string unit name is not parseable as a valid unit\n# \n#     Example\n#     -------\n# \n#     >>> from unyt import Unit\n#     >>> Unit('hello')\\\n#   # doctest: +IGNORE_EXCEPTION_DETAIL +NORMALIZE_WHITESPACE\n#     Traceback (most recent call last):\n#     ...\n#     unyt.exceptions.UnitParseError: Could not find unit symbol\n#     'hello' in the provided symbols.\n#     \"\"\"\n# \n#     pass\n# \n# Path: unyt/_unit_lookup_table.py\n# def generate_name_alternatives():\n#     def append_name(n, okey, key):\n# \n# Path: unyt/unit_systems.py\n# def add_symbols(namespace, registry):\n# def add_constants(namespace, registry):\n# def _split_prefix(symbol_str, unit_symbol_lut):\n# def _get_system_unit_string(dims, base_units):\n#     def __init__(\n#         self,\n#         name,\n#         length_unit,\n#         mass_unit,\n#         time_unit,\n#         temperature_unit=\"K\",\n#         angle_unit=\"rad\",\n#         current_mks_unit=\"A\",\n#         luminous_intensity_unit=\"cd\",\n#         logarithmic_unit=\"Np\",\n#         registry=None,\n#     ):\n#     def __getitem__(self, key):\n#     def __setitem__(self, key, value):\n#     def __str__(self):\n#     def __repr__(self):\n#     def has_current_mks(self):\n# class UnitSystem(object):\n\n\n# Path: unyt/unit_registry.py\nimport json\nfrom functools import lru_cache\nfrom unyt import dimensions as unyt_dims\nfrom unyt.exceptions import SymbolNotFoundError, UnitParseError\nfrom unyt._unit_lookup_table import default_unit_symbol_lut, unit_prefixes\nfrom unyt.unit_systems import mks_unit_system, _split_prefix, unit_system_registry\nfrom hashlib import md5\nfrom sympy import sympify\n        from unyt.unit_object import _validate_dimensions\n\n\"\"\"\nA registry for units that can be added to and modified.\n\n\n\"\"\"\n\n# -----------------------------------------------------------------------------\n# Copyright (c) 2018, yt Development Team.\n#\n# Distributed under the terms of the Modified BSD License.\n#\n# The full license is in the LICENSE file, distributed with this software.\n# -----------------------------------------------------------------------------\n\n\n\n\n\ndef _sanitize_unit_system(unit_system, obj):\n\n    if unit_system is None:\n        try:\n            unit_system = obj.units.registry.unit_system\n        except AttributeError:\n", "next_line": "            unit_system = mks_unit_system", "id": 8, "__internal_uuid__": "1a59186a-d727-4b2a-8b68-35b3a0a49af6"}
{"repo_name": "auDeep/auDeep", "file_path": "audeep/backend/parsers/base.py", "context": "# Path: audeep/backend/data/data_set.py\n# class Split(Enum):\n#     \"\"\"\n#     Identifiers for cross validation splits.\n#     \"\"\"\n#     TRAIN = 0\n#     VALID = 1\n# \n# class Partition(Enum):\n#     \"\"\"\n#     Identifiers for different data partitions.\n#     \"\"\"\n#     TRAIN = 0\n#     DEVEL = 1\n#     TEST = 2\n\n", "import_statement": "import abc\nfrom pathlib import Path\nfrom typing import Sequence, Mapping, Optional\nfrom audeep.backend.data.data_set import Split, Partition", "code": "#\n# auDeep is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# auDeep is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n\n# You should have received a copy of the GNU General Public License\n# along with auDeep. If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Defines an abstract interface for data set parsers\"\"\"\n\n\n\nclass _InstanceMetadata:\n    \"\"\"\n    Stores metadata about a single instance.\n    \n    This class should only be used within the audeep.backend.parsers module.\n    \"\"\"\n\n    def __init__(self,\n                 path: Path,\n                 filename: str,\n                 label_nominal: Optional[str],\n                 label_numeric: Optional[int],\n", "prompt": "# Path: audeep/backend/data/data_set.py\n# class Split(Enum):\n#     \"\"\"\n#     Identifiers for cross validation splits.\n#     \"\"\"\n#     TRAIN = 0\n#     VALID = 1\n# \n# class Partition(Enum):\n#     \"\"\"\n#     Identifiers for different data partitions.\n#     \"\"\"\n#     TRAIN = 0\n#     DEVEL = 1\n#     TEST = 2\n\n\n# Path: audeep/backend/parsers/base.py\nimport abc\nfrom pathlib import Path\nfrom typing import Sequence, Mapping, Optional\nfrom audeep.backend.data.data_set import Split, Partition\n\n#\n# auDeep is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# auDeep is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n\n# You should have received a copy of the GNU General Public License\n# along with auDeep. If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"Defines an abstract interface for data set parsers\"\"\"\n\n\n\nclass _InstanceMetadata:\n    \"\"\"\n    Stores metadata about a single instance.\n    \n    This class should only be used within the audeep.backend.parsers module.\n    \"\"\"\n\n    def __init__(self,\n                 path: Path,\n                 filename: str,\n                 label_nominal: Optional[str],\n                 label_numeric: Optional[int],\n", "next_line": "                 cv_folds: Optional[Sequence[Split]],", "id": 9, "__internal_uuid__": "4ebaa55f-87c2-416e-91e6-3c16cdfc5f8b"}
{"repo_name": "CleanCut/green", "file_path": "green/test/test_config.py", "context": "# Path: green/config.py\n# class StoreOpt(object):  # pragma: no cover\n# class ConfigFile(object):  # pragma: no cover\n#     def __init__(self):\n#     def __call__(self, action):\n# def parseArguments(argv=None):  # pragma: no cover\n#     def __init__(self, filepath):\n#     def readline(self):\n#     def next(self):\n#     def __iter__(self):\n#     def __next__(self):\n# def getConfig(filepath=None):  # pragma: no cover\n# def mergeConfig(args, testing=False):  # pragma: no cover\n# \n# Path: green/output.py\n# class GreenStream(object):\n#     \"\"\"\n#     Wraps a stream-like object with the following additonal features:\n# \n#     1) A handy writeln() method (which calls write() under-the-hood)\n#     2) Handy formatLine() and formatText() methods, which support indent\n#        levels, and outcome codes.\n#     3) Compatibility with real file objects (by implementing real file object\n#        methods as we discover people need them).  So far we have implemented the\n#        following functions just for compatibility:\n#            writelines(lines)\n#     \"\"\"\n# \n#     indent_spaces = 2\n#     _ascii_only_output = False  # default to printing output in unicode\n#     coverage_pattern = re.compile(r\"TOTAL\\s+\\d+\\s+\\d+\\s+(?P<percent>\\d+)%\")\n# \n#     def __init__(\n#         self,\n#         stream,\n#         override_appveyor=False,\n#         disable_windows=False,\n#         disable_unidecode=False,\n#     ):\n#         self.disable_unidecode = disable_unidecode\n#         self.stream = stream\n#         # Ironically, Windows CI platforms such as GitHub Actions and AppVeyor don't support windows\n#         # win32 system calls for colors, but it WILL interpret posix ansi escape codes! (The\n#         # opposite of an actual windows command prompt)\n#         on_windows = platform.system() == \"Windows\"\n#         on_windows_ci = os.environ.get(\"GITHUB_ACTIONS\", False) or os.environ.get(\n#             \"APPVEYOR\", False\n#         )\n# \n#         if override_appveyor or (\n#             (on_windows and not on_windows_ci) and not disable_windows\n#         ):  # pragma: no cover\n#             self.stream = wrap_stream(self.stream, None, None, None, True)\n#             # set output is ascii-only\n#             self._ascii_only_output = True\n#         self.closed = False\n#         # z3 likes to look at sys.stdout.encoding\n#         try:\n#             self.encoding = stream.encoding\n#         except:\n#             self.encoding = \"UTF-8\"\n#         # Susceptible to false-positives if other matching lines are output,\n#         # so set this to None immediately before running a coverage report to\n#         # guarantee accuracy.\n#         self.coverage_percent = None\n# \n#     def flush(self):\n#         self.stream.flush()\n# \n#     def writeln(self, text=\"\"):\n#         self.write(text + \"\\n\")\n# \n#     def write(self, text):\n#         if type(text) == bytes:\n#             text = text.decode(\"utf-8\")\n#         # Compensate for windows' anti-social unicode behavior\n#         if self._ascii_only_output and not self.disable_unidecode:\n#             # Windows doesn't actually want unicode, so we get\n#             # the closest ASCII equivalent\n#             text = text_type(unidecode(text))\n#         # Since coverage doesn't like us switching out it's stream to run extra\n#         # reports to look for percent covered. We should replace this with\n#         # grabbing the percentage directly from coverage if we can figure out\n#         # how.\n#         match = self.coverage_pattern.search(text)\n#         if match:\n#             percent_str = match.groupdict().get(\"percent\")\n#             if percent_str:\n#                 self.coverage_percent = int(percent_str)\n#         self.stream.write(text)\n# \n#     def writelines(self, lines):\n#         \"\"\"\n#         Just for better compatibility with real file objects\n#         \"\"\"\n#         for line in lines:\n#             self.write(line)\n# \n#     def formatText(self, text, indent=0, outcome_char=\"\"):\n#         # We'll go through each line in the text, modify it, and store it in a\n#         # new list\n#         updated_lines = []\n#         for line in text.split(\"\\n\"):\n#             # We only need to format the line if there's something visible on\n#             # it.\n#             if line.strip(\" \"):\n#                 updated_lines.append(self.formatLine(line, indent, outcome_char))\n#             else:\n#                 updated_lines.append(\"\")\n#             outcome_char = \"\"  # only the first line gets an outcome character\n#         # Join the list back together\n#         output = \"\\n\".join(updated_lines)\n#         return output\n# \n#     def formatLine(self, line, indent=0, outcome_char=\"\"):\n#         \"\"\"\n#         Takes a single line, optionally adds an indent and/or outcome\n#         character to the beginning of the line.\n#         \"\"\"\n#         actual_spaces = (indent * self.indent_spaces) - len(outcome_char)\n#         return outcome_char + \" \" * actual_spaces + line\n# \n#     def isatty(self):\n#         \"\"\"\n#         Wrap internal self.stream.isatty.\n#         \"\"\"\n#         return self.stream.isatty()\n\n", "import_statement": "    import configparser\n    import ConfigParser as configparser\nimport copy\nimport os\nimport shutil\nimport tempfile\nimport unittest\nfrom io import StringIO\nfrom green import config\nfrom green.output import GreenStream", "code": "        Result: empty config\n        \"\"\"\n        os.unlink(self.default_filename)\n        os.unlink(self.env_filename)\n        os.unlink(self.cmd_filename)\n        with ModifiedEnvironment(GREEN_CONFIG=None, HOME=self.tmpd):\n            cfg = config.getConfig()\n            ae = self.assertEqual\n            ar = self.assertRaises\n            ae(self.setup_verbose, cfg.getint(\"green\", \"verbose\"))\n            ae(self.setup_failfast, cfg.getboolean(\"green\", \"failfast\"))\n            ar(configparser.NoOptionError, cfg.get, \"green\", \"omit-patterns\")\n            ar(configparser.NoOptionError, cfg.get, \"green\", \"run-coverage\")\n            ar(configparser.NoOptionError, cfg.get, \"green\", \"logging\")\n            ar(configparser.NoOptionError, cfg.get, \"green\", \"no-skip-report\")\n            ar(configparser.NoOptionError, cfg.get, \"green\", \"version\")\n\n\nclass TestMergeConfig(ConfigBase):\n    \"\"\"\n    Merging config files and command-line arguments works as expected.\n    \"\"\"\n\n    def test_overwrite(self):\n        \"\"\"\n        Non-default command-line argument values overwrite config values.\n        \"\"\"\n        # This config environment should set the values we look at to False and\n        # a filename in omit-patterns\n        s = StringIO()\n", "prompt": "# Path: green/config.py\n# class StoreOpt(object):  # pragma: no cover\n# class ConfigFile(object):  # pragma: no cover\n#     def __init__(self):\n#     def __call__(self, action):\n# def parseArguments(argv=None):  # pragma: no cover\n#     def __init__(self, filepath):\n#     def readline(self):\n#     def next(self):\n#     def __iter__(self):\n#     def __next__(self):\n# def getConfig(filepath=None):  # pragma: no cover\n# def mergeConfig(args, testing=False):  # pragma: no cover\n# \n# Path: green/output.py\n# class GreenStream(object):\n#     \"\"\"\n#     Wraps a stream-like object with the following additonal features:\n# \n#     1) A handy writeln() method (which calls write() under-the-hood)\n#     2) Handy formatLine() and formatText() methods, which support indent\n#        levels, and outcome codes.\n#     3) Compatibility with real file objects (by implementing real file object\n#        methods as we discover people need them).  So far we have implemented the\n#        following functions just for compatibility:\n#            writelines(lines)\n#     \"\"\"\n# \n#     indent_spaces = 2\n#     _ascii_only_output = False  # default to printing output in unicode\n#     coverage_pattern = re.compile(r\"TOTAL\\s+\\d+\\s+\\d+\\s+(?P<percent>\\d+)%\")\n# \n#     def __init__(\n#         self,\n#         stream,\n#         override_appveyor=False,\n#         disable_windows=False,\n#         disable_unidecode=False,\n#     ):\n#         self.disable_unidecode = disable_unidecode\n#         self.stream = stream\n#         # Ironically, Windows CI platforms such as GitHub Actions and AppVeyor don't support windows\n#         # win32 system calls for colors, but it WILL interpret posix ansi escape codes! (The\n#         # opposite of an actual windows command prompt)\n#         on_windows = platform.system() == \"Windows\"\n#         on_windows_ci = os.environ.get(\"GITHUB_ACTIONS\", False) or os.environ.get(\n#             \"APPVEYOR\", False\n#         )\n# \n#         if override_appveyor or (\n#             (on_windows and not on_windows_ci) and not disable_windows\n#         ):  # pragma: no cover\n#             self.stream = wrap_stream(self.stream, None, None, None, True)\n#             # set output is ascii-only\n#             self._ascii_only_output = True\n#         self.closed = False\n#         # z3 likes to look at sys.stdout.encoding\n#         try:\n#             self.encoding = stream.encoding\n#         except:\n#             self.encoding = \"UTF-8\"\n#         # Susceptible to false-positives if other matching lines are output,\n#         # so set this to None immediately before running a coverage report to\n#         # guarantee accuracy.\n#         self.coverage_percent = None\n# \n#     def flush(self):\n#         self.stream.flush()\n# \n#     def writeln(self, text=\"\"):\n#         self.write(text + \"\\n\")\n# \n#     def write(self, text):\n#         if type(text) == bytes:\n#             text = text.decode(\"utf-8\")\n#         # Compensate for windows' anti-social unicode behavior\n#         if self._ascii_only_output and not self.disable_unidecode:\n#             # Windows doesn't actually want unicode, so we get\n#             # the closest ASCII equivalent\n#             text = text_type(unidecode(text))\n#         # Since coverage doesn't like us switching out it's stream to run extra\n#         # reports to look for percent covered. We should replace this with\n#         # grabbing the percentage directly from coverage if we can figure out\n#         # how.\n#         match = self.coverage_pattern.search(text)\n#         if match:\n#             percent_str = match.groupdict().get(\"percent\")\n#             if percent_str:\n#                 self.coverage_percent = int(percent_str)\n#         self.stream.write(text)\n# \n#     def writelines(self, lines):\n#         \"\"\"\n#         Just for better compatibility with real file objects\n#         \"\"\"\n#         for line in lines:\n#             self.write(line)\n# \n#     def formatText(self, text, indent=0, outcome_char=\"\"):\n#         # We'll go through each line in the text, modify it, and store it in a\n#         # new list\n#         updated_lines = []\n#         for line in text.split(\"\\n\"):\n#             # We only need to format the line if there's something visible on\n#             # it.\n#             if line.strip(\" \"):\n#                 updated_lines.append(self.formatLine(line, indent, outcome_char))\n#             else:\n#                 updated_lines.append(\"\")\n#             outcome_char = \"\"  # only the first line gets an outcome character\n#         # Join the list back together\n#         output = \"\\n\".join(updated_lines)\n#         return output\n# \n#     def formatLine(self, line, indent=0, outcome_char=\"\"):\n#         \"\"\"\n#         Takes a single line, optionally adds an indent and/or outcome\n#         character to the beginning of the line.\n#         \"\"\"\n#         actual_spaces = (indent * self.indent_spaces) - len(outcome_char)\n#         return outcome_char + \" \" * actual_spaces + line\n# \n#     def isatty(self):\n#         \"\"\"\n#         Wrap internal self.stream.isatty.\n#         \"\"\"\n#         return self.stream.isatty()\n\n\n# Path: green/test/test_config.py\n    import configparser\n    import ConfigParser as configparser\nimport copy\nimport os\nimport shutil\nimport tempfile\nimport unittest\nfrom io import StringIO\nfrom green import config\nfrom green.output import GreenStream\n\n        Result: empty config\n        \"\"\"\n        os.unlink(self.default_filename)\n        os.unlink(self.env_filename)\n        os.unlink(self.cmd_filename)\n        with ModifiedEnvironment(GREEN_CONFIG=None, HOME=self.tmpd):\n            cfg = config.getConfig()\n            ae = self.assertEqual\n            ar = self.assertRaises\n            ae(self.setup_verbose, cfg.getint(\"green\", \"verbose\"))\n            ae(self.setup_failfast, cfg.getboolean(\"green\", \"failfast\"))\n            ar(configparser.NoOptionError, cfg.get, \"green\", \"omit-patterns\")\n            ar(configparser.NoOptionError, cfg.get, \"green\", \"run-coverage\")\n            ar(configparser.NoOptionError, cfg.get, \"green\", \"logging\")\n            ar(configparser.NoOptionError, cfg.get, \"green\", \"no-skip-report\")\n            ar(configparser.NoOptionError, cfg.get, \"green\", \"version\")\n\n\nclass TestMergeConfig(ConfigBase):\n    \"\"\"\n    Merging config files and command-line arguments works as expected.\n    \"\"\"\n\n    def test_overwrite(self):\n        \"\"\"\n        Non-default command-line argument values overwrite config values.\n        \"\"\"\n        # This config environment should set the values we look at to False and\n        # a filename in omit-patterns\n        s = StringIO()\n", "next_line": "        gs = GreenStream(s)", "id": 10, "__internal_uuid__": "79fa9e42-61d2-4682-8626-2f0d726245a2"}
{"repo_name": "deepmind/brave", "file_path": "brave/evaluate/evaluate_video_embedding_test.py", "context": "# Path: brave/datasets/datasets.py\n# class View:\n# class MiniBatch:\n#   def __repr__(self):\n#   def __repr__(self):\n# def multi_view_dataset(\n#     shards: Sequence[str],\n#     features: Sequence[media_sequences.FeatureKind],\n#     view_sampler: ViewSamplerFn,\n#     view_decoder: ViewDecoderFn,\n#     *,\n#     shuffle: bool = False,\n#     shard_reader: media_sequences.ShardReaderFn = media_sequences\n#     .tf_record_shard_reader,\n# ) -> tf.data.Dataset:\n# def _multi_view_batches_from_sequences(ds: tf.data.Dataset,\n#                                        view_sampler: ViewSamplerFn,\n#                                        view_decoder: ViewDecoderFn, *,\n#                                        deterministic: bool) -> tf.data.Dataset:\n# def _make_batch(sequences: Dict[str, media_sequences.EncodedSequence],\n#                 view_decoder: ViewDecoderFn) -> MiniBatch:\n# \n# Path: brave/datasets/fixtures.py\n# DEFAULT_AUDIO_SAMPLES = int(10.376 * 48_000)\n# def write_tf_record_dataset_fixture(path: str) -> List[str]:\n# def _make_fixture() -> Dict[str, bytes]:\n# def _fake_sequence_1(label: int) -> tf.train.SequenceExample:\n# def _jpeg_feature(img: np.ndarray) -> tf.train.Feature:\n# def _audio_feature(value: np.ndarray) -> tf.train.Feature:\n# def _label_feature(value: Sequence[int]) -> tf.train.Feature:\n# \n# Path: brave/evaluate/evaluate_video_embedding.py\n# def evaluate_video_embedding(\n#     train_dataset_shards: Sequence[str],\n#     test_dataset_shards: Sequence[str],\n#     embedding_fn: evaluate.EmbeddingFn,\n#     config: VideoConfig,\n#     svm_regularization: float,\n#     batch_size: int = DEFAULT_EVAL_BATCH_SIZE,\n#     shard_reader: media_sequences.ShardReaderFn = media_sequences\n#     .tf_record_shard_reader,\n# ) -> evaluate.EvaluationResults:\n#   \"\"\"Standardized evaluation for embeddings.\"\"\"\n# \n#   train_ds = eval_datasets.random_sampling_dataset(\n#       train_dataset_shards,\n#       image_size=config.image_size,\n#       num_video_frames=config.num_frames,\n#       video_step=config.video_step,\n#       min_crop_window_area=DEFAULT_TRAIN_MIN_CROP_WINDOW_AREA,\n#       max_crop_window_area=DEFAULT_TRAIN_MAX_CROP_WINDOW_AREA,\n#       min_crop_window_aspect_ratio=DEFAULT_TRAIN_MIN_CROP_WINDOW_ASPECT_RATIO,\n#       max_crop_window_aspect_ratio=DEFAULT_TRAIN_MAX_CROP_WINDOW_ASPECT_RATIO,\n#       shuffle=True,\n#       shard_reader=shard_reader)\n# \n#   train_ds = train_ds.map(_transform_train, num_parallel_calls=tf.data.AUTOTUNE)\n#   train_ds = train_ds.repeat(DEFAULT_EVAL_NUM_TRAIN_EPOCHS)\n#   train_ds = train_ds.batch(batch_size)\n#   train_ds = tfds.as_numpy(train_ds)\n# \n#   test_ds = eval_datasets.multiple_crop_dataset(\n#       test_dataset_shards,\n#       num_temporal_crops=DEFAULT_TEST_NUM_TEMPORAL_CROPS,\n#       num_spatial_crops=DEFAULT_TEST_NUM_SPATIAL_CROPS,\n#       num_video_frames=config.num_frames,\n#       video_step=config.video_step,\n#       initial_resize=DEFAULT_TEST_INITIAL_RESIZE,\n#       center_crop_size=config.image_size,\n#       shuffle=False,\n#       shard_reader=shard_reader)\n# \n#   test_ds = test_ds.map(_transform_test, num_parallel_calls=tf.data.AUTOTUNE)\n#   test_ds = test_ds.batch(batch_size)\n#   test_ds = tfds.as_numpy(test_ds)\n# \n#   group_size = DEFAULT_TEST_NUM_TEMPORAL_CROPS * DEFAULT_TEST_NUM_SPATIAL_CROPS\n# \n#   return evaluate.linear_svm_classifier(\n#       train_ds,\n#       test_ds,\n#       embedding_fn,\n#       test_predictions_group_size=group_size,\n#       svm_regularization=svm_regularization)\n\n", "import_statement": "import tempfile\nimport chex\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom brave.datasets import datasets\nfrom brave.datasets import fixtures\nfrom brave.evaluate import evaluate_video_embedding", "code": "# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Tests for evaluate video embedding.\"\"\"\n\n\n\n\n\nclass EvaluateVideoEmbeddingTest(parameterized.TestCase):\n\n  def test_evaluate_embedding(self):\n    with tempfile.TemporaryDirectory() as fixture_dir:\n      shards = fixtures.write_tf_record_dataset_fixture(fixture_dir)\n      rng = jax.random.PRNGKey(0)\n\n      def fake_embedding(view, is_training):\n        del is_training\n        video = view.video\n        chex.assert_rank(video, 5)  # B, T, H, W, C\n        flat_video = jnp.reshape(video, (video.shape[0], -1))\n        feats = flat_video[..., :16]\n        chex.assert_shape(feats, (None, 16))\n\n        return hk.Linear(2048)(feats)\n\n      fake_embedding_fn = hk.transform(fake_embedding)\n", "prompt": "# Path: brave/datasets/datasets.py\n# class View:\n# class MiniBatch:\n#   def __repr__(self):\n#   def __repr__(self):\n# def multi_view_dataset(\n#     shards: Sequence[str],\n#     features: Sequence[media_sequences.FeatureKind],\n#     view_sampler: ViewSamplerFn,\n#     view_decoder: ViewDecoderFn,\n#     *,\n#     shuffle: bool = False,\n#     shard_reader: media_sequences.ShardReaderFn = media_sequences\n#     .tf_record_shard_reader,\n# ) -> tf.data.Dataset:\n# def _multi_view_batches_from_sequences(ds: tf.data.Dataset,\n#                                        view_sampler: ViewSamplerFn,\n#                                        view_decoder: ViewDecoderFn, *,\n#                                        deterministic: bool) -> tf.data.Dataset:\n# def _make_batch(sequences: Dict[str, media_sequences.EncodedSequence],\n#                 view_decoder: ViewDecoderFn) -> MiniBatch:\n# \n# Path: brave/datasets/fixtures.py\n# DEFAULT_AUDIO_SAMPLES = int(10.376 * 48_000)\n# def write_tf_record_dataset_fixture(path: str) -> List[str]:\n# def _make_fixture() -> Dict[str, bytes]:\n# def _fake_sequence_1(label: int) -> tf.train.SequenceExample:\n# def _jpeg_feature(img: np.ndarray) -> tf.train.Feature:\n# def _audio_feature(value: np.ndarray) -> tf.train.Feature:\n# def _label_feature(value: Sequence[int]) -> tf.train.Feature:\n# \n# Path: brave/evaluate/evaluate_video_embedding.py\n# def evaluate_video_embedding(\n#     train_dataset_shards: Sequence[str],\n#     test_dataset_shards: Sequence[str],\n#     embedding_fn: evaluate.EmbeddingFn,\n#     config: VideoConfig,\n#     svm_regularization: float,\n#     batch_size: int = DEFAULT_EVAL_BATCH_SIZE,\n#     shard_reader: media_sequences.ShardReaderFn = media_sequences\n#     .tf_record_shard_reader,\n# ) -> evaluate.EvaluationResults:\n#   \"\"\"Standardized evaluation for embeddings.\"\"\"\n# \n#   train_ds = eval_datasets.random_sampling_dataset(\n#       train_dataset_shards,\n#       image_size=config.image_size,\n#       num_video_frames=config.num_frames,\n#       video_step=config.video_step,\n#       min_crop_window_area=DEFAULT_TRAIN_MIN_CROP_WINDOW_AREA,\n#       max_crop_window_area=DEFAULT_TRAIN_MAX_CROP_WINDOW_AREA,\n#       min_crop_window_aspect_ratio=DEFAULT_TRAIN_MIN_CROP_WINDOW_ASPECT_RATIO,\n#       max_crop_window_aspect_ratio=DEFAULT_TRAIN_MAX_CROP_WINDOW_ASPECT_RATIO,\n#       shuffle=True,\n#       shard_reader=shard_reader)\n# \n#   train_ds = train_ds.map(_transform_train, num_parallel_calls=tf.data.AUTOTUNE)\n#   train_ds = train_ds.repeat(DEFAULT_EVAL_NUM_TRAIN_EPOCHS)\n#   train_ds = train_ds.batch(batch_size)\n#   train_ds = tfds.as_numpy(train_ds)\n# \n#   test_ds = eval_datasets.multiple_crop_dataset(\n#       test_dataset_shards,\n#       num_temporal_crops=DEFAULT_TEST_NUM_TEMPORAL_CROPS,\n#       num_spatial_crops=DEFAULT_TEST_NUM_SPATIAL_CROPS,\n#       num_video_frames=config.num_frames,\n#       video_step=config.video_step,\n#       initial_resize=DEFAULT_TEST_INITIAL_RESIZE,\n#       center_crop_size=config.image_size,\n#       shuffle=False,\n#       shard_reader=shard_reader)\n# \n#   test_ds = test_ds.map(_transform_test, num_parallel_calls=tf.data.AUTOTUNE)\n#   test_ds = test_ds.batch(batch_size)\n#   test_ds = tfds.as_numpy(test_ds)\n# \n#   group_size = DEFAULT_TEST_NUM_TEMPORAL_CROPS * DEFAULT_TEST_NUM_SPATIAL_CROPS\n# \n#   return evaluate.linear_svm_classifier(\n#       train_ds,\n#       test_ds,\n#       embedding_fn,\n#       test_predictions_group_size=group_size,\n#       svm_regularization=svm_regularization)\n\n\n# Path: brave/evaluate/evaluate_video_embedding_test.py\nimport tempfile\nimport chex\nimport haiku as hk\nimport jax\nimport jax.numpy as jnp\nimport numpy as np\nfrom absl.testing import absltest\nfrom absl.testing import parameterized\nfrom brave.datasets import datasets\nfrom brave.datasets import fixtures\nfrom brave.evaluate import evaluate_video_embedding\n\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"Tests for evaluate video embedding.\"\"\"\n\n\n\n\n\nclass EvaluateVideoEmbeddingTest(parameterized.TestCase):\n\n  def test_evaluate_embedding(self):\n    with tempfile.TemporaryDirectory() as fixture_dir:\n      shards = fixtures.write_tf_record_dataset_fixture(fixture_dir)\n      rng = jax.random.PRNGKey(0)\n\n      def fake_embedding(view, is_training):\n        del is_training\n        video = view.video\n        chex.assert_rank(video, 5)  # B, T, H, W, C\n        flat_video = jnp.reshape(video, (video.shape[0], -1))\n        feats = flat_video[..., :16]\n        chex.assert_shape(feats, (None, 16))\n\n        return hk.Linear(2048)(feats)\n\n      fake_embedding_fn = hk.transform(fake_embedding)\n", "next_line": "      view = datasets.View(", "id": 11, "__internal_uuid__": "f42854a8-f7a7-433d-b9be-4ec40cdd9ef1"}
{"repo_name": "apacha/OMR-Datasets", "file_path": "omrdatasettools/HomusImageGenerator.py", "context": "# Path: omrdatasettools/Point2D.py\n# class Point2D():\n#     \"\"\"A point in a 2-dimensional Euclidean space. \"\"\"\n# \n#     def __init__(self, x: float, y: float) -> None:\n#         super().__init__()\n#         self.x = x\n#         self.y = y\n# \n#     def __eq__(self, o: object) -> bool:\n#         return isinstance(o, Point2D) and self.x == o.x and self.y == o.y\n# \n# Path: omrdatasettools/ExportPath.py\n# class ExportPath:\n#     \"\"\" An internal helper class to automatically build path names when generating images from annotations with variations \"\"\"\n# \n#     def __init__(self, destination_directory: str, symbol_class: str, raw_file_name_without_extension: str,\n#                  extension: str = \"png\", stroke_thickness: int = None) -> None:\n#         super().__init__()\n#         self.stroke_thickness = stroke_thickness\n#         self.extension = extension\n#         self.raw_file_name_without_extension = raw_file_name_without_extension\n#         self.symbol_class = symbol_class\n#         self.destination_directory = destination_directory\n# \n#     def get_full_path(self, offset: int = None):\n#         \"\"\"\n#         :return: Returns the full path that will join all fields according to the following format if no offset if provided:\n#         'destination_directory'/'symbol_class'/'raw_file_name_without_extension'_'stroke_thickness'.'extension',\n#         e.g.: data/images/3-4-Time/1-13_3.png\n# \n#         or with an additional offset-appendix if an offset is provided\n#         'destination_directory'/'symbol_class'/'raw_file_name_without_extension'_'stroke_thickness'_offset_'offset'.'extension',\n#         e.g.: data/images/3-4-Time/1-13_3_offset_74.png\n#         \"\"\"\n#         stroke_thickness = \"\"\n#         if self.stroke_thickness is not None:\n#             stroke_thickness = \"_{0}\".format(self.stroke_thickness)\n# \n#         staffline_offset = \"\"\n#         if offset is not None:\n#             staffline_offset = \"_offset_{0}\".format(offset)\n# \n#         return os.path.join(self.destination_directory, self.symbol_class,\n#                             \"{0}{1}{2}.{3}\".format(self.raw_file_name_without_extension,\n#                                                    stroke_thickness, staffline_offset, self.extension))\n# \n#     def get_class_name_and_file_path(self, offset: int = None):\n# \n#         staffline_offset = \"\"\n#         if offset is not None:\n#             staffline_offset = \"_offset_{0}\".format(offset)\n# \n#         return os.path.join(self.symbol_class, \"{0}_{1}{2}.{3}\".format(self.raw_file_name_without_extension,\n#                                                                        self.stroke_thickness, staffline_offset,\n#                                                                        self.extension))\n# \n# Path: omrdatasettools/Rectangle.py\n# class Rectangle:\n#     def __init__(self, origin: Point2D, width: int, height: int):\n#         super().__init__()\n#         self.height = height\n#         self.width = width\n#         self.origin = origin  # Resembles the left top point\n#         self.left = origin.x\n#         self.top = origin.y\n#         self.right = self.left + self.width\n#         self.bottom = self.top + self.height\n# \n#     @staticmethod\n#     def overlap(r1: 'Rectangle', r2: 'Rectangle'):\n#         \"\"\"\n#         Overlapping rectangles overlap both horizontally & vertically\n#         \"\"\"\n#         h_overlaps = (r1.left <= r2.right) and (r1.right >= r2.left)\n#         v_overlaps = (r1.bottom >= r2.top) and (r1.top <= r2.bottom)\n#         return h_overlaps and v_overlaps\n# \n#     @staticmethod\n#     def merge(r1: 'Rectangle', r2: 'Rectangle') -> 'Rectangle':\n#         left = min(r1.left, r2.left)\n#         top = min(r1.top, r2.top)\n#         right = max(r1.right, r2.right)\n#         bottom = max(r1.bottom, r2.bottom)\n#         width = right - left\n#         height = bottom - top\n# \n#         return Rectangle(Point2D(left, top), width, height)\n# \n#     def as_bounding_box_with_margin(self, margin: int = 1) -> Tuple[int, int, int, int]:\n#         bounding_box_with_margin = (self.left - margin,\n#                                     self.top - margin,\n#                                     self.left + self.width + 2 * margin,\n#                                     self.top + self.height + 2 * margin)\n#         return bounding_box_with_margin\n# \n#     def __eq__(self, o: object) -> bool:\n#         are_equal = self.width == o.width and self.height == o.height and self.origin == o.origin\n#         return are_equal\n# \n#     def __str__(self) -> str:\n#         return \"Rectangle[Origin:{0},{1}, Width:{2}, Height:{3}]\".format(self.origin.x, self.origin.y, self.width,\n#                                                                          self.height)\n\n", "import_statement": "import argparse\nimport os\nimport random\nimport sys\nfrom glob import glob\nfrom typing import List\nfrom PIL import Image, ImageDraw\nfrom tqdm import tqdm\nfrom omrdatasettools.Point2D import Point2D\nfrom omrdatasettools.ExportPath import ExportPath\nfrom omrdatasettools.Rectangle import Rectangle", "code": "        min_x = sys.maxsize\n        max_x = 0\n        min_y = sys.maxsize\n        max_y = 0\n\n        symbol_name = lines[0]\n        strokes = []\n\n        for stroke_string in lines[1:]:\n            stroke = []\n\n            for point_string in stroke_string.split(\";\"):\n                if point_string is \"\":\n                    continue  # Skip the last element, that is due to a trailing ; in each line\n\n                point_x, point_y = point_string.split(\",\")\n                x = int(point_x)\n                y = int(point_y)\n                stroke.append(Point2D(x, y))\n\n                max_x = max(max_x, x)\n                min_x = min(min_x, x)\n                max_y = max(max_y, y)\n                min_y = min(min_y, y)\n\n            strokes.append(stroke)\n\n        dimensions = Rectangle(Point2D(min_x, min_y), max_x - min_x + 1, max_y - min_y + 1)\n        return HomusSymbol(content, strokes, symbol_name, dimensions)\n\n", "prompt": "# Path: omrdatasettools/Point2D.py\n# class Point2D():\n#     \"\"\"A point in a 2-dimensional Euclidean space. \"\"\"\n# \n#     def __init__(self, x: float, y: float) -> None:\n#         super().__init__()\n#         self.x = x\n#         self.y = y\n# \n#     def __eq__(self, o: object) -> bool:\n#         return isinstance(o, Point2D) and self.x == o.x and self.y == o.y\n# \n# Path: omrdatasettools/ExportPath.py\n# class ExportPath:\n#     \"\"\" An internal helper class to automatically build path names when generating images from annotations with variations \"\"\"\n# \n#     def __init__(self, destination_directory: str, symbol_class: str, raw_file_name_without_extension: str,\n#                  extension: str = \"png\", stroke_thickness: int = None) -> None:\n#         super().__init__()\n#         self.stroke_thickness = stroke_thickness\n#         self.extension = extension\n#         self.raw_file_name_without_extension = raw_file_name_without_extension\n#         self.symbol_class = symbol_class\n#         self.destination_directory = destination_directory\n# \n#     def get_full_path(self, offset: int = None):\n#         \"\"\"\n#         :return: Returns the full path that will join all fields according to the following format if no offset if provided:\n#         'destination_directory'/'symbol_class'/'raw_file_name_without_extension'_'stroke_thickness'.'extension',\n#         e.g.: data/images/3-4-Time/1-13_3.png\n# \n#         or with an additional offset-appendix if an offset is provided\n#         'destination_directory'/'symbol_class'/'raw_file_name_without_extension'_'stroke_thickness'_offset_'offset'.'extension',\n#         e.g.: data/images/3-4-Time/1-13_3_offset_74.png\n#         \"\"\"\n#         stroke_thickness = \"\"\n#         if self.stroke_thickness is not None:\n#             stroke_thickness = \"_{0}\".format(self.stroke_thickness)\n# \n#         staffline_offset = \"\"\n#         if offset is not None:\n#             staffline_offset = \"_offset_{0}\".format(offset)\n# \n#         return os.path.join(self.destination_directory, self.symbol_class,\n#                             \"{0}{1}{2}.{3}\".format(self.raw_file_name_without_extension,\n#                                                    stroke_thickness, staffline_offset, self.extension))\n# \n#     def get_class_name_and_file_path(self, offset: int = None):\n# \n#         staffline_offset = \"\"\n#         if offset is not None:\n#             staffline_offset = \"_offset_{0}\".format(offset)\n# \n#         return os.path.join(self.symbol_class, \"{0}_{1}{2}.{3}\".format(self.raw_file_name_without_extension,\n#                                                                        self.stroke_thickness, staffline_offset,\n#                                                                        self.extension))\n# \n# Path: omrdatasettools/Rectangle.py\n# class Rectangle:\n#     def __init__(self, origin: Point2D, width: int, height: int):\n#         super().__init__()\n#         self.height = height\n#         self.width = width\n#         self.origin = origin  # Resembles the left top point\n#         self.left = origin.x\n#         self.top = origin.y\n#         self.right = self.left + self.width\n#         self.bottom = self.top + self.height\n# \n#     @staticmethod\n#     def overlap(r1: 'Rectangle', r2: 'Rectangle'):\n#         \"\"\"\n#         Overlapping rectangles overlap both horizontally & vertically\n#         \"\"\"\n#         h_overlaps = (r1.left <= r2.right) and (r1.right >= r2.left)\n#         v_overlaps = (r1.bottom >= r2.top) and (r1.top <= r2.bottom)\n#         return h_overlaps and v_overlaps\n# \n#     @staticmethod\n#     def merge(r1: 'Rectangle', r2: 'Rectangle') -> 'Rectangle':\n#         left = min(r1.left, r2.left)\n#         top = min(r1.top, r2.top)\n#         right = max(r1.right, r2.right)\n#         bottom = max(r1.bottom, r2.bottom)\n#         width = right - left\n#         height = bottom - top\n# \n#         return Rectangle(Point2D(left, top), width, height)\n# \n#     def as_bounding_box_with_margin(self, margin: int = 1) -> Tuple[int, int, int, int]:\n#         bounding_box_with_margin = (self.left - margin,\n#                                     self.top - margin,\n#                                     self.left + self.width + 2 * margin,\n#                                     self.top + self.height + 2 * margin)\n#         return bounding_box_with_margin\n# \n#     def __eq__(self, o: object) -> bool:\n#         are_equal = self.width == o.width and self.height == o.height and self.origin == o.origin\n#         return are_equal\n# \n#     def __str__(self) -> str:\n#         return \"Rectangle[Origin:{0},{1}, Width:{2}, Height:{3}]\".format(self.origin.x, self.origin.y, self.width,\n#                                                                          self.height)\n\n\n# Path: omrdatasettools/HomusImageGenerator.py\nimport argparse\nimport os\nimport random\nimport sys\nfrom glob import glob\nfrom typing import List\nfrom PIL import Image, ImageDraw\nfrom tqdm import tqdm\nfrom omrdatasettools.Point2D import Point2D\nfrom omrdatasettools.ExportPath import ExportPath\nfrom omrdatasettools.Rectangle import Rectangle\n\n        min_x = sys.maxsize\n        max_x = 0\n        min_y = sys.maxsize\n        max_y = 0\n\n        symbol_name = lines[0]\n        strokes = []\n\n        for stroke_string in lines[1:]:\n            stroke = []\n\n            for point_string in stroke_string.split(\";\"):\n                if point_string is \"\":\n                    continue  # Skip the last element, that is due to a trailing ; in each line\n\n                point_x, point_y = point_string.split(\",\")\n                x = int(point_x)\n                y = int(point_y)\n                stroke.append(Point2D(x, y))\n\n                max_x = max(max_x, x)\n                min_x = min(min_x, x)\n                max_y = max(max_y, y)\n                min_y = min(min_y, y)\n\n            strokes.append(stroke)\n\n        dimensions = Rectangle(Point2D(min_x, min_y), max_x - min_x + 1, max_y - min_y + 1)\n        return HomusSymbol(content, strokes, symbol_name, dimensions)\n\n", "next_line": "    def draw_into_bitmap(self, export_path: ExportPath, stroke_thickness: int, margin: int = 0) -> None:", "id": 12, "__internal_uuid__": "f01697dc-0a66-4e09-a363-ecd73290ffbf"}
{"repo_name": "asdacap/iiumschedule", "file_path": "admincontroller.py", "context": "# Path: bootstrap.py\n\n# \n# Path: models.py\n# class SavedSchedule(DBBase,CMethods):\n#     __tablename__='savedschedules'\n# \n#     token=Column(String(150),primary_key=True)\n#     data=Column(Text)\n#     createddate=Column(DateTime)\n# \n#     def post_process(self):\n#         #add section data\n#         self.data=self.data.replace(unichr(160),\" \")\n#         obj=json.loads(self.data)\n#         for s in obj['coursearray']:\n#             s['lecturer']=''\n# \n#         if(obj['scheduletype']!='MAINCAMPUS'):\n#             #we have no data other than main campus data but we still need to format it\n#             self.data=json.dumps(obj)\n#             return\n# \n#         for s in obj['coursearray']:\n#             sectiondata=SectionData.get_section_data(s['code'],obj['session'],obj['semester'],s['section'])\n#             if(sectiondata==None):\n#                 app.logger.warning('Warning, no section data for %s session %s semester %s section %s'%(s['code'],obj['session'],obj['semester'],s['section']))\n#             else:\n#                 s['lecturer']=sectiondata.lecturer\n# \n#         self.data=json.dumps(obj)\n# \n# class SubjectData(DBBase,CMethods):\n#     __tablename__='subjectdatas'\n#     __table_args__= (\n#         UniqueConstraint('code','session','semester'),\n#     )\n# \n#     id = Column(Integer, primary_key=True)\n#     code=Column(String(20))\n#     title=Column(String(200))\n#     credit=Column(Float)\n#     coursetype=Column(String(200))\n#     kuliyyah=Column(String(200))\n#     session=Column(String(200))\n#     semester=Column(Integer)\n# \n#     @classmethod\n#     def get_subject_data(cls,code,session,semester):\n#         try:\n#             return cls.query.filter(cls.code==code).filter(cls.semester==semester).filter(cls.session==session).one()\n#         except sqlalchemy.orm.exc.NoResultFound:\n#             return None\n# \n# class SectionData(DBBase,CMethods):\n#     __tablename__='sectiondatas'\n#     __table_args__= (\n#         UniqueConstraint('subject_id','sectionno'),\n#     )\n# \n#     id = Column(Integer, primary_key=True)\n#     subject_id=Column(Integer,ForeignKey('subjectdatas.id'))\n#     sectionno = Column(Integer)\n#     lecturer=Column(String(200))\n# \n#     subject=relationship(SubjectData,backref=backref('sections',cascade=\"all, delete-orphan\"))\n# \n#     @classmethod\n#     def get_section_data(cls,code,session,semester,section):\n#         subject=SubjectData.get_subject_data(code,session,semester)\n#         if(subject==None):\n#             return None\n#         try:\n#             return cls.query.filter(cls.subject==subject).filter(cls.sectionno==section).one()\n#         except sqlalchemy.orm.exc.NoResultFound:\n#             return None\n# \n# class SectionScheduleData(DBBase,CMethods):\n#     __tablename__='sectionscheduledatas'\n# \n#     id = Column(Integer, primary_key=True)\n#     section_id=Column(Integer,ForeignKey('sectiondatas.id'))\n#     venue=Column(String(200))\n#     day=Column(String(200))\n#     time=Column(String(200))\n#     lecturer=Column(String(200))\n# \n#     section=relationship(SectionData,backref=backref('schedules',cascade=\"all, delete-orphan\"))\n# \n# class ErrorLog(DBBase,CMethods):\n#     __tablename__='errorlogs'\n# \n#     id = Column(Integer, primary_key=True)\n#     submitter=Column(String(200))\n#     html=Column(Text)\n#     error=Column(Text)\n#     additionalinfo=Column(Text)\n#     created_at=Column(DateTime)\n# \n# class Theme(DBBase,CMethods):\n#     __tablename__='themes'\n# \n#     name=Column(String(250),primary_key=True)\n#     submitter=Column(String(250))\n#     email=Column(String(250))\n#     data=Column(Text)\n#     counter=Column(Integer)\n#     rendered=Column(Text)\n# \n#     def simple_to_hash(self):\n#       return { \"name\":self.name, \"submitter\":self.submitter }\n# \n#     def generate_photo(self):\n#       with tempfile.NamedTemporaryFile(suffix=\".html\") as tfile:\n#         tfile.write(self.rendered.encode('utf8'))\n#         tfile.flush()\n#         try:\n#           subprocess.check_output([\"python\",\"/usr/local/bin/webkit2png\",\"-x\",\"800\",\"600\",os.path.abspath(tfile.name),\"--output=\"+os.path.join(os.path.dirname(__file__), \"static/themeimage/%s.png\"%(self.name)),\"--scale=200\",\"150\"], cwd=os.path.dirname(__file__) )\n#         except subprocess.CalledProcessError as e:\n#           logging.error(\"Exception on Generate Photo\")\n#           logging.error(e.cmd)\n#           logging.error(e.returncode)\n#           logging.error(e.output)\n#           raise e\n\n", "import_statement": "    import JSON\n    import json as JSON\nimport json\nimport logging\nimport sqlalchemy.orm.exc\nfrom functools import wraps\nfrom flask import Flask, render_template, request, g, session, redirect, url_for\nfrom datetime import *\nfrom bootstrap import app,db\nfrom models import SavedSchedule,SubjectData,SectionData,SectionScheduleData,ErrorLog,Theme\nfrom staticsettings import LOGIN_USERNAME,LOGIN_PASSWORD", "code": "            obj.title=data['title']\n            obj.credit=float(data['credit'])\n            obj.kuliyyah=kuly\n            obj.session=session\n            obj.semester=sem\n            obj.put()\n            update=True\n    else:\n        obj=SubjectData()\n        obj.code=code\n        obj.coursetype=stype\n        obj.title=data['title']\n        obj.credit=float(data['credit'])\n        obj.kuliyyah=kuly\n        obj.session=session\n        obj.semester=sem\n        obj.put()\n        insert=True\n\n    sectionupdated=0\n    sectionadded=0\n\n    for section in data['sections']:\n        val=data['sections'][section]\n        try:\n            sdata=SectionData.query.filter(SectionData.subject==obj).filter(SectionData.sectionno==section).one()\n            sdata.lecturer=val['lecturer']\n            sdata.schedules = [] # Remove it\n\n            for sched in val['schedules']:\n", "prompt": "# Path: bootstrap.py\n\n# \n# Path: models.py\n# class SavedSchedule(DBBase,CMethods):\n#     __tablename__='savedschedules'\n# \n#     token=Column(String(150),primary_key=True)\n#     data=Column(Text)\n#     createddate=Column(DateTime)\n# \n#     def post_process(self):\n#         #add section data\n#         self.data=self.data.replace(unichr(160),\" \")\n#         obj=json.loads(self.data)\n#         for s in obj['coursearray']:\n#             s['lecturer']=''\n# \n#         if(obj['scheduletype']!='MAINCAMPUS'):\n#             #we have no data other than main campus data but we still need to format it\n#             self.data=json.dumps(obj)\n#             return\n# \n#         for s in obj['coursearray']:\n#             sectiondata=SectionData.get_section_data(s['code'],obj['session'],obj['semester'],s['section'])\n#             if(sectiondata==None):\n#                 app.logger.warning('Warning, no section data for %s session %s semester %s section %s'%(s['code'],obj['session'],obj['semester'],s['section']))\n#             else:\n#                 s['lecturer']=sectiondata.lecturer\n# \n#         self.data=json.dumps(obj)\n# \n# class SubjectData(DBBase,CMethods):\n#     __tablename__='subjectdatas'\n#     __table_args__= (\n#         UniqueConstraint('code','session','semester'),\n#     )\n# \n#     id = Column(Integer, primary_key=True)\n#     code=Column(String(20))\n#     title=Column(String(200))\n#     credit=Column(Float)\n#     coursetype=Column(String(200))\n#     kuliyyah=Column(String(200))\n#     session=Column(String(200))\n#     semester=Column(Integer)\n# \n#     @classmethod\n#     def get_subject_data(cls,code,session,semester):\n#         try:\n#             return cls.query.filter(cls.code==code).filter(cls.semester==semester).filter(cls.session==session).one()\n#         except sqlalchemy.orm.exc.NoResultFound:\n#             return None\n# \n# class SectionData(DBBase,CMethods):\n#     __tablename__='sectiondatas'\n#     __table_args__= (\n#         UniqueConstraint('subject_id','sectionno'),\n#     )\n# \n#     id = Column(Integer, primary_key=True)\n#     subject_id=Column(Integer,ForeignKey('subjectdatas.id'))\n#     sectionno = Column(Integer)\n#     lecturer=Column(String(200))\n# \n#     subject=relationship(SubjectData,backref=backref('sections',cascade=\"all, delete-orphan\"))\n# \n#     @classmethod\n#     def get_section_data(cls,code,session,semester,section):\n#         subject=SubjectData.get_subject_data(code,session,semester)\n#         if(subject==None):\n#             return None\n#         try:\n#             return cls.query.filter(cls.subject==subject).filter(cls.sectionno==section).one()\n#         except sqlalchemy.orm.exc.NoResultFound:\n#             return None\n# \n# class SectionScheduleData(DBBase,CMethods):\n#     __tablename__='sectionscheduledatas'\n# \n#     id = Column(Integer, primary_key=True)\n#     section_id=Column(Integer,ForeignKey('sectiondatas.id'))\n#     venue=Column(String(200))\n#     day=Column(String(200))\n#     time=Column(String(200))\n#     lecturer=Column(String(200))\n# \n#     section=relationship(SectionData,backref=backref('schedules',cascade=\"all, delete-orphan\"))\n# \n# class ErrorLog(DBBase,CMethods):\n#     __tablename__='errorlogs'\n# \n#     id = Column(Integer, primary_key=True)\n#     submitter=Column(String(200))\n#     html=Column(Text)\n#     error=Column(Text)\n#     additionalinfo=Column(Text)\n#     created_at=Column(DateTime)\n# \n# class Theme(DBBase,CMethods):\n#     __tablename__='themes'\n# \n#     name=Column(String(250),primary_key=True)\n#     submitter=Column(String(250))\n#     email=Column(String(250))\n#     data=Column(Text)\n#     counter=Column(Integer)\n#     rendered=Column(Text)\n# \n#     def simple_to_hash(self):\n#       return { \"name\":self.name, \"submitter\":self.submitter }\n# \n#     def generate_photo(self):\n#       with tempfile.NamedTemporaryFile(suffix=\".html\") as tfile:\n#         tfile.write(self.rendered.encode('utf8'))\n#         tfile.flush()\n#         try:\n#           subprocess.check_output([\"python\",\"/usr/local/bin/webkit2png\",\"-x\",\"800\",\"600\",os.path.abspath(tfile.name),\"--output=\"+os.path.join(os.path.dirname(__file__), \"static/themeimage/%s.png\"%(self.name)),\"--scale=200\",\"150\"], cwd=os.path.dirname(__file__) )\n#         except subprocess.CalledProcessError as e:\n#           logging.error(\"Exception on Generate Photo\")\n#           logging.error(e.cmd)\n#           logging.error(e.returncode)\n#           logging.error(e.output)\n#           raise e\n\n\n# Path: admincontroller.py\n    import JSON\n    import json as JSON\nimport json\nimport logging\nimport sqlalchemy.orm.exc\nfrom functools import wraps\nfrom flask import Flask, render_template, request, g, session, redirect, url_for\nfrom datetime import *\nfrom bootstrap import app,db\nfrom models import SavedSchedule,SubjectData,SectionData,SectionScheduleData,ErrorLog,Theme\nfrom staticsettings import LOGIN_USERNAME,LOGIN_PASSWORD\n\n            obj.title=data['title']\n            obj.credit=float(data['credit'])\n            obj.kuliyyah=kuly\n            obj.session=session\n            obj.semester=sem\n            obj.put()\n            update=True\n    else:\n        obj=SubjectData()\n        obj.code=code\n        obj.coursetype=stype\n        obj.title=data['title']\n        obj.credit=float(data['credit'])\n        obj.kuliyyah=kuly\n        obj.session=session\n        obj.semester=sem\n        obj.put()\n        insert=True\n\n    sectionupdated=0\n    sectionadded=0\n\n    for section in data['sections']:\n        val=data['sections'][section]\n        try:\n            sdata=SectionData.query.filter(SectionData.subject==obj).filter(SectionData.sectionno==section).one()\n            sdata.lecturer=val['lecturer']\n            sdata.schedules = [] # Remove it\n\n            for sched in val['schedules']:\n", "next_line": "              newsched = SectionScheduleData()", "id": 13, "__internal_uuid__": "6a1fbf3b-df41-4fe0-a899-b50fd9acebf6"}
{"repo_name": "mike820324/microProxy", "file_path": "microproxy/protocol/tls.py", "context": "# Path: microproxy/pyca_tls/_constructs.py\n\n# \n# Path: microproxy/utils.py\n# HAS_ALPN = SSL._lib.Cryptography_HAS_ALPN\n# \n# Path: microproxy/exception.py\n# class ProtocolError(Exception):\n#     pass\n# \n# Path: microproxy/log.py\n# class ProxyLogger(object):\n#     formatter = logging.Formatter(\"%(asctime)s - %(name)-30s - %(levelname)-8s - %(message)s\")\n# \n#     @classmethod\n#     def init_proxy_logger(cls, config):\n#         if config[\"logger_config\"]:\n#             # NOTE: If user specify the logging config file,\n#             # used it to configure the logger behavior.\n#             # Moreover, the disable_existing_loggers is necessary,\n#             # since most of our code will get logger before we initialize it.\n#             logging.config.fileConfig(config[\"logger_config\"], disable_existing_loggers=False)\n#         else:\n#             # NOTE: Otherwise, we start setup the logger based on other configure value.\n#             logger = logging.getLogger()\n#             log_level = getattr(logging, config[\"log_level\"].upper())\n#             logger.setLevel(log_level)\n# \n#             if config[\"log_file\"]:\n#                 cls.register_file_handler(config[\"log_file\"])\n#             else:\n#                 cls.register_stream_handler()\n# \n#     @classmethod\n#     def register_zmq_handler(cls, zmq_socket):  # pragma: no cover\n#         handler = PUBHandler(zmq_socket)\n#         handler.root_topic = \"logger\"\n# \n#         logger = logging.getLogger()\n#         logger.addHandler(handler)\n# \n#     @classmethod\n#     def register_file_handler(cls, filename):  # pragma: no cover\n#         fileHandler = logging.FileHandler(filename, encoding=\"utf8\")\n#         fileHandler.setFormatter(cls.formatter)\n# \n#         logger = logging.getLogger()\n#         logger.addHandler(fileHandler)\n# \n#     @classmethod\n#     def register_stream_handler(cls):  # pragma: no cover\n#         basicHandler = logging.StreamHandler()\n#         basicHandler.setFormatter(cls.formatter)\n# \n#         logger = logging.getLogger()\n#         logger.addHandler(basicHandler)\n# \n#     @classmethod\n#     def get_logger(cls, name):  # pragma: no cover\n#         logger = logging.getLogger(name)\n#         return logger\n\n", "import_statement": "from OpenSSL import SSL, crypto\nfrom tornado import gen\nfrom microproxy.pyca_tls import _constructs\nfrom microproxy.utils import HAS_ALPN\nfrom microproxy.exception import ProtocolError\nfrom microproxy.log import ProxyLogger\nimport certifi\nimport construct", "code": ")\n\n\ndef create_basic_sslcontext():\n    ssl_ctx = SSL.Context(SSL.SSLv23_METHOD)\n    ssl_ctx.set_options(SSL.OP_NO_SSLv2 | SSL.OP_NO_SSLv3 | SSL.OP_CIPHER_SERVER_PREFERENCE)\n\n    ssl_ctx.set_cipher_list(\":\".join(_SUPPROT_CIPHERS_SUITES))\n\n    # NOTE: cipher suite related to ECDHE will need this\n    ssl_ctx.set_tmp_ecdh(crypto.get_elliptic_curve('prime256v1'))\n    return ssl_ctx\n\n\ndef certificate_verify_cb(conn, x509, err_num, err_depth, verify_status):\n    return verify_status\n\n\ndef create_dest_sslcontext(insecure=False, trusted_ca_certs=\"\", alpn=None):\n    ssl_ctx = create_basic_sslcontext()\n\n    if not insecure:\n        trusted_ca_certs = trusted_ca_certs or certifi.where()\n        ssl_ctx.load_verify_locations(trusted_ca_certs)\n        ssl_ctx.set_verify(\n            SSL.VERIFY_PEER | SSL.VERIFY_FAIL_IF_NO_PEER_CERT,\n            certificate_verify_cb)\n    else:\n        ssl_ctx.set_verify(SSL.VERIFY_NONE, certificate_verify_cb)\n\n", "prompt": "# Path: microproxy/pyca_tls/_constructs.py\n\n# \n# Path: microproxy/utils.py\n# HAS_ALPN = SSL._lib.Cryptography_HAS_ALPN\n# \n# Path: microproxy/exception.py\n# class ProtocolError(Exception):\n#     pass\n# \n# Path: microproxy/log.py\n# class ProxyLogger(object):\n#     formatter = logging.Formatter(\"%(asctime)s - %(name)-30s - %(levelname)-8s - %(message)s\")\n# \n#     @classmethod\n#     def init_proxy_logger(cls, config):\n#         if config[\"logger_config\"]:\n#             # NOTE: If user specify the logging config file,\n#             # used it to configure the logger behavior.\n#             # Moreover, the disable_existing_loggers is necessary,\n#             # since most of our code will get logger before we initialize it.\n#             logging.config.fileConfig(config[\"logger_config\"], disable_existing_loggers=False)\n#         else:\n#             # NOTE: Otherwise, we start setup the logger based on other configure value.\n#             logger = logging.getLogger()\n#             log_level = getattr(logging, config[\"log_level\"].upper())\n#             logger.setLevel(log_level)\n# \n#             if config[\"log_file\"]:\n#                 cls.register_file_handler(config[\"log_file\"])\n#             else:\n#                 cls.register_stream_handler()\n# \n#     @classmethod\n#     def register_zmq_handler(cls, zmq_socket):  # pragma: no cover\n#         handler = PUBHandler(zmq_socket)\n#         handler.root_topic = \"logger\"\n# \n#         logger = logging.getLogger()\n#         logger.addHandler(handler)\n# \n#     @classmethod\n#     def register_file_handler(cls, filename):  # pragma: no cover\n#         fileHandler = logging.FileHandler(filename, encoding=\"utf8\")\n#         fileHandler.setFormatter(cls.formatter)\n# \n#         logger = logging.getLogger()\n#         logger.addHandler(fileHandler)\n# \n#     @classmethod\n#     def register_stream_handler(cls):  # pragma: no cover\n#         basicHandler = logging.StreamHandler()\n#         basicHandler.setFormatter(cls.formatter)\n# \n#         logger = logging.getLogger()\n#         logger.addHandler(basicHandler)\n# \n#     @classmethod\n#     def get_logger(cls, name):  # pragma: no cover\n#         logger = logging.getLogger(name)\n#         return logger\n\n\n# Path: microproxy/protocol/tls.py\nfrom OpenSSL import SSL, crypto\nfrom tornado import gen\nfrom microproxy.pyca_tls import _constructs\nfrom microproxy.utils import HAS_ALPN\nfrom microproxy.exception import ProtocolError\nfrom microproxy.log import ProxyLogger\nimport certifi\nimport construct\n\n)\n\n\ndef create_basic_sslcontext():\n    ssl_ctx = SSL.Context(SSL.SSLv23_METHOD)\n    ssl_ctx.set_options(SSL.OP_NO_SSLv2 | SSL.OP_NO_SSLv3 | SSL.OP_CIPHER_SERVER_PREFERENCE)\n\n    ssl_ctx.set_cipher_list(\":\".join(_SUPPROT_CIPHERS_SUITES))\n\n    # NOTE: cipher suite related to ECDHE will need this\n    ssl_ctx.set_tmp_ecdh(crypto.get_elliptic_curve('prime256v1'))\n    return ssl_ctx\n\n\ndef certificate_verify_cb(conn, x509, err_num, err_depth, verify_status):\n    return verify_status\n\n\ndef create_dest_sslcontext(insecure=False, trusted_ca_certs=\"\", alpn=None):\n    ssl_ctx = create_basic_sslcontext()\n\n    if not insecure:\n        trusted_ca_certs = trusted_ca_certs or certifi.where()\n        ssl_ctx.load_verify_locations(trusted_ca_certs)\n        ssl_ctx.set_verify(\n            SSL.VERIFY_PEER | SSL.VERIFY_FAIL_IF_NO_PEER_CERT,\n            certificate_verify_cb)\n    else:\n        ssl_ctx.set_verify(SSL.VERIFY_NONE, certificate_verify_cb)\n\n", "next_line": "    if alpn and HAS_ALPN:", "id": 14, "__internal_uuid__": "168a3122-733f-40e8-8851-5dacbce00142"}
{"repo_name": "hootnot/oanda-api-v20", "file_path": "oandapyV20/contrib/requests/extensions.py", "context": "# Path: oandapyV20/contrib/requests/baserequest.py\n# class BaseRequest(object):\n#     \"\"\"baseclass for request classes.\"\"\"\n# \n#     @abstractmethod\n#     def __init__(self):\n#         self._data = dict()\n# \n#     def __repr__(self):\n#         return json.dumps(self.__dict__)\n# \n#     @property\n#     def data(self):\n#         \"\"\"data - return the JSON body.\n# \n#         The data property returns a dict representing the\n#         JSON-body needed for the API-request. All values that are\n#         not set will be left out\n#         \"\"\"\n#         d = dict()\n#         for k, v in self._data.items():\n#             # skip unset properties\n#             if v is None:\n#                 continue\n# \n#             d.update({k: v})\n# \n#         return d\n# \n#     def toJSON(self):\n#         return json.dumps(self,\n#                           default=lambda o: o.__dict__,\n#                           sort_keys=True,\n#                           indent=4)\n# \n# Path: oandapyV20/types/types.py\n# class ClientID(OAType):\n#     \"\"\"representation of ClientID, a string value of max 128 chars.\"\"\"\n# \n#     def __init__(self, clientID):\n#         length = len(clientID)\n#         if not length or length > 128:\n#             raise ValueError(\"ClientID: length {}\".format(length))\n# \n#         self._v = clientID\n# \n# class ClientTag(OAType):\n#     \"\"\"representation of ClientTag, a string value of max 128 chars.\"\"\"\n# \n#     def __init__(self, clientTag):\n#         length = len(clientTag)\n#         if not length or length > 128:\n#             raise ValueError(\"ClientTag: length {}\".format(length))\n# \n#         self._v = clientTag\n# \n# class ClientComment(OAType):\n#     \"\"\"representation of ClientComment, a string value of max 128 chars.\"\"\"\n# \n#     def __init__(self, clientComment):\n#         length = len(clientComment)\n#         if not length or length > 128:\n#             raise ValueError(\"ClientComment: length {}\".format(length))\n# \n#         self._v = clientComment\n\n", "import_statement": "from .baserequest import BaseRequest\nfrom oandapyV20.types import ClientID, ClientTag, ClientComment", "code": "            >>> # add clientExtensions to it also\n            >>> takeProfitOnFillOrder = TakeProfitDetails(\n            ...     price=1.10,\n            ...     clientExtensions=ClientExtensions(clientTag=\"mytag\").data)\n            >>> print(takeProfitOnFillOrder.data)\n            {\n                'timeInForce': 'GTC',\n                'price\": '1.10000',\n                'clientExtensions': {'tag': 'mytag'}\n            }\n            >>> ordr = MarketOrderRequest(\n            ...     instrument=\"EUR_USD\",\n            ...     units=10000,\n            ...     takeProfitOnFill=takeProfitOnFillOrder.data\n            ... )\n            >>> # or as shortcut ...\n            >>> #   takeProfitOnFill=TakeProfitDetails(price=1.10).data\n            >>> print(json.dumps(ordr.data, indent=4))\n            >>> r = orders.OrderCreate(accountID, data=ordr.data)\n            >>> rv = client.request(r)\n            >>> ...\n        \"\"\"\n        super(ClientExtensions, self).__init__()\n        if not (clientID or clientTag or clientComment):\n            raise ValueError(\"clientID, clientTag, clientComment required\")\n\n        if clientID:\n            self._data.update({\"id\": ClientID(clientID).value})\n\n        if clientTag:\n", "prompt": "# Path: oandapyV20/contrib/requests/baserequest.py\n# class BaseRequest(object):\n#     \"\"\"baseclass for request classes.\"\"\"\n# \n#     @abstractmethod\n#     def __init__(self):\n#         self._data = dict()\n# \n#     def __repr__(self):\n#         return json.dumps(self.__dict__)\n# \n#     @property\n#     def data(self):\n#         \"\"\"data - return the JSON body.\n# \n#         The data property returns a dict representing the\n#         JSON-body needed for the API-request. All values that are\n#         not set will be left out\n#         \"\"\"\n#         d = dict()\n#         for k, v in self._data.items():\n#             # skip unset properties\n#             if v is None:\n#                 continue\n# \n#             d.update({k: v})\n# \n#         return d\n# \n#     def toJSON(self):\n#         return json.dumps(self,\n#                           default=lambda o: o.__dict__,\n#                           sort_keys=True,\n#                           indent=4)\n# \n# Path: oandapyV20/types/types.py\n# class ClientID(OAType):\n#     \"\"\"representation of ClientID, a string value of max 128 chars.\"\"\"\n# \n#     def __init__(self, clientID):\n#         length = len(clientID)\n#         if not length or length > 128:\n#             raise ValueError(\"ClientID: length {}\".format(length))\n# \n#         self._v = clientID\n# \n# class ClientTag(OAType):\n#     \"\"\"representation of ClientTag, a string value of max 128 chars.\"\"\"\n# \n#     def __init__(self, clientTag):\n#         length = len(clientTag)\n#         if not length or length > 128:\n#             raise ValueError(\"ClientTag: length {}\".format(length))\n# \n#         self._v = clientTag\n# \n# class ClientComment(OAType):\n#     \"\"\"representation of ClientComment, a string value of max 128 chars.\"\"\"\n# \n#     def __init__(self, clientComment):\n#         length = len(clientComment)\n#         if not length or length > 128:\n#             raise ValueError(\"ClientComment: length {}\".format(length))\n# \n#         self._v = clientComment\n\n\n# Path: oandapyV20/contrib/requests/extensions.py\nfrom .baserequest import BaseRequest\nfrom oandapyV20.types import ClientID, ClientTag, ClientComment\n\n            >>> # add clientExtensions to it also\n            >>> takeProfitOnFillOrder = TakeProfitDetails(\n            ...     price=1.10,\n            ...     clientExtensions=ClientExtensions(clientTag=\"mytag\").data)\n            >>> print(takeProfitOnFillOrder.data)\n            {\n                'timeInForce': 'GTC',\n                'price\": '1.10000',\n                'clientExtensions': {'tag': 'mytag'}\n            }\n            >>> ordr = MarketOrderRequest(\n            ...     instrument=\"EUR_USD\",\n            ...     units=10000,\n            ...     takeProfitOnFill=takeProfitOnFillOrder.data\n            ... )\n            >>> # or as shortcut ...\n            >>> #   takeProfitOnFill=TakeProfitDetails(price=1.10).data\n            >>> print(json.dumps(ordr.data, indent=4))\n            >>> r = orders.OrderCreate(accountID, data=ordr.data)\n            >>> rv = client.request(r)\n            >>> ...\n        \"\"\"\n        super(ClientExtensions, self).__init__()\n        if not (clientID or clientTag or clientComment):\n            raise ValueError(\"clientID, clientTag, clientComment required\")\n\n        if clientID:\n            self._data.update({\"id\": ClientID(clientID).value})\n\n        if clientTag:\n", "next_line": "            self._data.update({\"tag\": ClientTag(clientTag).value})", "id": 15, "__internal_uuid__": "9a846ca0-e90f-476a-88c4-8cd62b71e472"}
{"repo_name": "EeOneDown/spbu4u", "file_path": "app/new_functions.py", "context": "# Path: app/constants.py\n\n# \n# Path: config.py\n# class Config(object):\n#     SECRET_KEY = os.getenv('SECRET_KEY')\n#     SQLALCHEMY_DATABASE_URI = os.getenv('DATABASE_URL')\n#     SQLALCHEMY_TRACK_MODIFICATIONS = False\n#     TELEGRAM_BOT_RELEASE_TOKEN = os.getenv('TELEGRAM_BOT_RELEASE_TOKEN')\n#     OTHER_SECRET_KEY = os.getenv('OTHER_SECRET_KEY')\n#     YANDEX_API_KEY = os.getenv('YANDEX_API_KEY')\n#     BOT_NAME = os.getenv('BOT_NAME')\n#     IS_THREADED_BOT = True if os.getenv('IS_THREADED_BOT') == \"True\" else False\n\n", "import_statement": "import hashlib\nimport json\nimport logging\nimport re\nimport requests\nfrom datetime import datetime, timedelta, date\nfrom telebot.apihelper import ApiException\nfrom telebot.types import Message\nfrom app.constants import (\n    emoji, subject_short_types, week_day_number, months,\n    reg_before_30, reg_only_30, reg_only_31, interval_off_answer, urls,\n    yandex_error_answer, yandex_segment_answer, all_stations,\n    ask_to_select_types_answer, updated_types_answer\n)\nfrom config import Config", "code": "\n    return yandex_segment_answer.format(\n        time_mark=time_mark,\n        lef_time=lef_time,\n        train_mark=train_mark,\n        dep_time=departure_datetime.time().strftime(\"%H:%M\"),\n        arr_time=arrival_datetime.time().strftime(\"%H:%M\"),\n        price=price,\n        ruble_sign=emoji[\"ruble_sign\"]\n    )\n\n\ndef create_suburbans_answer(from_code, to_code, for_date, limit=3):\n    \"\"\"\n    Creates yandex suburbans answer for date by stations codes\n\n    :param from_code: `from` yandex station code\n    :type from_code: str\n    :param to_code: `to` yandex station code\n    :type to_code: str\n    :param for_date: date for which data should be received\n    :type for_date: date\n    :param limit: limit of segments in answer\n    :type limit: int\n    :return: tuple with `answer`, `is_tomorrow` and `is_error` data\n    :rtype: tuple\n    \"\"\"\n    code, data = get_yandex_raw_data(from_code, to_code, for_date)\n\n    if code != 200:\n", "prompt": "# Path: app/constants.py\n\n# \n# Path: config.py\n# class Config(object):\n#     SECRET_KEY = os.getenv('SECRET_KEY')\n#     SQLALCHEMY_DATABASE_URI = os.getenv('DATABASE_URL')\n#     SQLALCHEMY_TRACK_MODIFICATIONS = False\n#     TELEGRAM_BOT_RELEASE_TOKEN = os.getenv('TELEGRAM_BOT_RELEASE_TOKEN')\n#     OTHER_SECRET_KEY = os.getenv('OTHER_SECRET_KEY')\n#     YANDEX_API_KEY = os.getenv('YANDEX_API_KEY')\n#     BOT_NAME = os.getenv('BOT_NAME')\n#     IS_THREADED_BOT = True if os.getenv('IS_THREADED_BOT') == \"True\" else False\n\n\n# Path: app/new_functions.py\nimport hashlib\nimport json\nimport logging\nimport re\nimport requests\nfrom datetime import datetime, timedelta, date\nfrom telebot.apihelper import ApiException\nfrom telebot.types import Message\nfrom app.constants import (\n    emoji, subject_short_types, week_day_number, months,\n    reg_before_30, reg_only_30, reg_only_31, interval_off_answer, urls,\n    yandex_error_answer, yandex_segment_answer, all_stations,\n    ask_to_select_types_answer, updated_types_answer\n)\nfrom config import Config\n\n\n    return yandex_segment_answer.format(\n        time_mark=time_mark,\n        lef_time=lef_time,\n        train_mark=train_mark,\n        dep_time=departure_datetime.time().strftime(\"%H:%M\"),\n        arr_time=arrival_datetime.time().strftime(\"%H:%M\"),\n        price=price,\n        ruble_sign=emoji[\"ruble_sign\"]\n    )\n\n\ndef create_suburbans_answer(from_code, to_code, for_date, limit=3):\n    \"\"\"\n    Creates yandex suburbans answer for date by stations codes\n\n    :param from_code: `from` yandex station code\n    :type from_code: str\n    :param to_code: `to` yandex station code\n    :type to_code: str\n    :param for_date: date for which data should be received\n    :type for_date: date\n    :param limit: limit of segments in answer\n    :type limit: int\n    :return: tuple with `answer`, `is_tomorrow` and `is_error` data\n    :rtype: tuple\n    \"\"\"\n    code, data = get_yandex_raw_data(from_code, to_code, for_date)\n\n    if code != 200:\n", "next_line": "        return yandex_error_answer, False, True", "id": 16, "__internal_uuid__": "2f384c78-11c6-4be6-baf0-f2af88f50cca"}
{"repo_name": "chaicko/AlgorithmicToolbox", "file_path": "test/test_introduction_problems.py", "context": "# Path: introduction_problems/fib.py\n# def calc_fib(n):\n#     if n < 1:\n#         return n\n#     a = [1, 0]\n#     for i in range(2, n):\n#         aux = a[0] + a[1]\n#         a[1] = a[0]\n#         a[0] = aux\n#     return a[0] + a[1]\n# \n# Path: introduction_problems/fibonacci_last_digit.py\n# def get_fibonacci_last_digit(n):\n#     if n < 1:\n#         return n\n#     a, b = 1, 1\n#     for i in range(2, n):\n#         a, b = b, (a + b) % 10\n#     return b\n# \n# FIBONACCI_LAST_DIGIT_MAX_VALUE = 10 ** 7\n# \n# Path: introduction_problems/gcd.py\n# def gcd(a, b):\n#     if b == 0:\n#         return a\n#     return gcd(b, a % b)\n# \n# Path: introduction_problems/lcm.py\n# def lcm(a, b):\n#     d = gcd(a, b)\n#     return (a * b) // d\n# \n# Path: introduction_problems/fibonacci_huge.py\n# def get_fibonaccihuge(n, m):\n#     seq = []\n#     for i, f in enumerate(fibseq(m)):\n#         seq.append(f)  # append fibonacci mod m to a list\n# \n#         # search for the [0, 1] period boundary (is unique)\n#         if len(seq) > 3 and seq[:2] == seq[len(seq) - 2:]:\n#             # if [0, 1] is found, remove top two elements of the\n#             # sequence because those are not part of the period\n#             # i.e. for m == 2, seq = [0, 1, 1, 0, 1] at this point\n#             del seq[-2:]\n#             break\n# \n#         # quit the loop if the current sequence is longer or\n#         # equal to the input n because we compute the remainder\n#         # of this number divided by the sequence length, and if\n#         # n is less than the length of the sequence then the number\n#         # is just the required remainder\n#         if i >= n:\n#             break\n# \n#     seq_len = len(seq)\n#     rem = n % seq_len\n#     return seq[rem]\n# \n# def fib_seq_mod(m):\n#     \"\"\"Returns the fibonacci modulo 'm' periodic sequence.\n# \n#     This is quite slow actually and needs to be enhanced.\n#     :param m: modulo\n#     :return: sequence of numbers that form the Pisano period (modulo m)\n#     \"\"\"\n#     seq = []\n#     for f in fibseq(m):\n#         seq.append(f)  # append fibonacci mod m to a list\n#         if len(seq) > 3 and seq[:2] == seq[len(seq) - 2:]:\n#             return seq[:len(seq) - 2]\n# \n# FIBO_HUGE_MAX_MOD = 10 ** 5\n# \n# FIBO_HUGE_MAX_NUM = 10 ** 18\n\n", "import_statement": "from unittest import TestCase\nfrom introduction_problems.fib import calc_fib\nfrom introduction_problems.fibonacci_last_digit import \\\n    get_fibonacci_last_digit, FIBONACCI_LAST_DIGIT_MAX_VALUE\nfrom introduction_problems.gcd import gcd\nfrom introduction_problems.lcm import lcm\nfrom introduction_problems.fibonacci_huge import get_fibonaccihuge, \\\n    fib_seq_mod, FIBO_HUGE_MAX_MOD, FIBO_HUGE_MAX_NUM", "code": "\n\ndef fib(n):\n    a, b = 0, 1\n    while a <= n:  # First iteration:\n        yield a  # yield 0 to start with and then\n        a, b = b, a + b  # a will now be 1, and b will also be 1, (0 + 1)\n\n\nclass TestSmallFibonacci(TestCase):\n    def test_fib_0_is_0(self):\n", "prompt": "# Path: introduction_problems/fib.py\n# def calc_fib(n):\n#     if n < 1:\n#         return n\n#     a = [1, 0]\n#     for i in range(2, n):\n#         aux = a[0] + a[1]\n#         a[1] = a[0]\n#         a[0] = aux\n#     return a[0] + a[1]\n# \n# Path: introduction_problems/fibonacci_last_digit.py\n# def get_fibonacci_last_digit(n):\n#     if n < 1:\n#         return n\n#     a, b = 1, 1\n#     for i in range(2, n):\n#         a, b = b, (a + b) % 10\n#     return b\n# \n# FIBONACCI_LAST_DIGIT_MAX_VALUE = 10 ** 7\n# \n# Path: introduction_problems/gcd.py\n# def gcd(a, b):\n#     if b == 0:\n#         return a\n#     return gcd(b, a % b)\n# \n# Path: introduction_problems/lcm.py\n# def lcm(a, b):\n#     d = gcd(a, b)\n#     return (a * b) // d\n# \n# Path: introduction_problems/fibonacci_huge.py\n# def get_fibonaccihuge(n, m):\n#     seq = []\n#     for i, f in enumerate(fibseq(m)):\n#         seq.append(f)  # append fibonacci mod m to a list\n# \n#         # search for the [0, 1] period boundary (is unique)\n#         if len(seq) > 3 and seq[:2] == seq[len(seq) - 2:]:\n#             # if [0, 1] is found, remove top two elements of the\n#             # sequence because those are not part of the period\n#             # i.e. for m == 2, seq = [0, 1, 1, 0, 1] at this point\n#             del seq[-2:]\n#             break\n# \n#         # quit the loop if the current sequence is longer or\n#         # equal to the input n because we compute the remainder\n#         # of this number divided by the sequence length, and if\n#         # n is less than the length of the sequence then the number\n#         # is just the required remainder\n#         if i >= n:\n#             break\n# \n#     seq_len = len(seq)\n#     rem = n % seq_len\n#     return seq[rem]\n# \n# def fib_seq_mod(m):\n#     \"\"\"Returns the fibonacci modulo 'm' periodic sequence.\n# \n#     This is quite slow actually and needs to be enhanced.\n#     :param m: modulo\n#     :return: sequence of numbers that form the Pisano period (modulo m)\n#     \"\"\"\n#     seq = []\n#     for f in fibseq(m):\n#         seq.append(f)  # append fibonacci mod m to a list\n#         if len(seq) > 3 and seq[:2] == seq[len(seq) - 2:]:\n#             return seq[:len(seq) - 2]\n# \n# FIBO_HUGE_MAX_MOD = 10 ** 5\n# \n# FIBO_HUGE_MAX_NUM = 10 ** 18\n\n\n# Path: test/test_introduction_problems.py\nfrom unittest import TestCase\nfrom introduction_problems.fib import calc_fib\nfrom introduction_problems.fibonacci_last_digit import \\\n    get_fibonacci_last_digit, FIBONACCI_LAST_DIGIT_MAX_VALUE\nfrom introduction_problems.gcd import gcd\nfrom introduction_problems.lcm import lcm\nfrom introduction_problems.fibonacci_huge import get_fibonaccihuge, \\\n    fib_seq_mod, FIBO_HUGE_MAX_MOD, FIBO_HUGE_MAX_NUM\n\n\n\ndef fib(n):\n    a, b = 0, 1\n    while a <= n:  # First iteration:\n        yield a  # yield 0 to start with and then\n        a, b = b, a + b  # a will now be 1, and b will also be 1, (0 + 1)\n\n\nclass TestSmallFibonacci(TestCase):\n    def test_fib_0_is_0(self):\n", "next_line": "        assert calc_fib(0) == 0", "id": 17, "__internal_uuid__": "402d78fa-b342-4d93-9bfc-ba89946a83c8"}
{"repo_name": "mradway/hycohanz", "file_path": "hycohanz/modeler3d.py", "context": "# Path: hycohanz/expression.py\n# class Expression(object):\n#     \"\"\"\n#     An HFSS expression.\n#     \n#     This object enables manipulation of HFSS expressions using Python \n#     arithmetic operators, which is much more convenient than manipulating \n#     their string representation.\n#     \n#     Parameters\n#     ----------\n#     expr : str\n#         Initialize the expression using its string representation.\n#     \n#     Attributes\n#     ----------\n#     expr : str\n#         The string representation of the expression object.\n#     \n#     Raises\n#     ------\n#     NotImplementedError\n#         For operations involving floor division (Python 2 '/' or \n#         Python 3 '//') \n#     \n#     \"\"\"\n#     def __init__(self, expr):\n#         if isinstance(expr, Expression):\n#             self.expr = expr.expr\n#         else:\n#             self.expr = str(expr)\n#         \n#     def __add__(self, y):\n#         \"\"\"\n#         Overloads the addition (+) operator.\n#         \"\"\"\n#         if isinstance(y, Expression):\n#             return Expression('(' + self.expr + ') + ' + str(y.expr))\n#         else:\n#             return Expression('(' + self.expr + ') + ' + str(y))\n#         \n#     def __sub__(self, y):\n#         \"\"\"\n#         Overloads the subtraction (-) operator.\n#         \"\"\"\n#         if isinstance(y, Expression):\n#             return Expression('(' + self.expr + ') - ' + str(y.expr))\n#         else:\n#             return Expression('(' + self.expr + ') - ' + str(y))\n#         \n#     def __mul__(self, y):\n#         \"\"\"\n#         Overloads the multiplication (*) operator.\n#         \"\"\"\n#         if isinstance(y, Expression):\n#             return Expression('(' + self.expr + ') * ' + str(y.expr))\n#         else:\n#             return Expression('(' + self.expr + ') * ' + str(y))\n#         \n#     def __truediv__(self, y):\n#         \"\"\"\n#         Overloads the Python 3 division (/) operator.\n#         \"\"\"\n#         if isinstance(y, Expression):\n#             return Expression('(' + self.expr + ') / ' + str(y.expr))\n#         else:\n#             return Expression('(' + self.expr + ') / ' + str(y))\n#             \n#     def __div__(self, y):\n#         \"\"\"\n#         Overloads the Python 3 floor division (//) operator.\n#         \"\"\"\n#         raise NotImplementedError(\"\"\"\"Classic\" division is not implemented by \n# design.  Please use from __future__ import division in the calling code.\"\"\")\n#         \n#     def __neg__(self):\n#         \"\"\"\n#         Overloads the negation (-) operator.\n#         \"\"\"\n#         return Expression('-(' + self.expr + ')')\n\n", "import_statement": "import warnings\nfrom hycohanz.expression import Expression as Ex", "code": "    WhichAxis : str\n        The axis normal to the circle.  Can be 'X', 'Y', or 'Z'.\n    Name : str\n        The requested name of the object.  If this is not available, HFSS \n        will assign a different name, which is returned by this function.\n    Flags : str\n        Flags associated with this object.  See HFSS Scripting Guide for details.\n    Color : tuple of length=3\n        RGB components of the circle\n    Transparency : float between 0 and 1\n        Fractional transparency.  0 is opaque and 1 is transparent.\n    PartCoordinateSystem : str\n        The name of the coordinate system in which the object is drawn.\n    MaterialName : str\n        Name of the material to assign to the object.  Name must be surrounded \n        by double quotes.\n    SolveInside : bool\n        Whether to mesh the interior of the object and solve for the fields \n        inside.\n    IsCovered : bool\n        Whether the rectangle is has a surface or has only edges.\n        \n    Returns\n    -------\n    str\n        The actual name of the created object.\n        \n    \"\"\"\n    RectangleParameters = [ \"NAME:RectangleParameters\",\n                            \"IsCovered:=\", IsCovered,\n", "prompt": "# Path: hycohanz/expression.py\n# class Expression(object):\n#     \"\"\"\n#     An HFSS expression.\n#     \n#     This object enables manipulation of HFSS expressions using Python \n#     arithmetic operators, which is much more convenient than manipulating \n#     their string representation.\n#     \n#     Parameters\n#     ----------\n#     expr : str\n#         Initialize the expression using its string representation.\n#     \n#     Attributes\n#     ----------\n#     expr : str\n#         The string representation of the expression object.\n#     \n#     Raises\n#     ------\n#     NotImplementedError\n#         For operations involving floor division (Python 2 '/' or \n#         Python 3 '//') \n#     \n#     \"\"\"\n#     def __init__(self, expr):\n#         if isinstance(expr, Expression):\n#             self.expr = expr.expr\n#         else:\n#             self.expr = str(expr)\n#         \n#     def __add__(self, y):\n#         \"\"\"\n#         Overloads the addition (+) operator.\n#         \"\"\"\n#         if isinstance(y, Expression):\n#             return Expression('(' + self.expr + ') + ' + str(y.expr))\n#         else:\n#             return Expression('(' + self.expr + ') + ' + str(y))\n#         \n#     def __sub__(self, y):\n#         \"\"\"\n#         Overloads the subtraction (-) operator.\n#         \"\"\"\n#         if isinstance(y, Expression):\n#             return Expression('(' + self.expr + ') - ' + str(y.expr))\n#         else:\n#             return Expression('(' + self.expr + ') - ' + str(y))\n#         \n#     def __mul__(self, y):\n#         \"\"\"\n#         Overloads the multiplication (*) operator.\n#         \"\"\"\n#         if isinstance(y, Expression):\n#             return Expression('(' + self.expr + ') * ' + str(y.expr))\n#         else:\n#             return Expression('(' + self.expr + ') * ' + str(y))\n#         \n#     def __truediv__(self, y):\n#         \"\"\"\n#         Overloads the Python 3 division (/) operator.\n#         \"\"\"\n#         if isinstance(y, Expression):\n#             return Expression('(' + self.expr + ') / ' + str(y.expr))\n#         else:\n#             return Expression('(' + self.expr + ') / ' + str(y))\n#             \n#     def __div__(self, y):\n#         \"\"\"\n#         Overloads the Python 3 floor division (//) operator.\n#         \"\"\"\n#         raise NotImplementedError(\"\"\"\"Classic\" division is not implemented by \n# design.  Please use from __future__ import division in the calling code.\"\"\")\n#         \n#     def __neg__(self):\n#         \"\"\"\n#         Overloads the negation (-) operator.\n#         \"\"\"\n#         return Expression('-(' + self.expr + ')')\n\n\n# Path: hycohanz/modeler3d.py\nimport warnings\nfrom hycohanz.expression import Expression as Ex\n\n    WhichAxis : str\n        The axis normal to the circle.  Can be 'X', 'Y', or 'Z'.\n    Name : str\n        The requested name of the object.  If this is not available, HFSS \n        will assign a different name, which is returned by this function.\n    Flags : str\n        Flags associated with this object.  See HFSS Scripting Guide for details.\n    Color : tuple of length=3\n        RGB components of the circle\n    Transparency : float between 0 and 1\n        Fractional transparency.  0 is opaque and 1 is transparent.\n    PartCoordinateSystem : str\n        The name of the coordinate system in which the object is drawn.\n    MaterialName : str\n        Name of the material to assign to the object.  Name must be surrounded \n        by double quotes.\n    SolveInside : bool\n        Whether to mesh the interior of the object and solve for the fields \n        inside.\n    IsCovered : bool\n        Whether the rectangle is has a surface or has only edges.\n        \n    Returns\n    -------\n    str\n        The actual name of the created object.\n        \n    \"\"\"\n    RectangleParameters = [ \"NAME:RectangleParameters\",\n                            \"IsCovered:=\", IsCovered,\n", "next_line": "                            \"XStart:=\", Ex(xs).expr,", "id": 18, "__internal_uuid__": "8af19a5c-0d42-4f0d-b786-0d7fdcdc09e2"}
{"repo_name": "getninjas/adwords-client", "file_path": "adwords_client/adwords_api/operations/campaign_extensions_setting.py", "context": "# Path: adwords_client/adwords_api/operations/utils.py\n# def _build_new_bidding_strategy_configuration(with_bids=True, strategy_type=None):\n#     bidding_strategy = {'xsi_type': 'BiddingStrategyConfiguration'}\n#     if with_bids:\n#         bidding_strategy['bids'] = []\n#     if strategy_type:\n#         bidding_strategy['biddingStrategyType'] = strategy_type\n#     return bidding_strategy\n# \n# def _build_money(money):\n#     return {\n#         'xsi_type': 'Money',\n#         'microAmount': money,\n#     }\n# \n# def _get_selector(fields, predicates:'list'=None, ordering=None):\n#     predicates = predicates or []\n# \n#     selector = {\n#         'xsi_type': 'Selector',\n#         'paging': {\n#             'xsi_type': 'Paging',\n#             'startIndex': 0,\n#             # maximum number of results allowed by API\n#             # https://developers.google.com/adwords/api/docs/appendix/limits#general\n#             'numberResults': 10000,\n#         },\n#         'fields': list(fields),\n#         'predicates': [],\n#         'ordering': [],\n#     }\n# \n#     for field, operator, values in predicates:\n#         if isinstance(values, str) or isinstance(values, int) or isinstance(values, float):\n#             values = [values]\n#         predicate = {\n#             'xsi_type': 'Predicate',\n#             'field': field,\n#             'operator': operator,\n#             'values': values,\n#         }\n#         selector['predicates'].append(predicate)\n# \n#     return selector\n\n", "import_statement": "from .utils import _build_new_bidding_strategy_configuration, _build_money, _get_selector", "code": "            'campaignId': campaign_id,\n            'extensionType': 'STRUCTURED_SNIPPET',\n            'extensionSetting': {\n                'extensions': []\n            }\n        },\n    }\n    for snippet in snippets_configuration:\n        operation['operand']['extensionSetting']['extensions'].append({\n            'xsi_type': 'StructuredSnippetFeedItem',\n            'header': snippet['header'],\n            'values': snippet['values'],\n        })\n\n    return operation\n\n\ndef get_campaign_extension_operation(fields: 'list'=None, predicates: 'list'=None, **kwargs):\n    default_fields = kwargs.pop('default_fields', False)\n    object_type = kwargs.pop('object_type', None)\n    predicates = predicates or []\n    fields = set(fields or [])\n    if default_fields:\n        fields.update({'CampaignId', 'Extensions'})\n    if object_type == 'campaign_sitelink':\n        predicates.append(('ExtensionType', 'EQUALS', 'SITELINK'))\n    if object_type == 'campaign_callout':\n        predicates.append(('ExtensionType', 'EQUALS', 'CALLOUT'))\n    if object_type == 'campaign_structured_snippet':\n        predicates.append(('ExtensionType', 'EQUALS', 'STRUCTURED_SNIPPET'))\n", "prompt": "# Path: adwords_client/adwords_api/operations/utils.py\n# def _build_new_bidding_strategy_configuration(with_bids=True, strategy_type=None):\n#     bidding_strategy = {'xsi_type': 'BiddingStrategyConfiguration'}\n#     if with_bids:\n#         bidding_strategy['bids'] = []\n#     if strategy_type:\n#         bidding_strategy['biddingStrategyType'] = strategy_type\n#     return bidding_strategy\n# \n# def _build_money(money):\n#     return {\n#         'xsi_type': 'Money',\n#         'microAmount': money,\n#     }\n# \n# def _get_selector(fields, predicates:'list'=None, ordering=None):\n#     predicates = predicates or []\n# \n#     selector = {\n#         'xsi_type': 'Selector',\n#         'paging': {\n#             'xsi_type': 'Paging',\n#             'startIndex': 0,\n#             # maximum number of results allowed by API\n#             # https://developers.google.com/adwords/api/docs/appendix/limits#general\n#             'numberResults': 10000,\n#         },\n#         'fields': list(fields),\n#         'predicates': [],\n#         'ordering': [],\n#     }\n# \n#     for field, operator, values in predicates:\n#         if isinstance(values, str) or isinstance(values, int) or isinstance(values, float):\n#             values = [values]\n#         predicate = {\n#             'xsi_type': 'Predicate',\n#             'field': field,\n#             'operator': operator,\n#             'values': values,\n#         }\n#         selector['predicates'].append(predicate)\n# \n#     return selector\n\n\n# Path: adwords_client/adwords_api/operations/campaign_extensions_setting.py\nfrom .utils import _build_new_bidding_strategy_configuration, _build_money, _get_selector\n\n            'campaignId': campaign_id,\n            'extensionType': 'STRUCTURED_SNIPPET',\n            'extensionSetting': {\n                'extensions': []\n            }\n        },\n    }\n    for snippet in snippets_configuration:\n        operation['operand']['extensionSetting']['extensions'].append({\n            'xsi_type': 'StructuredSnippetFeedItem',\n            'header': snippet['header'],\n            'values': snippet['values'],\n        })\n\n    return operation\n\n\ndef get_campaign_extension_operation(fields: 'list'=None, predicates: 'list'=None, **kwargs):\n    default_fields = kwargs.pop('default_fields', False)\n    object_type = kwargs.pop('object_type', None)\n    predicates = predicates or []\n    fields = set(fields or [])\n    if default_fields:\n        fields.update({'CampaignId', 'Extensions'})\n    if object_type == 'campaign_sitelink':\n        predicates.append(('ExtensionType', 'EQUALS', 'SITELINK'))\n    if object_type == 'campaign_callout':\n        predicates.append(('ExtensionType', 'EQUALS', 'CALLOUT'))\n    if object_type == 'campaign_structured_snippet':\n        predicates.append(('ExtensionType', 'EQUALS', 'STRUCTURED_SNIPPET'))\n", "next_line": "    return _get_selector(fields, predicates)", "id": 19, "__internal_uuid__": "40379dc2-102e-4af9-a885-a25375b72d2a"}